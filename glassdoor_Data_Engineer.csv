Job Title,Salary Estimate,Company Name,Location,Job Description,Rating,Size,Founded,Type of ownership,Industry,Sector,Revenue
Senior Data Engineer,$100K - $133K (Glassdoor est.),LOOP BELL TECH INC3.7 ★,"Fort Worth, TX","Contract: W2
Note: No C2C
Responsibilities:
* Collaborates with architecture and technical leadership to define the vision and solutions
* Collaborate alongside other engineers of various disciplines to take the design and create executable pieces of work
* Participates in all phases of development
* Establishes and champions First Command data and data engineering standards/best practices
* Communicate and work alongside members of their team in support of their day-to-day work items
* Works with business partners to ensure alignment between the ask and the output
* Participate and lead peer reviews and champion peer review best practices and culture
* Key player and leader in an Agile environment, participating in daily huddles, sprint planning, retrospectives, etc.
* Mentors junior team members in best practices and standards
* Serve as escalation point for other team members on technical issues
* Leads effort to create and document deployment and release plans
* Works with architects to evaluate new technologies and patterns that will inform the technology roadmap
* Leads Communities of Practices or other cross functional training opportunities
* Leads troubleshooting processes to determine root cause analysis
REQUIREMENTS:
* Bachelor's degree required; MBA or MS or equivalent a plus
* 7+ years of applied experience in data integration, ETL, and data management or comparable positions that handle large/complex data sets, developing automation, and fostering business partner relationships
* Expert in one or more of the following ETL tools such as Azure Data Factory, Informatica, Matillion, Fivetran and DBT
* Experience working with a diverse set of data sources such as Flat File, Database, API, Event Streaming
* Expert in SQL with knowledge of T-SQL
* Strong experience in data modeling, data warehousing, and MDM solutions
* Familiar with Azure Synapse or Snowflake
* Familiar with Databricks Delta Lake
* Familiar with a scripting language such as python, powershell, or bash
* Familiar with data lake design patterns
* Excellent written communication and presentation skills
* Proficient in understanding of data mapping and lineage strategies
* Proficient in understanding in conceptual, logical, and physical data design
* Proficient in understanding of data management practices, data architecture principles, and data governance process
Preferred Qualifications:
* Expert in Azure Data Factory, Data Bricks, and Python
* Strong dimensional modelling skills
* Familiarity with DevOps principles and processes
* Applied experience in Agile, SAFe, or Scrum
* Financial services industry experience or other highly regulated industry experience a plus
* Familiarity with data science and analytics tools such as Alteryx, SPSS, SAS, Tableau, PowerBIResponsibilities:
* Collaborates with architecture and technical leadership to define the vision and solutions
* Collaborate alongside other engineers of various disciplines to take the design and create executable pieces of work
* Participates in all phases of development
* Establishes and champions First Command data and data engineering standards/best practices
* Communicate and work alongside members of their team in support of their day-to-day work items
* Works with business partners to ensure alignment between the ask and the output
* Participate and lead peer reviews and champion peer review best practices and culture
* Key player and leader in an Agile environment, participating in daily huddles, sprint planning, retrospectives, etc.
* Mentors junior team members in best practices and standards
* Serve as escalation point for other team members on technical issues
* Leads effort to create and document deployment and release plans
* Works with architects to evaluate new technologies and patterns that will inform the technology roadmap
* Leads Communities of Practices or other cross functional training opportunities
* Leads troubleshooting processes to determine root cause analysis
REQUIREMENTS:
* Bachelor's degree required; MBA or MS or equivalent a plus
* 7+ years of applied experience in data integration, ETL, and data management or comparable positions that handle large/complex data sets, developing automation, and fostering business partner relationships
* Expert in one or more of the following ETL tools such as Azure Data Factory, Informatica, Matillion, Fivetran and DBT
* Experience working with a diverse set of data sources such as Flat File, Database, API, Event Streaming
* Expert in SQL with knowledge of T-SQL
* Strong experience in data modeling, data warehousing, and MDM solutions
* Familiar with Azure Synapse or Snowflake
* Familiar with Databricks Delta Lake
* Familiar with a scripting language such as python, powershell, or bash
* Familiar with data lake design patterns
* Excellent written communication and presentation skills
* Proficient in understanding of data mapping and lineage strategies
* Proficient in understanding in conceptual, logical, and physical data design
* Proficient in understanding of data management practices, data architecture principles, and data governance process
Preferred Qualifications:
* Expert in Azure Data Factory, Data Bricks, and Python
* Strong dimensional modelling skills
* Familiarity with DevOps principles and processes
* Applied experience in Agile, SAFe, or Scrum
* Financial services industry experience or other highly regulated industry experience a plus
* Familiarity with data science and analytics tools such as Alteryx, SPSS, SAS, Tableau, PowerBI
Job Type: Contract
Schedule:
8 hour shift
Ability to commute/relocate:
Fort Worth, TX 76102: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Willing to work W2
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person
Show Less
Report",3.7,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Data Engineer,$65K - $93K (Glassdoor est.),Intralox4.3 ★,"New Orleans, LA","Intralox, L.L.C., a division of Laitram, L.L.C., and a global provider of conveyance solutions and services, has an opening for a Data Engineer/Power BI Developer within the Digital Solutions team. The Digital Solution (DS) team is leading Intralox’s implementation and evolution of our enterprise business applications and new digital solutions. DS is more than just an IT department – we are business domain experts, software developers, and operational support resources that use technology to bring more value to customers and Intralox.

Intralox is a division of Laitram, L.L.C., with an extensive portfolio of innovative conveyance solutions and services that improve lives and optimize businesses worldwide.

Our global workforce of over 3,000 employees in 20+ countries consist of reliable problem solvers, continuously developing and directly delivering solutions that have driven our customers’ growth worldwide for more than 50 years.

Intralox was founded on the principle of doing the right thing, by treating customers, employees, and suppliers with honesty, fairness, and respect. We invest heavily in these values and aim to practice our business philosophy principles every day, which is why we have been consistently recognized for innovation and workplace excellence. We believe in the power of a good idea no matter where it comes from, using trust as the foundation to how we work, and that self-managed people are our greatest asset.

Responsibilities:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Microsoft ‘big data’ technologies.
Build analytics tools in Power BI that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create and support data tools for analytics and business team members that assist them in building and optimizing our products and business processes.
Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements:
Minimum 2 years of experience in a Data Engineer role.
Bachelor's degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. A graduate degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field is preferred.
Advanced working knowledge of SQL and experience with relational databases, including query authoring (SQL). Additionally, familiarity with a variety of databases is desirable.
Proven experience in building and optimizing 'big data' data pipelines, architectures, and data sets.
Strong analytical skills for working with structured and unstructured datasets.
Proficiency in developing processes to support data transformation, data structures, metadata, dependency, and workload management.
Demonstrated success in manipulating, processing, and extracting value from large, disparate datasets.
Excellent project management and organizational skills.
Experience with or knowledge of the following software/tools or similar is required:
Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases.
Data pipeline and workflow management tools: Azure Data Factory.
Azure cloud services: VMs, Synapse Analytics, Data Factory, Azure SQL, Data Lake.
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Knowledge of Oracle EBS preferred.

EOE/M/F/Vet/Disabled
Show Less
Report",4.3,1001 to 5000 Employees,1971,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
Data Engineer - Deep Learning,$125K - $150K (Employer est.),Flawless AI4.6 ★,"Santa Monica, CA","Flawless AI is an energetic, growing startup operating at the intersection of AI and film making.
Our technology uses generative AI to make it look like your favorite actor or actress is speaking another language, without subtitles or voice-overs, making it indistinguishable from the original performance.
It doesn’t matter where you live or what languages you speak, audiences can now experience authentic storytelling exactly as the filmmaker originally intended through the magic of automated visual translation.
We believe the most important breakthroughs in AI are unlocked only when we apply it to real-world use cases that reach millions of people and are generally available. This is our north star and guiding principle.
Ethical, licensed, and balanced data is central to our AI research. The data team is responsible for sourcing, annotating, curating, and deploying large multi-modal datasets within the film media domain. The data team works with core and applied ML, lighting, staging, engineering, and film innovation teams to understand data requirements and deliver high-quality datasets that power next-generation AI models. The team is also responsible for data versioning, data DevOps, and persistent storage.
Our work in automated visual translation is just the beginning, we’re developing countless exciting products based on the application of our proprietary, cornerstone research.
This is an unbelievable opportunity to join a team operating at the cutting edge of the generative revolution, don't hesitate, reach out today.
Qualifications
Minimum Requirements
Bachelor's degree in computer science, machine learning, computer vision, or a related field
2+ years of experience preparing large datasets for deep learning and neural network models
2+ years of experience writing and testing modularized, production-level Python code
Experience with deep learning frameworks such as PyTorch, Tensorflow, Keras, or MXNET
Experience building scalable ML pipelines for image and video modalities with tools such as Flyte, Prefect, AirFlow, or Kubeflow
Experience with data collection, labeling, cleaning, and generation with tools such as LabelBox, SuperAnnotate, Scale Ai, or V7
Preferred Requirements
MS in computer science, machine learning, computer vision, or a related field
Experience with CI / CD automation using tools such as GitHub Actions or GitLab
Experience setting up and configuring cloud infrastructure resources with tools such as Terraform or CloudFormation
Experience with various cloud data storage technologies on AWS, GCP, or Azure
Experience writing production-grade C++
Experience with audio, text, or 3D data modalities
Benefits
Autonomy - You'll own your work from start to finish
Influence - You'll impact major research decisions
Publication - You’ll be encouraged to publish your work
Learning - You’ll push the state-of-the-art with the best in the world
Impact - Your input genuinely matters
Hybrid office model
Stock Options
Comprehensive medical, dental, and vision insurance
401(k) plan
Your choice of equipment
Flawless is proud to emphasize an equal opportunity, safe environment for people to do their best work. We are committed to providing equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Show Less
Report",4.6,51 to 200 Employees,2018,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
Test Data Engineer,$50.00 - $53.00 Per Hour (Employer est.),TekwissenGroup,"Raleigh, NC",-1,4.6,-1,-1,-1,-1,-1,-1
Cloud Data Protection Engineer,$70.00 - $78.00 Per Hour (Employer est.),Cloud Destinations4.8 ★,Remote,"My name is Muralidharan and I’m working with Cloud Destinations LLC as a Technical Recruiter.
Below are the job details as needed by the hiring manager. Kindly advise if you might be interested and qualified for the below opportunity and if we can discuss further on this
Position: Cloud Data Protection Engineer
Duration: 12-24+ Months Contract
Location: Remote
They are open to having remote resources (would have to travel occasionally), and would prefer candidates based in Charlotte, NC, Islen, NJ, or Dallas, TX.
Cloud Data Protection Engineer is responsible for designing, engineering and implementing a new, cutting edge, cloud platform security for transforming our business applications into scalable, elastic systems that can be instantiated on demand, on cloud.
The role requires for the Engineer to design, develop, configure, test, debug and document all layers of the Cloud stack to satisfy the new big data system & Security requirements.
This is expected to range from Cloud hosting platform to the design and implementation of higher level services such as the IaaS, PaaS and SaaS layers, big data platform, Authentication/Authorization, Data encryption techniques (field level, db level, application layer encryption) masking tokenization techniques, and other security services.
The focus of this role is on the Security of the product and service for cloud big data platform, with understanding of ETL, data consumption and data migration security as well as data loss prevention techniques.
The ideal candidate should be comfortable being directly involved with the design, development, testing, and operation of the solutions that will be composed into the Cloud Services environment.
They will also provide comprehensive security consultation to business unit, IT management and staff at the highest technical level. Be able to conduct detailed threat analysis and identify mitigating security controls and solutions.
They will work closely with Cloud Services management to identify and specify complex business requirements and processes that drive the platform and application security Patterns and roadmap.
They will research and evaluate alternative solutions and make recommendations for changes that would enhance the security of the platform
Job Type: Contract
Salary: $70.00 - $78.00 per hour
Schedule:
8 hour shift
Experience:
Cloud Security: 10 years (Required)
IaaS, PaaS, SaaS: 4 years (Required)
Data Encryption: 5 years (Required)
Data Protection: 4 years (Required)
Data loss prevention: 5 years (Required)
Big data: 1 year (Required)
Work Location: Remote
Speak with the employer
+91 925-887-0055
Show Less
Report",4.8,201 to 500 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,$5 to $25 million (USD)
Data Engineer W/ Pega,-1,Sunera Technologies3.8 ★,Remote,"Hi
We have a direct client requirement for Data Engineer W/ Pega @ Miami FL.
Role: Data Engineer W/ Pega
Location: Miami FL
Duration : Long Term
Bachelor’s degree in IT related areas.
•5 or more years of relational databases specifically Oracle, DB1 experience mandatory.
•5 or more years of SQL and PL/SQL query language experience mandatory.
•5 or more years of Unix experience mandatory.
•5 or more years of automating routine tasks via stored procedures mandatory.
•Previous experience with Pega required.
Experience writing and/or understanding SQL statements OR previous Campaign design and execution with a campaign management solution application is mandatory
Job Types: Full-time, Contract
Experience level:
9 years
Schedule:
8 hour shift
Work Location: Remote
Show Less
Report",3.8,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
Data Engineer,$74K - $105K (Glassdoor est.),Gorbel3.6 ★,"Victor, NY","Gorbel’s mission is simple: We improve people’s lives.
That mission guides everything we do, from the products and service we provide to our outside customers to the work environment we foster for our employees. We are a manufacturer of material handling and fall protection products for the production and warehouse/distribution sectors. We’re on the cutting edge of manufacturing and distribution; a thriving, growing company that is constantly seeking out new ways to innovate and elevate our products and our processes – and we’re looking for people like you to join us in that mission.
We’re currently hiring for open positions in the US and Canada. We operate in Canada as Engineered Lifting Systems and Equipment (ELS)/DBA Gorbel® Canada, and subsequent communication related to Canadian positions may show the ELS name. You may be contacted by phone by recruitment personnel based in either Canada or New York.
Work Shift:
Job Description:
The Data Engineer is responsible for working with interdepartmental stakeholders and the data scientist to transform business requirements into effective high-quality professional visualization for consuming analytics. This person must be a self-starter who is comfortable with ambiguity and has strong attention to detail. The Data Engineer performs the transformation, filtering, and aggregation of raw data into concise, accurate, and focused data models by using internal software capabilities to acquire, ingest, and transform big datasets. This position is also responsible for collaborating with cross-functional teams for generating insights and presenting findings to senior management or using data visualization and presentation programs to suggest business improvements. Also, supporting ad-hoc analyses and reports needed for business decisions, planning, and execution. The Data Engineer also implements scalable data services using serverless Azure resources such as Data Factory, Synapse, Databricks, Azure Functions, and traditional SQL. The Data Engineer is responsible for new database design, performance tuning, and advanced administration both On-premise and cloud.
RESPONSIBILITIES:
Create and maintain optimal data pipeline architecture with Azure Data Factory and SSIS.
Build Data Storage Solutions with SQL Servers, Azure SQL DB, and Data Lakes.
Translate reporting and business needs into a scalable and manageable data solution
Engage with data source platform leads to gain a tactical and strategic understanding of data sources required by Data Services AI/ML.
Ensure data extraction, transformation, and loading data meet data security & compliance requirements.
Engage with Information Technology and Software Engineering for database design, performance tuning, and advanced administration. Both On-premise and in the cloud.
Create, maintain, and store documentation that describes the ETL solutions process for future reference
Keeps a working knowledge of new technologies that can be leveraged to drive improvement in our data management processes
Assist with the development and implementation of best practices around data management to ensure the accuracy, validity, reusability, and consistent definitions for common reference data.
REQUIRED QUALIFICATIONS:
Bachelor’s degree or combination of relevant experience in Computer Science, Information Systems, or other related field.
3-5 Years experience with coding and application development experience with multiple programming languages such as Python, R, SQL, or similar scripting languages.
3-5 Years Hands-on experience with cloud orchestration, automation tools, and CI/CD pipeline creation using Azure DevOps.
Strong Understanding of data modeling, data warehousing, data lakes, and big-data concepts.
Proficient in using visualization technology, proficiency in DAX, and working knowledge of Power Query to produce large-scale visual analytics implementations, performance tuning, and optimization
Ability to meet tight deadlines with high quality requirements
WORK ENVIRONMENT:
ADA Physical/Mental/Workplace Requirements
Occasional lifting up to 25 lbs.
Sitting, working at desk/personal computer for extended periods of time
Primary work environment is professional corporate
Gorbel® is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, alienage or national origin, ancestry, citizenship status, age, disability or handicap, gender, marital status, veteran status, sexual orientation, genetic information, arrest record, or any other characteristic protected by applicable federal, state or local laws. Gorbel® is also committed to providing reasonable accommodations to qualified individuals so that an individual can perform their job related duties. If you are interested in applying for an employment opportunity and require special assistance or an accommodation to apply due to a disability, please contact us at 585-924-6204.
Show Less
Report",3.6,201 to 500 Employees,1977,Company - Private,Machinery Manufacturing,Manufacturing,$25 to $100 million (USD)
Data Engineer,$70.00 Per Hour (Employer est.),Tellus solutions3.7 ★,"Sunnyvale, CA","Job Description:
The role will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoy optimizing data systems and building them from the ground up.
The Data Engineer will support our software developers, database architects, and data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
The right candidate will be excited by the prospect of optimizing or even re-designing our data architecture to support our next generation of products and data initiatives.
Responsibilities:
Create and maintain optimal data pipeline architecture for data intensive applications.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure SQL, Cosmo DB, Databricks and other legacy databases.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and Azure regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive Experience on Databricks on Azure Cloud platform, deep understanding on Delta lake, Lake House Architecture.
Programming experience on Python, Shell scripting, PySpark, and other data programming language.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with Data Visualization Dashboard, Metrics and etc.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Skills:
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Familiar with Deployment tool like Docker and building CI/CD pipelines.
Experience supporting and working with cross-functional teams in a dynamic environment.
8+ years' experience in software development, Data engineering, and
Bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field. Postgraduate/master's degree is preferred.
Experience in Machine Learning and Data Modeling is a plus.
Job Type: Contract
Salary: Up to $70.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Day shift
Application Question(s):
Only US Citizen and Green Card Holder
Experience:
Python, Shell scripting, PySpark: 5 years (Required)
Azure SQL: 5 years (Required)
Work Location: On the road
Show Less
Report",3.7,51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
Azure Data Engineer,$45.00 - $50.00 Per Hour (Employer est.),AppsIntegration Inc2.9 ★,Remote,"Position: Sr. Azure Data Engineer
Duration: 12+months
Location: 100% Remote
TOP 3:
Azure Data Engineer:
Must have ADF, Azure SQL, Spark
1) Support and refine Constellation’s data and analytics technology stack with an emphasis on improving reliability, scale, and availability. ADF, Spark
2) Assist in the design and management of enterprise grade data pipelines and data stores that will used for developing sophisticated analytics programs, machine learning models, and statistical methods.
3) Experience delivering data solutions via Agile methodologies and designing CI/CD workflows.
PRIMARY DUTIES AND ACCOUNTABILITIES
Item Accountability %
1 Create and maintain optimal data pipeline architecture 20
2 Assemble large, complex data sets that meet functional / non-functional business requirements. 20
3 Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 20
4 Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Big Data technologies. 20
5 Deliver automation & lean processes to ensure high quality throughput & performance of the entire data & analytics platform.? 10
6 Work with data and analytics experts to strive for greater functionality in our analytics platforms. 10
POSITION SPECIFICATIONS
Minimum: Preferred:
Experience in building/operating/maintaining fault tolerant and scalable data processing integrations using Azure
Strong problem-solving skills with emphasis on optimization data pipelines
Excellent written and verbal communication skills for coordinating across teams
A drive to learn and master new technologies and techniques
Experienced in DevOps and Agile • Experience using Docker or Kubernetes is a plus
Demonstrated capabilities with cloud infrastructures and multi-cloud environments such as Azure, AWS, IBM cloud
environments and using CI/CD pipelines.
Experienced using Databricks & Apache Spark
Experienced using Azure Data Factory or Synapse Analytics
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Compensation package:
Weekly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Show Less
Report",2.9,1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
Data Engineer,$55K - $65K (Employer est.),NYC Office Of The Mayor4.0 ★,"New York, NY","Mayor’s Office of Immigrant Affairs
Position Title: Data Engineer

The Agency You’ll Join:
The NYC Mayor's Office administers all city services, public property, most public agencies, and enforces
all city, state, and federal laws within New York City. New York City’s Mayor, Eric Adams is head of the
executive branch of New York City's government. Mayor Adams has served the people of New York City
as an NYPD officer, State Senator, and Brooklyn Borough President. The Adams’ Administration is
leading the fight to make New York City’s economy stronger, reduce inequality, improve public safety,
and build a stronger, healthier city that delivers for all New Yorkers. As an agency, we value fairness,
helpfulness, transparency, leadership and build our teams around these values. For current job
opportunities visit our careers page

The Team You’ll Work With:
The Mayor’s Office of Immigrant Affairs (MOIA) was established pursuant to the New York City Charter to
promote the well-being of immigrant communities. To achieve this, MOIA serves as a bridge between city
government and immigrant communities; advises and assists in developing and implementing policies
designed to assist immigrants and speakers of languages; and supports and enhances the ability of city
agencies and offices to serve these immigrant populations. MOIA’s work cuts across a broad range of
issues citywide and MOIA works closely with cities around the country and the world to promote
innovations in immigrant access to services. To learn more, visit nyc.dov/site/immigrants/index.page

The Problems You’ll Solve
The Mayor’s Office of Immigrant Affairs is seeking a data engineer to be the tech lead on key projects,
including research, feasibility assessments, requirements gathering, solution development,
implementation planning, documentation creation, and roll-out of systems. Involves developing creative
solutions for challenges including disparate data, limited systems/resources, and understanding the City’s
policies for technology usage. Data analysis will focus on organizational performance more than
predictive analysis.
This role will act as the technical lead in the design and execution of systems implementations, will a ct
as lead for recommendations on technology solutions and approaches, and will provide data analysis
solutions for the office.
Responsibilities include, but not limited to:
Collaborate and coordinate with all MOIA teams to understand business requirements and
develop business solutions to meet these requirements.
Manage the planning, execution, and on-time delivery of assessments related to technology
solutions and approaches to data collection and visualization


Mayor’s Office of Immigrant Affairs
Make recommendation on technology platforms and solutions.
Monitor and track progress of solutions development.
Provide technical assistance and training to staff in the implementation of solutions.
Generate various project-related documents including technical guides, user guides, installation
guides, templates, and technical and maintenance documentation.
Initiate, facilitate, and participate in on-going meetings with project team members to ensure that
solutions meet business needs and continue to iterate on enhancements.
Provide first level support to MOIA staff for troubleshooting and addressing questions related to
use of implemented solutions.
Work with external agencies such as DoITT, the Department of Information Technology and
Telecommunication, and the Mayor’s Office MIS department, to communicate needs, track
progress of trouble tickets and enhancement request, and arrange priorities according to
resources.
Provide support in technology-related issues to the Director of Operations.
Resolve and/or escalate issues in a timely manner.
About You
High level of technical skills, including but not limited to advanced Excel and Power Query
High level of comfort with data-driven analysis, and skills necessary to present data and trends in
a useful manner
2 years of experience in advanced statistical analysis and data visualization using Power BI
2 years of experience in CRM systems (Microsoft Dynamics highly desirable).
3 years of experience in SQL creation and data manipulation
2 years of experience in HTML and JQuery
Experience with integration of data from multiple data sources
Experience with improving data reliability, efficiency and quality and ability to make
recommendations
Strong intuition for analytical methodologies and desire to solve novel technical challenges
Experience with BI frameworks and/or organization performance management analysis
Proficiency in designing efficient and robust ETL workflows
Excellent critical thinking and problem-solving skills with the ability to set priorities and hold staff
accountable on other teams for outcomes
Excellent communications skills, both written and verbal; training experience highly desired
Excellent organizational skills
Highly professional demeanor
Ability to work independently


Mayor’s Office of Immigrant Affairs
Patience, and the ability to navigate and work within a system in which many agency players
have input into technology decisions.
Ability to develop solutions that are not always obvious
A track record of effectively handling multiple priorities
Proven ability to work in a fast-paced environment and meet deadlines, and work productively
under pressure, both as an individual and as part of a team
Salary
The City of New York Office of the Mayor’s compensation package includes a market competitive salary,
equity for all full-time roles and exceptional benefits. Our cash compensation range for this role is
$ 55,000 – $ 65,000.
Final offers may vary from the amount listed based on candidate experience and expertise, and other
factors.

Equal Opportunity | Diversity Equity & Inclusion Statement
The Office of the Mayor is an is an inclusive equal opportunity employer committed to recruiting and
retaining a diverse workforce and providing a work environment that is free from discrimination and
harassment based upon any legally protected status or protected characteristic, including but not
limited to an individual's sex, race, color, ethnicity, national origin, age, religion, disability, sexual
orientation, veteran status, gender identity, or pregnancy.
The Adams Administration values diversity — in backgrounds and in experiences that is reflective of the
city it serves. Applicants of all backgrounds are strongly encouraged and welcome to apply.
If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable
accommodation if you are unable or limited in your ability to access job openings or apply for a job on
this site as a result of your disability. You can request reasonable accommodations by EEO at
EEO@cityhall.nyc.gov.

New York City Residency Is Required Within 90 Days of Appointment
Show Less
Report",4.0,Unknown,-1,Government,Municipal Agencies,Government & Public Administration,Unknown / Non-Applicable
Data Engineer,$45.00 - $50.00 Per Hour (Employer est.),Business Integra Inc.3.8 ★,Remote,"Remote Data Engineer / Architect
Location: Remote Work
Experience level: 7+ years
Required skills:
- Shell Scripting
- SQL, Python, Certificate Management
- Delphix, Genrocket, TDM
Description:
The Data Architect works in all data environments which includes data design, database architecture, metadata and repository creation. The Data Architect work assignments are varied and frequently require interpretation and independent determination of the appropriate courses of action.
for developing blueprints for all data repositories, evaluating hardware and software platforms, and integrating systems. Translates business needs into long-term data architecture solutions. Defines, designs and builds dimensional database schemas. Evaluates reusability of current data for separate analyses. Conducts data sheering to rid the system of old, unused or duplicate data. Reviews object and data models and the metadata repository to structure the data for better management and quicker access. Understands department, segment, and organizational strategy and operating objectives, including their linkages to related areas. Makes decisions regarding own work methods, occasionally in ambiguous situations, and requires minimal direction and receives guidance where needed. Follows established guidelines/procedures.
Required Qualifications
Bachelor's degree in Computer Science, Information Technology or related field
Less than 5 years of technical experience
Operational Data Integration for real-time APIs
Big Data Integration & Analytics
Must be passionate about contributing to an organization focused on continuously improving consumer experiences
Preferred Qualifications
Master's Degree
Job Type: Contract
Salary: $45.00 - $50.00 per hour
Experience level:
7 years
Schedule:
8 hour shift
Experience:
delphix: 4 years (Preferred)
genrocket: 4 years (Preferred)
Shell Scripting: 5 years (Preferred)
SQL: 6 years (Preferred)
test data management: 5 years (Preferred)
Work Location: Remote
Show Less
Report",3.8,201 to 500 Employees,2001,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
Data Engineer,$141K - $155K (Employer est.),Sephora3.6 ★,"San Francisco, CA","Job ID: 227599
Location Name: CA-FSC SF Off (0174)
Address: 525 Market St, 4th Floor, San Francisco, CA 94105, United States (US)
Job Type: Full Time
Position Type: Regular
Job Function: Information Technology

Company Overview:
At Sephora we inspire our customers, empower our teams, and help them become the best versions of themselves. We create an environment where people are valued, and differences are celebrated. Every day, our teams across the world bring to life our purpose: to expand the way the world sees beauty by empowering the Extra Ordinary in each of us. We are united by a common goal - to reimagine the future of beauty.

The Opportunity:

Your role at Sephora:
As a Data Engineer at Sephora, you will: Gather requirements using interviews, document analysis, requirements workshops, business process descriptions, business analysis and workflow analysis. Perform data migration from Enterprise data warehouse to new data platform. Create source to Stage and Stage to Target data mapping documents indicating the source tables, columns, data types, transformation required and business rules to be applied. Collect and analyze business data to develop solutions. Write SQL queries to analyze large datasets and resolve business engineering needs. Analyze and report on computer systems application implementation using Cognos, Tableau, SQL, and Big Data (Hive). Work with Tableau testing on mobile and web platforms. Define aggregation logic for new reporting requests. Analyze new source ingestion required for migration. Develop production support architecture, design, configuration, customization, integration and user acceptance testing. Work on multiple computer systems engineering projects using Software Development Life Cycle (SDLC) and Agile development methodologies. (Position allows some work-from- home flexibility, with schedule to be approved by manager. Must be able to work on site as required).

We are excited about you if you have:
Bachelor’s or foreign equivalent degree in Computer Science, Engineering or Information Systems.
Three (3) years of experience in data engineering and business analytics.

Experience must include:
Azure Databricks
Agile
SDLC
Microsoft SQL server
Cognos, SSAS Cube
Kafka
Tableau
Data analysis and mapping, data profiling, functional and data testing.

Salary: $141,000 to $155,000 per year depending on experience

Please visit our career website for additional information about our benefits package

While at Sephora, you’ll enjoy…

The people. You will be surrounded by some of the most talented leaders and teams – people you can be proud to work with.
The learning. We invest in training and developing our teams, and you will continue evolving and building your skills through personalized career plans.
The culture. As a leading beauty retailer within the LVMH family, our reach is broad, and our impact is global. It is in our DNA to innovate and, at Sephora, all 40,000 passionate team members across 35 markets and 3,000+ stores, are united by a common goal - to reimagine the future of beauty.

You can unleash your creativity, because we’ve got disruptive spirit. You can learn and evolve, because we empower you to be your best. You can be yourself, because you are what sets us apart. This, is the future of beauty. Reimagine your future, at Sephora.

Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities.

Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.
To apply to this job, click Apply Now
Show Less
Report",3.6,10000+ Employees,1969,Company - Private,Beauty & Personal Accessories Stores,Retail & Wholesale,$1 to $5 billion (USD)
Data Engineer,$90K - $119K (Employer est.),Rockstar Games4.2 ★,"New York, NY","At Rockstar Games, we create world-class entertainment experiences.
A career at Rockstar Games is about being part of a team working on some of the most creatively rewarding and ambitious projects to be found in any entertainment medium. You would be welcomed to a dedicated and inclusive environment where you can learn, and collaborate with some of the most talented people in the industry.
Rockstar is seeking a Data Engineer to join a team focused on building a cutting-edge game analytics platform and tools to better understand our players and enhance their experience in our games. This is a full-time permanent position based out of Rockstar's unique game development studio in New York, NY.
The ideal candidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability and performance. In collaboration with other data engineers, machine learning engineers, and software engineers, the candidate will empower the team of analysts and data scientists to deliver data driven insights and applications to company stakeholders.
WHAT WE DO
The Rockstar Analytics team provide insights and actionable results to a wide variety of stakeholders across the organization in support of their decision making.
We are currently adding team members to multiple verticals including; Machine Learning and Game Data Pipeline.
RESPONSIBILITIES
Implement and maintain real-time and batch Data Models.
Deliver real-time and non-real-time data models to analysts and data scientists who create insights and analytics applications for our stakeholders.
Implement and support streaming technologies such as Kafka, Spark, Cassandra & AzureML.
Assist in the development of deployment automation and operational support strategies.
Assist in the development of a big data platform in Hadoop using pipeline technologies such as Spark, Airflow, and more to support a variety of requirements and applications.
Set the standards for warehouse and schema design in massively parallel processing engines such as Hadoop and Snowflake while collaborating with analysts and data scientist in the creation of efficient data models.
Maintain and extend our CI/CD processes and documentation.
QUALIFICATIONS
3+ years of work experience with data modeling, business intelligence and machine learning on big data architectures.
2+ years of experience with the Hadoop ecosystem (HDFS, Spark, Oozie, Impala, etc.) and big data ecosystems (Kafka, Cassandra, etc.).
2+ years of experience with the Azure ecosystem (Azure ML, Azure Data Factory)
Expert in at least one SQL language such as T-SQL or PL/SQL.
Experience developing and managing data warehouses on a terabyte or petabyte scale.
Strong experience in massively parallel processing & columnar databases.
Experience building Real-Time and/or Near-Real-Time ML pipelines.
Experience with Python, Scala, or Java.
Experience with shell scripting.
Experience working in a Linux environment.
SKILLS
Deep understanding of advanced data warehousing concepts and track record of applying these concepts on the job.
Ability to manage numerous projects concurrently and strategically, prioritizing when necessary.
Good communication skills.
Dynamic team player.
A passion for technology.
PLUSES
Please note that these are desirable skills and are not required to apply for the position.
Experience with Python based libraries such as Scikit-Learn
Experience with Databricks
Experience with Spark-ML, Jupyter Notebook, AzureML.
Experience in Lambda architecture.
Experience with CI/CD.
Familiar with Restful APIs.
Experience with Artifact Repositories.
Knowledge of the video game industry.
HOW TO APPLY
Please apply with a resume and cover letter demonstrating how you meet the skills above. If we would like to move forward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through the process.
Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and compensating employees based on their qualifications and demonstrated ability to perform job responsibilities.
If you've got the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates regardless of age, disability, gender identity, sexual orientation, religion, belief, or race.
The pay range for this position in New York City at the start of employment is expected to be between the range below* per year. However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations. Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards, in addition to a full range of medical, financial, and/or other benefits. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an ""at-will position"" and the company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, company or individual department/team performance, and market factors.

NYC Pay Range
$89,500—$119,400 USD
Show Less
Report",4.2,1001 to 5000 Employees,1998,Subsidiary or Business Segment,Video Game Publishing,Media & Communication,$5 to $25 million (USD)
Looking for Data Engineer with EMR and/or EHR with 10+ years(Remote),$60.00 - $70.00 Per Hour (Employer est.),Lethyagroupinc,Remote,"Role: Data Engineer with EMR and/or EHR
Client: Confidential
Duration: Long Term
Location: Remote
Job Description
10+ years in IT with at least 3+ years’ experience in data warehousing, modelling, end-to-end BI solutions
Strong SQL, Spark, and PySpark programming skills for data analysis.
Experience with hospital/provider solutions like (EMH, EMR codes, Billing, etc.):
Strong understanding of Data Engineering Solutions, Data modelling, and Software Engineering principles and best practices
Experience in developing data platforms/ Big data and cloud technologies (e.g., Azure)
Advanced knowledge of SQL and query optimization techniques and approaches
Experience designing, developing, and supporting Power BI data sources and reports.
Able to work as a team member and willing to work independently when required.
Strong troubleshooting and problem-solving skills
Experience working in an Agile/SCRUM SDLC environment.
Problem-solving aptitude, with a willingness to work in a fast-paced product development environment and hands-on mentality to do whatever it takes to deliver a successful product.
Experience Skill Matrix:
Data Engineer: Years
Experience with hospital/provider solutions: Years
Must - EMR (electronic medical record) and/or EHR (electronic health record): Years
Data warehousing, modelling, end-to-end BI solutions: Years
SQL, Spark, and PySpark programming: Years
Azure: Years
SQL and query optimization: Years
Designing, developing, and supporting Power BI data sources and reports: Years
Agile/SCRUM SDLC environment: Years
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Health insurance
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure (Preferred)
SQL and query optimization (Preferred)
Designing, developing, and supporting Power BI (Preferred)
Agile/SCRUM SDLC environment (Preferred)
Data Engineer - 10 Years (Preferred)
hospital/provider solutions (Preferred)
EMR / EHR (Preferred)
Data warehousing, modelling, end-to-end BI solutions (Preferred)
SQL, Spark, and PySpark programming (Preferred)
Work Location: Remote
Show Less
Report",-1,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
Data Visualization Engineer,$66K - $90K (Glassdoor est.),GE Gas Power3.9 ★,"Greenville, SC","Job Description Summary
The individual will be part of a team tasked with unlocking value from the data lake both directly and in partnership with other functional and IT organizations. The candidate should have related expertise with report design, report tuning and performance optimization, data analysis, and data manipulation as well as a fundamental understanding of end-to-end business processes and BI systems.
Job Description
In this role, you will:
Work Closely with teams and stakeholders to Increase shop adoption/utilization of existing GSC products/applications using Tableau & other analytics to identify and monitor gaps in both functionality and operational rigor
Help create shop operations standard work through analytics, data driven decision making, and data visibility. Work closely with teams and individuals to design and create dashboards/reports to ensure accurate data. Provide test support
Influence GSC product capabilities & functionality by feeding requirements/enhancements to close gaps identified through analytics; identify opportunities to build best practice/critical analytics into core GSC products
Coordinate with regional & global deployment team to identify analytics needs to support deployment roadmap – including gaps & existing capabilities
Partner with key functional & DT team members to improve overall adoption of existing analytics/dashboards through best practice sharing, training, and enhancements. Strong interpersonal skills, with ability to professionally interact with a diverse blend of personalities to reach resolution and maintain strong relationships.
Assist and manage customers through the report requirements and design process
Ability to query data both in relational database and big data platforms, can troubleshoot data issues and provide corrective measures
Ability to work with other technical teams across multiple initiatives to provide timely delivery in an enterprise environment
Education Qualification
Bachelor's Degree in Computer Engineering, Computer Science, Information Systems, Information Technology, “STEM” Majors (Science, Technology, Engineering, Math),or similar with a minimum of 0-2 years of experience
Desired Characteristics
Prior experience with Tableau &/or Business Objects, Spotfire
Expert understanding of multiple modern BI software & visualization tools
Experience developing analytic solutions on top of MPP databases such as Greenplum, Teradata
Prior experience working with logical or semantic models as the basis for report development
Communicate complex issues effectively to internal staff and customers
Strong attention to detail
Work effectively with project managers and client stakeholders to understand and refine project requirements and deadlines
Desire to be part of a leading edge program with tight deadlines, global enterprise scope and high expectations, ability to learn new technology, processes, organizations, data sets and use cases fast and create solutions immediately

This Job Description is intended to provide a high level guide to the role. However, it is not intended to amend or otherwise restrict/expand the duties required from each individual employee as set out in their respective employment contract and/or as otherwise agreed between an employee and their manager.
Additional Information
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: No
Start your job application: click Apply Now
Show Less
Report",3.9,10000+ Employees,1892,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
Data Engineer,-1,CareMetx3.4 ★,Remote,"Hey!

Are YOU passionate about applying cutting-edge technology to improve the human experience? Are you passionate about fixing a broken healthcare system that is difficult to navigate, with barriers to access and afford life-changing medicine and treatment? Are you passionate about technical excellence and deploying software that makes people happier and healthier? If so, CareMetx wants you to be a part of our growing engineering team!

At CareMetx, data teams own an outcome - that means teams are both accountable AND empowered for a unit of business value. We create an environment for learning opportunities and believe there is no such thing as “that’s not my job.” Our vision is to generate valuable insights from the data and drive strategic decision-making. All Caremetx data team members have the opportunity to explore new technologies and data trends. That means you will have the opportunity to grow deeper in the skills you’re passionate about and expand your breadth by learning skills that will help the team succeed.

As a Data engineer/analyst/scientist you will constantly deliver business value. You will also function as a catalyst for innovation and new ideas through creative problem-solving, elegant engineering, and the application of new technology and architectural patterns. You will help shape a performance-oriented learning culture by sharing your knowledge and skill depth within the team.


Manage large data sets and model complex problems that impact patient outcomes. Discover insights and identify opportunities using statistical, algorithmic, mining, and visualization techniques.


The core of the role is…
Define and build an industry-standard pipeline and Data Warehouse for a variety of data sources (No SQL, Relational, Text)
Enhance data collection procedures to include information that is relevant for building analytic systems
Model front end and backend data sources to help draw a more comprehensive picture of user flows throughout our system and enable powerful data analysis
Processing, cleansing, and verifying the integrity of data used for analysis.
Performing ad-hoc analysis and presenting results in a clear manner
Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
Reformulating existing frameworks to optimize their functioning.
Testing such structures to ensure that they are fit for use.
Preparing raw data for manipulation by data scientists
Relevant experience…
Degree(s) in Engineering, Computer Science, Math, Statistics, Economics, or related fields
5+ years of professional experience either in Big Data, Data Engineering, or Business Intelligence. This might include ETL, data warehousing, or data visualization.
Experience with Talend is preferred
Experience with API is preferred
Understanding of CI/CD, data governance, and data quality framework (great expectations) is preferred
5+ years of hands-on experience applying principles, best practices, and trade-offs of schema design to various types of database systems: relational (Oracle, MSSQL, Postgres, MySQL), NoSQL (HBase, Cassandra, MongoDB), and in-memory (e.g., VoltDB). Understanding data manipulation principles.
Deep understanding of NoSQL databases like MongoDB/Dynamo DB.
Understanding of data flows, data architecture, ETL, Star vs Snowflake schema, and processing of structured and unstructured data
Minimum 3 years of designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Java, Python, Scala, etc.
Show Less
Report",3.4,501 to 1000 Employees,2011,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
Data Engineer,$60.00 - $70.00 Per Hour (Employer est.),zettalogix.Inc,Remote,"Title: Data Engineer
Experience in Retail merchandising analytics is a must,
Duration: 12 months
Location: Remote
Interview Process:
1*_st_ round – Hirevue Video Call*
2*_nd_ round – Hiring Manager*
Skills Required:
Data engineering, analytics, and data modeling experience
Python, Spark, Databricks, Azure, Power BI
Experience in retail merchandising analytics is a must
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
retail merchandising analytics: 8 years (Required)
Python: 8 years (Required)
Power BI: 8 years (Required)
Work Location: Remote
Show Less
Report",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Data Engineer,$87K - $129K (Glassdoor est.),Malin USA3.7 ★,"Addison, TX","***NO SPONSORSHIP AT THIS TIME***
Data Engineer Duties and Responsibilities
· Assemble large, complex sets of data that meet non-functional and functional business requirements
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
· Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies
· Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
· Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues
· Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
Skills and Qualifications
· Ability to build and optimize data sets, ‘big data’ data pipelines and architectures
· Ability to understand and build complex data models for Power BI reporting.
· Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
· Excellent analytic skills associated with working on unstructured datasets
· Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Must Have experience in (in order of importance)
· 10 + years experience as a Data Engineer or similar role.
· SQL Server
o T-SQL code
o Administrator functions
· Data Factory
o Python
· Power BI
o DAX code
o M code
· Azure
Malin is an Equal Opportunity Employer -- M/F/Veteran/Disability/Sexual Orientation/Gender Identity
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Paid time off
Referral program
Vision insurance
Experience level:
10 years
Schedule:
Monday to Friday
Work Location: Hybrid remote in Addison, TX 75001
Show Less
Report",3.7,501 to 1000 Employees,1971,Company - Private,Shipping & Trucking,Transportation & Logistics,$100 to $500 million (USD)
Data Engineer,$86K - $121K (Glassdoor est.),Virtualware Innovations4.8 ★,"Dallas, TX","Job Description
5+ years of experience in IT
Excellent knowledge in SQL and SSIS packages
Should have worked in GCP for more than 6 month
Good to have knowledge in Hadoop/spark ( any of this)
Should have basic knowledge in ETL
Good to have skill set: DB2 & Informix
Skillset Required – GCP, Spark , PySpark and Python, ETL tools , SQL, SSIS
Show Less
Report",4.8,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Data Engineer,$95.00 - $105.00 Per Hour (Employer est.),Ryzen Solutions2.7 ★,"Cupertino, CA","Data Engineer
Master degree in computer science, Mathematics or related field, or equivalent practical experience
3 to 6 years of experience in creating ETL workflows and automation using Python & database designing techniques
Experience writing code in Python, PySpark and SQL
Libraries Panda NumPy and good with time series
Knowledge in query troubleshooting such as isolating blocks of poor performing code, determining root cause, and developing remediation actions
Design-thinking & excellent verbal and written communication skills
Responsibilities
•Good experience in developing robust Python applications
•Efficient in database queries and data storage best practices
Data structure and Algorithm
Job Type: Contract
Pay: $95.00 - $105.00 per hour
Experience level:
2 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Cupertino, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data structures: 2 years (Required)
Python: 2 years (Required)
Link List: 2 years (Required)
Algorithm design: 2 years (Required)
Work Location: In person
Show Less
Report",2.7,Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Data Engineer,$50.00 - $75.00 Per Hour (Employer est.),Okaya infocom4.1 ★,"Jersey City, NJ","Experience in Bigdata (Kafka, Elastic search, Logstash, Kibana)
Experience in Aws Managed services MSK, Glue, IAM etc
Experience in Hortonworks Data Platform or Cloudera Distribution stacks
AWS Development experience using these services ( RDS with Postgres SQL Experience, DynamoDb, Data Pipeline, Data Base Migration, AWS Kafka )
Experience in CDC process (Added Advantage if knowledge in Debezium or any CDC tools)
Required Experience in SQL or ORACLE ( Must),Data Modeling Concepts.
Required experience with Data Engineering and ETL/EDW Design Process and Practices.
Experience with GIT / Bit Bucket / SVN configuration for code check-in/Check-out and CI CD knowledge.
Experience with Source to Target mapping and Data Modeling practice.
Experience with Rest API Process ( PUT/GET)
Experience in Agile Methodology using SCRUM or Kanban
Insurance domain experience is a plus.
Handling Client experience will be added advantage
Job Types: Full-time, Contract
Salary: $50.00 - $75.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
8 hour shift
Application Question(s):
What is your Work Authorization?
What's the RATE you are looking for?
Are you willing to relocate?
Experience:
Informatica: 7 years (Required)
SQL: 9 years (Required)
Data warehouse: 8 years (Required)
Work Location: On the road
Show Less
Report",4.1,Unknown,2006,Self-employed,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Data Engineer,$88K - $118K (Glassdoor est.),Loopback Analytics4.4 ★,"Dallas, TX","This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis.

Come join our Real World Data team at Loopback Analytics, the Loopback platform assembles clinical, pharmacy, enterprise and social data for insight and action across the specialty pharmacy and life sciences value chain. The ideal candidate would be an experienced Data Engineer who will be responsible for building and maintaining data pipelines. The Data Engineer will facilitate deeper analysis and reporting across complex data sets to support customers.

Job Duties to Include
Assemble and manage large, complex sets of data to meet functional business and analytical requirements
Build required infrastructure, documentation and roadmap for optimal extraction, transformation and loading of data from various data sources
Design infrastructure for greater scalability, optimizing data delivery and automating manual processes
Plan, coordinate and implement security measures to safeguard data
Work with stakeholders including data, product and executive teams and assist with data-related technical issues
Develop and maintain processes for data profiling, data documentation, and data quality measurement leveraging both manual and automated data quality testing

Requirements

Technical Experience: 3-5 years of experience to include:
Implementing and designing data infrastructure to support data curation and data analysis
Orchestrating data transformation through cloud native analytics platforms (Snowflake, Databricks) across cloud environments (Azure, AWS, GCP)
Building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development and performance and optimization techniques (e.g. SQL & NoSQL, C#, Python, etc.)
Learning and understanding the various technical domains across the enterprise and able to communicate complex technical and business concepts across the enterprise and various business stakeholders
Documenting and testing of designed solutions
Writing code that runs in a production system or experience in machine learning

Required Education:
Bachelors, masters, or Ph.D. in computer science, software engineering or a related field or equivalent experience

Personal Characteristics:
Complex problem solver
Excellent program/task organizational skills
Detail and results oriented
Excellent communication skills

Travel:
Minimal

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin. For immediate full-time consideration, please forward your resume to Loopback Analytics via email at careers@loopbackanalytics.com.

About Loopback

Founded in 2009, Loopback was rated as one of the best places to work in Dallas by the DBJ. Loopback Analytics is a leading provider of data-driven solutions for hospitals and health systems. The company’s comprehensive analytics platform drives growth for specialty and ambulatory pharmacy programs while connecting pharmacy activities with clinical and economic outcomes. Loopback’s clients include leading academic medical centers, health systems, and life sciences companies. For more information about our company and services please visit our website at www.loopbackanalytics.com.
Show Less
Report",4.4,51 to 200 Employees,2009,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
GCP Data Engineer,$70.00 - $75.00 Per Hour (Employer est.),Apolis3.9 ★,"Dearborn, MI","Desired Qualifications & Experiences:
• Five or more years’ experience in software engineering.
• Five or more years’ experience in large scale RDBMS environments or Google BigQuery
• Two or more years of Exadata experience OR Google BigQuery
• Four or more years’ experience with Informatica PowerCenter or IICS
• One or more years experience in Erwin
• Experience in code automation (e.g. pattern based integration)
• Experience in advanced SQL and PL/SQL techniques
Experience in building re-usable Utility packages
Experience with testing the code
Experience in Unix shell and Python scripting
Integration design & data modeling skills in Data lake and Data Warehousing environments
Exposure to both on-prem and cloud Integration solutions
Familiarity with non-relational DB technologies is a plus
Experience with automated testing
Experience with both batch and real-time patterns for integrations
Ability to build and analyze complex integration workflows from heterogeneous data sources
Experienced in large Enterprise Data Warehouse & Integration projects.
Strong background in full lifecycle development using multiple platforms or languages.
Ability to interact at a technical and non-technical level with Infrastructure, Network, Development, BA and QA teams.
Development experience in high transaction/high availability systems.
Experience with analyzing and recommending solutions for Production issues short-term and long term
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience:
Informatica Power Center: 1 year (Preferred)
SQL: 1 year (Preferred)
GCP: 1 year (Preferred)
BigQuery: 1 year (Required)
Python: 1 year (Preferred)
Work Location: In person
Show Less
Report",3.9,501 to 1000 Employees,1996,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
Remote Data Engineer,$120K - $140K (Employer est.),LookFar Labs,United States,"Remote AWS Data Engineer
We have an existing commercial SaaS platform that consists of 3 components: a web application, several 3rd party databases integrated into our backend, and a Natural Language Processing ML model based on a custom taxonomy.
We are looking to build 2.0 of our platform, with a brand new front end based on new algorithms, and scalable data science models that use a confluence of data from various data sources (e.g., patent, financial, and people). Its a challenge and a fun opportunity for someone looking to make the next big platform that the world is going to use.
Our Data Engineer would need to create a new data pipeline, ETL process, and architecture for 2.0 of our platform. This could include multi-modal databases, and should consider the delineation between production, development, and staging/testing data pipelines and environments.
The data pipeline should easily integrate new data sources, with both structured and unstructured data, and should enable associations between data as well. It should also enable and further enhance the strong entity resolution that we have already started building for our disparate, large data sets to be cleanly integrated.
You should also not rely solely on off the shelf tools or default pipelines. This role will require creativity and customization.
Your solutions should keep in mind scalability, to enable optimized usage of distributed computing frameworks like Spark. You should also have strong familiarity and experience with how to leverage the AWS ecosystem to bring in relevant AWS tools, services, and resources to enable substantial processing of very large datasets before runtime, entity resolution between very large datasets, and real-time processing in a scalable, distributed computing environment.

Role Responsibilities:
Create and maintain a scalable ETL data pipeline that ingests multiple large data sets of both structured data (in the form of financial and patent data) andunstructured data (in the form of white papers, scraped websites, etc.), and enablesentity resolution and other transformations for clean data integration and usage
Create and maintain a multi-modal data storage system that enables scalable, real-time processing for production-level data
Work with the data science team to enable ML Ops
Have curiosity and passion for data, and demonstrate strong and extensiveunderstanding of our data, including ability to efficiently query and obtain data viaSQL
Demonstrate a strong sense of ownership, of both technical and business outcomes
Assist dev and data science teams with processing and integrating data analysis
Clearly document processes, methodologies, and tools usedExperience

Required:
B.S. in relevant technical degree
Significant use and experience (at least 3-5 years) as a data engineer in the AWS ecosystem, including strong familiarity with structured and unstructured large datasets, enabling scalable and distributed compute, and ensuring real-time processingat scale
Significant use and experience (at least 3-5 years) with writing complex SQL queries and analysis of data correlations
Significant experience (at least 3-5 years) with the AWS ecosystem, including tools, services, and resources that enable scalable, distributed computing
Project management skills, ability to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform
Excellent communication and story-telling skills (written and verbal)

Our Current Tech Stack:
AWS to host the infrastructure, including the CICD, SpringBoot, Angular, Python, PySpark, Kubernetes, EMR, Spark, Elasticsearch, RedShift, AWS (S3, Code Commit, Code Build, Code Deploy, EC2, EMR, etc.), Docker, Spacy, Scikit learn, Openpyxl, Streamlit, Watchdog, sklearn, seaborn, nltk, matplotlib, pandas, SQLAlchemy, and additional ML and python libraries. This stack is subject to change as we build v2.0.
We want to modernize and streamline our models, MLOps, code, deployment, front-end, and distributed processing capabilities.
Logistics: Geography, Work Status, Etc.
The position is full-time on a W2 and fully remote. The candidate must have the legal right to work in the United States.

Interview Process:
We will conduct 3 rounds of interviews.
First Round: Culture, fit, and background interview with the Founders
Second Round: Technical Interview
Technical Project: Execute a small data engineering project, if selected for the third round of interview
Third Round: In-Person Day in Washington D.C. (We will have the candidate fly out to D.C. to meet the founders and team.) Present the results of the data engineeringproject during the In-Perso Day. How to Apply: Please provide the following: Resume
Cover Letter
Any links to Git repositories or data engineering projects that we can review

About the Company:
We are the source of truth for patent intelligence. Patents protect revenue and investment in the market. Given that, patent intelligence is not complete UNLESS it integrates financial and market data. We provide SaaS platforms that correlate multiple data sets (patent, financial, and people data) using scalable data science models, in order to answer fundamental questions related to patent and innovation strategy.
We provide patent intelligence to corporate IP departments and the defense sector. We are expanding to a larger commercial market, including technology transfer, venture capital, and financial institutions.
We are committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics.
Job Type: Full-time
Salary: $120,000.00 - $140,000.00 per year

Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance

Experience level:
4 years

Schedule:
Monday to Friday

Application Question(s):
Do you now or in the future need work authorization sponsorship?
If selected for the final interview round, are you able to fly to Washington, DC for an in-person interview (all travel expenses paid)?

Experience:
complex SQL queries and analysis: 3 years (Required)
AWS: 3 years (Required)
ML Ops: 3 years (Required)
building scalable ETL data pipelines: 3 years (Required)
Spark framework: 2 years (Required)
Work Location: Remote
Show Less
Report",-1,Unknown,2014,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
Data Engineer,$72K - $106K (Glassdoor est.),"Naval Systems, Inc.3.9 ★","District Heights, MD","Description: NSI requires a Data Engineer to support the contracted efforts. The Data Engineer will develop and implement a set of techniques and analytics applications to transform raw data into meaningful information using data-oriented programming languages and visualization software. The position is mainly focused on Raw Data File (RDF) smart aircraft data downloaded after an aircraft sortie, then loaded into an information system for analysis. The data engineer will be designing and maintaining the pipelines for loading the data and developing queries/procedures to support analyst requirements. Additional projects will include Condition-Based Maintenance+ and Health Usage and Monitoring System data pipeline engineering and warehousing.

Location: Washington, DC, Norfolk, VA, Philadelphia, PA

Education: Bachelor’s Degree in computer science, engineering, mathematics, statistics, business or a similar field.

Certifications: Current Network+, Security+, or higher as defined by DoD CIO Information Assurance (IA) Certification requirements

Experience: Three (3) years of relevant experience in data using architecture, data engineering, data hub / data warehouse development. Three (3) years of relevant experience in utilizing data management, enterprise repository, data modeling, data quality and data mapping tools. Experience with Databricks required.

Security Clearance: Secret Clearance is Required. Must be U.S. citizen.

Special Notes/Instructions: NSI is a privately held, small but quickly growing company with headquarters in Lexington Park, Maryland within 5 miles of the Patuxent River Naval Air Station. Established in 2004, we are now celebrating 19 years of excellence in providing quality products and services to the Department of Defense. Our benefits package includes medical, dental, vision, Long Term Disability, Life Insurance, Short Term Disability, paid time off, paid holidays, flexible spending account, employee assistance program, tuition assistance program, 401k Plan with company match as well as a fun and enthusiastic work environment!

To Apply: NSI offers a team-oriented work environment and a competitive compensation and employee benefits package. If you have a commitment to excellence and want to join our team of top caliber professionals, we invite you to submit your resume electronically by visiting our careers website at: https://n-s-i.us/careers/apply/.

Quality, Integrity, Teamwork, Success – that's NSI!

NSI is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.

XJ6
Show Less
Report",3.9,201 to 500 Employees,2004,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
Data Engineer I,$77K - $101K (Glassdoor est.),CSX2.9 ★,"Jacksonville, FL","Job Summary
The Data Engineer is responsible for the Cloud and on-prem data creation, maintenance, improvement, cleaning, and manipulation of data in the business’ operational and analytics databases. The Data Engineer works with the applications teams, business partners, and data & analytics team to understand and aid in the implementation of database requirements, analyze performance, and troubleshoot any existing issues. The data engineer maintains the optimal Azure cloud data pipeline architecture, assembles data sets that meets functional / non-functional business requirements and performs optimal extraction, transformation, and loading of data from a variety of data sources using SQL and/or other data technologies. This person will be responsible for delivering cloud analytical tools, solutions, and reporting. Applicants will be required to engage in ongoing background checks throughout the duration of this position with continued passing results.

Primary Activities and Responsibilities
Collaborate with other IT partners within the company.
Interact with internal customers to identify business requirements and develop the best course of actions to meet their needs.
Translate functional requirements into working solutions.
Develop information solutions for data-related problems using a variety of technical and analytical skills.
Provide impact analysis for changes to programs and systems.
Provide production support responsibilities for one or more reporting and analytical solutions.
Collaborate with team members, architects, data scientists and software developers to gather requirements and work towards delivering complete analytics/reporting solutions that help improve organizational decision making.
Provide ad-hoc analyses with large data sets.
Actively engaged with immediate team members to ensure alignment with specified technical deliverables.
Design and build Azure cloud technical solutions for typical business problems based on standards and guidance from more experienced team members.
Assist in the definition of best practices and strategies for the team.
Miscellaneous activities and responsibilities as assigned by manager.
Minimum Qualifications
Bachelor's Degree/4-year Degree
Equivalent Minimum Qualifications
High School Diploma/GED
2 or more years of experience in required in Software Application Design and Development
Preferred Qualifications
Graduate Degree
2 or more years of experience in Transportation/Intermodal Operations, Information Systems, or Logistics preferred.
Knowledge and Skills
Strong knowledge of Azure cloud services
Knowledge of Agile Scrum methodologies and their application
Knowledge of cloud data governance tools like Azure Purview.
Debugging and problem-solving skills
Knowledge of change management processes and the full Software Development Lifecycle (SDLC) concepts
Meticulous in approach to work and outstanding attention to detail
Experience with Big Data and data processing technologies including Azure Big Analytics services, Spark, Data Lakes, NoSQL, In-memory, SQL, PL/SQL, R, and/or Python.
Time management and organizational planning skills
Ability to handle multiple tasks and priorities.
Willingness to learn new technologies.
Data Analytical skills must know difference between OLTP and analytics.
Excellent customer service skills and strong problem-solving skills.
Experience in translating data into information that delivers business value through creative data processing and visualization techniques (e.g., Tableau)
Ability to communicate and collaborate effectively with team members and peers (both written and orally)
Experience writing SQL Queries for Relational Databases
Understanding of Facts & Dimensions
Analytical and visualization tools (e.g. Tableau, Power BI, SAS, Alteryx) ; Knowledge of reporting technologies (e.g. Business Objects)
Ability to analyze complex issues and to assist in their resolution.
Leadership
The CSX Leadership Model is the foundation of our Talent Strategy and is what drives CSX performance. CSX accordingly selects and develops talent based on each of the following: producing results, creating change, and inspiring excellence.

Job Requirements
May be required to complete a background check.
Work hours may vary in length and schedule (may include a non-standard work week)
Work flexible days/hours as dictated by business needs.
Our Commitment to Operate Safely
Led by our Guiding Principles, CSX is committed to protecting its employees, customers and the communities in which we operate. Therefore, as a condition of employment, successful external candidates for all positions and internal craft employees accepting management positions must be fully vaccinated for COVID-19. Successful candidates will be required to provide proof of full COVID-19 vaccination within three days of the position start date unless approved for a medical or religious accommodation. Should the successful candidate seek a medical or religious accommodation, appropriate documentation must be provided in support of such request.

About CSX
CSX, based in Jacksonville, Florida, is a premier transportation company. It provides rail, intermodal and rail-to-truck transload services and solutions to customers across a broad array of markets, including energy, industrial, construction, agricultural, and consumer products. For nearly 200 years, CSX has played a critical role in the nation's economic expansion and industrial development. Its network connects every major metropolitan area in the eastern United States, where nearly two-thirds of the nation's population resides. It also links more than 230 short-line railroads and more than 70 ocean, river and lake ports with major population centers and farming towns alike. More information about CSX Corporation and its subsidiaries is available at www.csx.com. Like us on Facebook and follow us on Twitter .

Closing Statement
At CSX, two of our five Guiding Principles are Valuing and Developing Employees as well as Operating Safely. We are committed to offering our team members the most competitive compensation and benefits package available, unlimited opportunities for development and growth throughout an exciting and rewarding career, and the safest work environment possible.

CSX is an Equal Opportunity Employer Veterans/Disabled. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Click here to view the EEO is Law poster. Click here to view the OFCCP pay transparency provision information.

CSX Transportation and its subsidiaries are not seeking outside assistance or accepting unsolicited resumes from staffing agencies or search firms for employment or contractor opportunities. Any resumes submitted by an outside vendor to any employee at CSX via e-mail, Internet, or directly to hiring managers without a valid written search agreement in place with the Talent Acquisition / HR department will be deemed the sole property of CSX. No placement fee will be paid in the event a candidate is hired as a result of the referral, or through other means.
Apply Now: click Apply Now
Show Less
Report",2.9,10000+ Employees,1978,Company - Public,Rail Transportation,Transportation & Logistics,$10+ billion (USD)
Data Engineer,$105K - $115K (Employer est.),Ipsos3.6 ★,"San Francisco, CA","About The Center for Analytics Excellence (CAE)
The Center for Analytics Excellence is a uniquely positioned group within Ipsos. We leverage the latest data science approaches to develop analytic tools and perform analyses for both clients and internal teams, respond to advanced analytic requests, and consult on a broad range of statistical, data, and research best practices. Working in a collaborative and supportive environment, we seek to expand what is possible in market research with data science.
What makes this role important at Ipsos?
We’re looking for a Data Engineer with a deep interest and expertise in data engineering and the dedication to apply that passion. We need someone with the technical expertise to expand and optimize our existing data pipeline and architecture. The ideal candidate will have experience using cloud-based platforms, like AWS and Google Cloud Services, to build and maintain serverless data pipelines to efficiently process and warehouse large scale datasets. The role also requires conscientious attention to detail, an ability to work well on a small team, and a self-starter approach to problem solving and debugging.
What you can expect to be doing:
Design, code, and test cloud-based data pipelines to process and store large-scale datasets.
Develop and maintain data warehouses to efficiently store large, complex datasets in a cost-optimal manner.
Ingest and assemble data from various disparate sources using a variety of tools, including SQL and Python.
Work with internal/external stakeholders to identify and understand needs and project goals.
Lead the migration of development code into production, providing technical assistance and guidance to refine and optimize processes.
Troubleshoot and improve existing infrastructure and codebases.
This might be the job for you if you have:
Bachelor’s degree (or equivalent) in statistics, computer science, or related field
Strong technical communication skills
Proficiency in Python & advanced working knowledge of SQL.
Experience with collaboration tools (e.g. Atlassian suite) and version control systems (e.g. Git).
Experience with cloud services such as AWS/Google
Comfortable working in a highly collaborative, consensus-oriented environment.
Prior success designing and deploying data pipelines in large-scale production environments.
Strong eye for detail, ability to communicate scientific concepts both in written form and verbally, and excellent time-management skills.
Large dataset manipulation. Experience in distributed storage and computing.
Experience with Linux server and system administration.
If you don’t meet 100% of the requirements, we encourage all who feel they might be a fit for the opportunity to apply. We may consider a variety of backgrounds for a particular role and are also committed to considering candidates for available positions throughout our organization, not just the one you’re applying to!
In accordance with NY/CO/CA/WA law, the estimated base salary range for this role is $105,000 to $115,000. Your final base salary will be determined based on several non-discriminatory factors which may include but are not limited to location, work experience, skills, knowledge, education and/or certifications.
What’s in it for you:
At Ipsos you’ll experience opportunities for Career Development, an exceptional benefits package (including generous PTO, healthcare plans, wellness benefits), a flexible workplace policy, and a strong collaborative culture.
To find out more about all the great reasons to work at Ipsos, how we’re making an impact around the world, and more about our benefits and employee programs, please visit: Why Work at Ipsos | US
Commitment to Diversity
Ipsos recognizes the necessity of building an inclusive culture that values each employee’s individuality and diverse perspectives. For more than 40 years, our mission has been to generate and analyze data about society, markets, brands, and behaviors to provide our clients with the insights that elevate their understanding of the world. This could not be fulfilled without Ipsos’ diverse employees who compile and analyze this data—they are the essence of who we are and what we do.
We are committed to providing equal opportunity to all employees, creating an environment that promotes inclusion, and enabling employees from all walks of life to flourish. Ipsos encourages our employees to act in a respectful and responsible manner, in line with code of best practices concerning diversity and inclusion, human rights, equality, and civility for every individual.
Ipsos is An Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or any other protected class and will not be discriminated against on the basis of disability.
#LI-AE1 #LI-Remote
Show Less
Report",3.6,10000+ Employees,1975,Company - Public,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
Data Engineer I,$71K - $93K (Glassdoor est.),"Hy-Vee, Inc.3.4 ★","Grimes, IA","At Hy-Vee our people are our strength. We promise “a helpful smile in every aisle” and those smiles can only come from a workforce that is fully engaged and committed to supporting our customers and each other.
Job Description:
Job Title: Data Engineer I
Department: Information Technology
FLSA : Exempt
Position Summary
An entry level professional who assists in development, implementation, and support of data pipelines for a specific area with strong mentorship and guidance.
Core Competencies
Partnerships
Growth mindset
Results oriented
Customer focused
Professionalism
Responsibilities
Design, Create and maintain on premise and cloud based data integration pipelines.
Assemble large, complex data sets that meet functional/non functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Build analytics tools that utilize the data pipeline to provide actionable insights into key business performance metrics.
Research and work on data-related technical issues and support tickets.
Create data pipelines to enable BI, Analytics and Data Science teams that assist them in building and optimizing their systems
Technical/Business Expertise
Basic Understanding of Database Programming Language (T-SQL, SQL, PLSQL, etc.)
Basic understanding of RDMS systems
Basic understanding of ETL Tools
Experience and Education
Bachelor degree preferred, or relevant experience.
Leadership Responsibilities (Direct Reports)
None
Physical Requirements
Visual requirements include: ability to see detail at near range with or without correction.
Must be physically able to perform sedentary work: operating a computer, occasionally lifting or carrying objects of no more than 10 pounds, and occasionally standing or walking.
Must be able to perform the following physical activities: meeting with customers, kneeling, reaching, handling, grasping, feeling, talking, hearing, and repetitive motions.
Working Conditions
The duties for this position are performed in a general or remote office setting. There is weekly pressure to meet deadlines and handle multiple tasks in a day.
Equipment Used to Perform Job
Laptop and desktop computer, telephone, copier, Fax, printer, PC with Microsoft Office programs and other software relevant to specific position.
Financial Responsibility
None
Contacts
Has frequent contact with office personnel in other departments related to the position as well as occasional contact with users and customers.
Confidentiality
Has access to confidential information.
Are you ready to smile, apply today.
Show Less
Report",3.4,10000+ Employees,1930,Company - Private,Grocery Stores,Retail & Wholesale,$5 to $10 billion (USD)
Data Engineer,$80.00 - $85.00 Per Hour (Employer est.),"3G Federal Solutions, LLC",Remote,"Re-opening for additional resume submissions. Minimum requirements HM is looking for:
1. Hands-on data engineering skills using Python and related programming skills
2. Demonstrated data transformation skills converting variety of unstructured data to structured data for analytics including experience in parser development, multi-modality extraction capabilities
3. Must have Data Engineering/solutioning experience in designing, developing, and implementing cloud-based data analytical solutions using more than 2 of the following Azure Products:
Azure Data Factory, Azure Analysis Services, Azure DataBricks (PySpark), Azure Machine Learning, Azure Storage/Data Lakes
4. Traditional BI experience in SQL Server Stack including writing complex T-SQL stored procedures; use of SSIS to develop and deploy complex SSIS Packages across multiple-environments; data migration experience
5. Data modeling and visualization experience using Power BI /DAX programming experience is highly preferred
Data Engineer Role
We are looking for Data Engineers to support operationalizing Security Governance and Risk (SG&R) Division`s Data Analytics Programs. The ideal candidate for this position with source, evaluate, and prepare data assets for easier access and use by data consumers and stakeholders. The person hired for this role will be part of a team that will help implement a modern end-toend data analytics infrastructure that enable data-centric risk decisioning.
Responsibilities
Implement and manage data transformations needed by customer (e.g., Analytics team, Data Scientists, ML Modelers)
Closely work with Data Architect(s) to build functional capabilities and analytical tools and needed to realize data architecture goals
Improve/automate internal processes for optimal data delivery and mature infrastructure for greater scalability and functionality
Confirm all data pipelines and data flows are operational and well-functioning
Implement data quality checks to maintain data integrity and monitor for high data quality
Provide support to assigned department; may work on small projects or portions of larger projects
Support business strategy and initiatives within the business unit and/or across the organization
Assist and collaborate with various levels of staff to accomplish tasks/assignments
Perform research and analysis; may include quantitative and/or qualitative analysis
Maintain professionalism in all interactions, placing member service as priority
Support comprehensive day-to-day
Qualifications
Must have a bachelor’s degree in Information Science, Data Analytics, Information Management, or Computer Science or 5+ years of experience in a directly related role with progressive growth
Proven experience as a data engineer with experiences in designing and building analytical solutions. Specifically, handson experience in setting up data pipelines for data science and machine learning projects.
Proficiency with Python in a data engineering context. E.g., Pandas, PySpark, PyTorch
Experience in cloud-based data products and solutions such as: Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service
Exposure to data/collaboration tools such as MS SharePoint, MS Power BI, MS Power Platform, Service Now, GRC applications, and related Azure Cloud Technology/Products
Significant experience in handling unstructured datasets and converting them into Structured datasets
Effective interpersonal, verbal, and written communication skills
Effective research, analytical, and problem-solving skills
Effective skill maintaining accuracy with attention to detail and meeting deadlines
Provide exceptional Customer Service support
Preferred Qualifications
Experience in designing and hands-on development in cloud-based analytics solutions
Proficiency with Python in a data engineering context. E.g., Pandas, PySpark, PyTorch
Experience with multiple programming languages, including Python, R, MATLAB, or SAS • Hands-on experience in modern Data Platforms related technologies such as Azure Synapse, Azure SQL Azure Databricks and Power BI (including using DAX) is desirable
Strong knowledge of Microsoft BI Stack (SSRS/SSAS (Tabular with DAX & OLAP with MDX) SSIS)
Strong understanding of NoSQL implementation (Mongo, Cosmos DB or other similar tools)
Designing and building of data pipelines using API ingestion and other ingestion methods (batch and real-time)
Knowledge of Dev-Ops processes (including CI/CD) and use of source control software such as Git or GitHub is essential
Experience in metadata management and data governance tools and processes (e.g., Microsoft Purview, Collibra, Informatica, Talend) is desirable
Strong experience in common data warehouse modelling principles including Kimball, Inmon is desirable
Ability to easily converse with Data Scientists, ML Modelers, and Data Engineers on various data analytical concepts and statistical modeling technique (e.g., regression, logistic regression, log-linear regression) is desirable
Strong academic background or experiences in Security-related domain and/or banking Industry is desirable
Excellent knowledge and passion for cloud computing technologies and current computing trends
Job Type: Contract
Pay: $80.00 - $85.00 per hour
Experience level:
10 years
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",-1,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
Azure Data Engineers / SQL Developers,$60.00 - $70.00 Per Hour (Employer est.),Tuwa Solutions,"Long Island City, NY","Job Title: Azure Data Engineers / SQL Developers
Location: 42-09 28th Street Long Island City,NY,11101
Duration: 48 Months
Qualifications and Requirements:
· 5+ years of experience with Azure data platform including Azure SQL, Azure Data Factory, Azure Databricks, Azure SQL Data Warehouse (Synapse Analytics), Azure Data Lake Storage, Azure Cosmos DB, SQL Server, SSIS, SSRS, etc.
· Strong Object relational mapping experience with UML modeling and OO modeling.
· Experience with T-SQL, SSIS, Power BI and Azure Analysis Services
· Experience with developing data pipelines using python.
· Strong problem solving and analytical skills.
· Passion for public health and solving problems and creating new insights through data.
· Experience working with Healthcare data and proven track of implementation experience in EDI data formats.
· Experience in job roles involving metadata management, relational dimensional modeling and big data solution approaches with native Azure Data Platform tools or 1st party services.
· Excellent communication, presentation skills with the right attitude towards problem solving in diverse teams.
· Strong aptitude and technical skills with conceptual strength in logical solutions driven towards balanced optimal considerations.
· Solid knowledge in SQL, security standards, BI tools, ETL tools and MS Azure specific technologies.
· Certifications in Azure, data modeling and metadata management.
· Ability to use BI tools like Power BI to represent insights and other presentation techniques.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Experience level:
7 years
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Long Island City, NY 11101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure data platform: 6 years (Preferred)
UML modeling and OO modeling: 6 years (Preferred)
Healthcare data: 4 years (Preferred)
Work Location: In person
Show Less
Report",-1,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
AWS Data Engineer,$109K - $131K (Employer est.),Techflairs4.5 ★,"Washington, DC","Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past five years
AWS Solutions Architect – Professional certification within past five years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
Specific experience designing, executing and supporting AWS data lakes at scale
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
Running production workload on AWS cloud
Workload migration to AWS cloud
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coach colleague. Seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Contract, Temporary
Pay: $108,665.12 - $130,865.52 per year
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Experience:
designing, optimizing, and maintaining relational: 3 years (Required)
design and building of ETL packages, data pipelines: 2 years (Required)
Running production workload on AWS cloud: 3 years (Required)
Workload migration to AWS cloud: 2 years (Required)
Work Location: In person
Show Less
Report",4.5,1 to 50 Employees,2002,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
"Software Engineer, Data Platform",$226K - $275K (Employer est.),"Grammarly, Inc.4.5 ★",United States,"Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Every day, tens of millions of people and 50,000 professional teams worldwide trust Grammarly's AI and human expertise to help ideate, compose, revise, and comprehend communications. Our team members have the autonomy to take on exciting challenges in pursuit of our mission to improve lives by improving communication. Together, we're building on more than a decade of steady growth and profitability. We're defining the communication assistance category with our tailored service offerings: Grammarly Free, Grammarly Premium, Grammarly Business, and Grammarly for Education. Our latest product offering, GrammarlyGO, brings the power of generative AI to our users. It all begins with our team collaborating in an inclusive, values-driven, and learning-oriented environment.
To achieve our ambitious goals, we’re looking for a Software Engineer to join our Data Engineering Platform team. This person will build back-end systems to enable data management and create full-stack software tools to help data engineers and end users work with data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure.
Your impact
As a Software Engineer on our Data Engineering Platform team, you will:
Install, provision, and manage data pipeline orchestration software as a service for all of Grammarly to use.
Install, provision, and manage container management software that will enable seamless scaling as our data volume grows.
Create alerting and monitoring services that allow engineers to accurately and efficiently debug and resolve failures in real time.
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re looking for someone who
.
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries.
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation and benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. . If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States:
United States:
Zone 1: $226,000 - $275,000/year (USD)
Zone 2: $203,000 – $245,000/year (USD)
Zone 3: $192,000 – $225,000/year (USD)
Zone 4: $181,000 – $215,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
#LI-DT1
#LI-Hybrid
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
To apply to this job, click Apply Now
Show Less
Report",4.5,501 to 1000 Employees,2009,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Data Engineer,$87K - $129K (Glassdoor est.),N J Malin & Associates3.7 ★,"Addison, TX","Data Engineer Duties and Responsibilities
Assemble large, complex sets of data that meet non-functional and functional business requirements
Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues
Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
Skills and Qualifications
Ability to build and optimize data sets, ‘big data' data pipelines and architectures
Ability to understand and build complex data models for Power BI reporting.
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Must Have experience in (in order of importance)
10 + years experience as a Data Engineer or similar role.
SQL Server
T-SQL code
Administrator functions
Data Factory
Python
Power BI
DAX code
M code
Azure
Malin is an Equal Opportunity Employer - M/F/Veteran/Disability/Sexual Orientation/Gender Identity
Show Less
Report",3.7,501 to 1000 Employees,1971,Company - Private,Shipping & Trucking,Transportation & Logistics,$100 to $500 million (USD)
Senior Data Engineer (Azure),$93K - $135K (Glassdoor est.),Momentum Consulting4.4 ★,"Miami, FL","Momentum takes pride in establishing a team of highly skilled professionals to deliver IT consulting services to our Fortune 500 clients. We provide successful technology solutions and solve critical business challenges to meet our client’s needs. Our alliances with industry-leading technology organizations have been instrumental to our success and allow us to offer robust solutions that are highly scalable and supportable using the best technologies available.

We are seeking to hire a seasoned Senior Data Engineer to join our team. To succeed in this data engineering position, the cloud engineer should have strong analytical skills and the ability to combine data from different sources. You will use various methods to develop raw data into useful data systems. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods. Overall, you’ll strive for efficiency by aligning data systems with business goals.

About the job:
Responsible for the development and management of a Microsoft Azure cloud data platform leveraging Azure Data Lake Storage Gen2, Azure Synapse Analytics, and Azure Data Factory
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
Work with data and analytics experts to strive for greater functionality in our data systems.

About You:
Bachelor’s Degree in Computer Science or a related discipline
Minimum of 5-7 years of data warehousing, data lifecycle management, and computer programming experience
MS Azure: Azure Data Lake Store, Azure Synapse
Azure Data Factory/SSIS or related tool
Business visualization tools, Semantic Layer – Views and Dimensional Modeling – Power BI knowledge is a plus
Experience in database design and data modeling
Experience with structured query language (SQL)
Strong learning orientation and curiosity; comfortable learning new systems/software applications
Strong analytical thinking skills and problem-solving skills
Solid written and oral communication skills
Strong proficiency in Python with an emphasis in building data pipelines
Knowledge of scheduling, logging, monitoring and alert frameworks
Knowledge of Data infrastructure

About us:
Recently honored for the eleventh consecutive year as one of the “Top 10 Best Places to Work” by the South Florida Business Journal. We offer a great working environment and provide our team members with the opportunity to enhance and expand their professional development. Come join our team and take part in our excellent compensation package that includes a competitive salary, health benefits, and a 401K plan. We look forward to hearing from you!

Momentum Consulting offers a great working environment and provide our team members with the opportunity to enhance and expand their professional development. Come join our team and take part in our excellent compensation package that includes competitive salary, health benefits, and a 401K plan. We look forward to hearing from you!

Momentum Consulting Corp. is an Equal Opportunity Employer.
Show Less
Report",4.4,51 to 200 Employees,1994,Private Practice / Firm,Business Consulting,Management & Consulting,$25 to $100 million (USD)
Senior Data Engineer,$130K - $160K (Employer est.),ReUp Education3.4 ★,Remote,"The Role
We are looking for an experienced data engineer to help manage, utilize, and build upon ReUp’s data infrastructure. This role will be a key member of the team creating and delivering against a data engineering strategy that will support ReUp’s tech-enabled service as we continue to grow. If you are a curious engineer strong in Python, SQL, AWS, and business acumen, then we want to hear from you!
As a Data Engineer, you’ll maintain critical data architecture, process large volumes of student data, and work collaboratively to create useful reports, dashboards, and data products that support internal teams and university partners. Data is at the heart of everything we do, so we’re always looking to improve these systems to better address a variety of open-ended questions related to student success. Along the way, you’ll help improve inefficiencies, ensure we follow best practices around data, and work on interesting data science projects in areas like predictive analytics and natural language processing. Through this critical role in our data operations, you will better equip us to support our students all the way to graduation.
What we do
Founded in 2015, ReUp Education is the only organization that focuses exclusively on helping colleges and universities engage and re-enroll the more than 40% of US students who have “stopped out” and support them until graduation, through our technology-enabled service. To date, we have re-enrolled nearly 22,500 students, assisted over 8,000 to graduate, and recaptured over $85 million in tuition for our university partners.
What you’ll do
Develop, construct, test and maintain data architectures and pipelines
Build and refine Data Integration Products (such as APIs, integrations, and other data tools)
Leverage (and expand) Python codebase to launch university partnerships, ingest student lists, process partner data, and perform validation
Support data security, compliance, and governance initiatives
Manage (and expand) daily monitoring automation for key systems/processes
Partner with stakeholders (marketing, product, coaching, partner success, finance, etc.) to optimize our systems and develop novel solutions
Contribute to our data strategy to ensure it is aligned with company goals
Manage and setup new ETLs, automated jobs, and workflows
Troubleshoot and resolve issue escalations pertaining to data infrastructure
Protect end users from risk of faulty or inaccurate data
Identify ways to improve data reliability, efficiency, and quality
Collaborate with data scientists to prepare data for machine learning applications (predictive analytics, natural language processing, etc.)
Collaborate with data analysts to create business intelligence assets (analytics reports, dashboards, KPIs, research etc.)
Qualifications
Research shows that women and people from underrepresented groups only apply to jobs if they meet all of the qualifications. However, no one ever meets 100% of the qualifications. ReUp encourages you to break that statistic and to apply. We look forward to your application.
10+ years of related work experience in data engineering required
Experience building and maintaining APIs required
Strong Python and SQL skills required
Experience building within Amazon Web Services (certification a plus)
Experience with Airflow and other workflow management tools
Experience with large scale production environments
Experience with modern development processes and practices
Experience with machine learning technologies
Tableau / Mode Analytics / BI experience a plus
Salesforce CRM experience a plus
Certification in CISA, CISSP or similar data security credential a plus
Higher ed, ed tech, and/or startup experience a plus
Bachelor’s or Master’s degree in Engineering, Computer Science, Data Science, or other quantitative focused field of study a plus
You believe in ReUp’s mission and are committed to helping us create new opportunities for millions of stopout students
Additional
You are a first-principle thinker and creative problem solver
Your attention to detail leaves no stone unturned
You are able to communicate complex ideas effectively
You proactively look for ways to automate and optimize tasks
You enjoy learning in a fast-paced environment
You are collaborative and open to feedback
You have a passion to support data driven decisions while recognizing the humans behind the numbers
Compensation & Benefits
Compensation: $130,000-160,000 annual salary commensurate with experience
Medical, dental, and vision insurance for employees
We pay 100% of the employee's premium and 50% of any dependents' premiums
FSA or HSA available
Company paid short term disability, long term disability, and life insurance for employee
Flexible time off and remote work opportunity
15 paid holidays per year (including Juneteenth and the last week of the calendar year)
Company wellness days (2 per year)
Day of Service (Paid day for volunteering)
401(k) plan
Paid parental leave (12 weeks primary parental leave, 6 weeks secondary parental leave)
A diverse team that fosters a high level of collaboration despite being highly distributed
We provide your choice of a Mac or PC laptop

Location
ReUp is a remote organization with a geographically distributed team. This position will be based remotely in one of the states listed:AL, AZ, CA, FL, GA, IL, MA, MI, MO, NC, NH, NY, OH, OR, PA, SC, TN, TX, VA, WA and WI.
Company Culture
TEAMWORK * RESULTS * CONSTANT LEARNING * AGENCY * DIVERSITY, EQUITY & INCLUSION * JOY

ReUp employees share a passion for improving outcomes for stopout students. We support students to get Results as they embark on finishing what they started. We believe in the power of human potential and that supporting an individual’s Agency acts as a catalyst for positive change and resiliency. We support Diversity, Equity & Inclusion, for both the students we work with and in our hiring practices. We value Teamwork and strive to create a safe and supportive environment where trust, communication, creativity, and humility are valued as highly as technical skills. We tackle hard problems with curiosity and take action towards continuous improvement and Constant Learning. Approaching our work with open hearts, open minds, and seeking collective success creates Joy. If that sounds like your dream work environment, we look forward to hearing from you.

ReUp Education is an equal opportunity employer. Our company values diversity and believes diverse teams make innovation possible. We encourage all qualified applicants from any race, color, religion, sex, gender identity, sexual orientation, national origin, disability status, protected veteran status, or other characteristics to apply.

ogDwCwY1YZ
Show Less
Report",3.4,51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Data Engineer/Analyst,-1,DeleteMe,United States,"About DeleteMe, The Online Privacy Company
DeleteMe is an online privacy company that makes easy-to-use tools for consumers and businesses to control what personal information companies, third parties, and other people see about them online.

DeleteMe is a rapidly growing SaaS privacy business operating globally and remotely – we are the emerging leader based on # customers and revenues in a fast-growing nascent category of consumer and enterprise security Privacy Services. What we do - our mission - matters because we are restoring a sense of privacy, fairness, and control of personal data in the possession of others. Easier, simpler, control underpinned by a suite of new data privacy laws being passed worldwide will play a part in greater personal security, and freedom, and in stronger democracies in an era where data collection is at unprecedented levels. This is what our work and brand stand for and we are building a large, sustainable, for-profit business to catalyze this. We have strong B2C and B2B businesses with respective product offerings informed by feedback from an active customer base growing between 30% and 200% year/year. DeleteMe is well-capitalized: profitable for the last three years with an 8-figure balance sheet and large-scale venture firms as investors.

DeleteMe is led by a passionate team, backed by premier investment firms, and supercharged by a strong mission to empower consumers with privacy.

Job Summary
The Data Engineer/Analyst will help us manage and use data to make good decisions about what we build, how we help our customers, and how we grow our company. The ideal candidate combines the ability to operate and evolve the data pipelines as well as perform data analysis tasks and will work closely with our engineering and product teams to improve the way we collect structure, and store data. Work with our management, marketing, sales, and service teams to understand what they need and drive the requirement process peaking their domain language to get them the data they need the way they want to empower them to make good decisions.

We are building our data practice from the ground up so you will have the opportunity to work on a new modern data engineering tech stack. We use Snowflake as our Data Warehouse and Fivetran as our ETL tool. We use DBT to extract, transform, and load our data and enable CI/CD capabilities. We are big Open Source users and use Apache Superset as our reporting tool. Reverse ETL is performed using Hightouch and components of Rudderstack. Our tech platform includes AWS, RDS, Terraform, Docker, and Kubernetes. Python is planned to become the to-go tech for data analysis and modeling.
We are just getting started, so you will be a key driver in building our data engineering practice from the ground up.

Responsibilities
Maintain and evolve our data pipeline infrastructure; help identify the new/best tools and tech for the job.
Improve our data collection and storage practices to enhance the value of our internal dataBuild pipelines and data flows.
Ensure our data is available where it is needed when it is needed.
Interview business users and document data reporting and analysis requirements, develop ELT, data models, and dashboards.
Design and implement a strategy and systems for data analysis, reporting, querying, and visualization to enable internal and external partners to explore our data, answer their questions, and make good decisions.

Requirements
4+ years of proven working experience in data engineering/warehousing, data analysis, data modeling, and reporting for a business department of a SaaS company, who wants to be a big part of helping a team achieve big things, who believes in privacy as a mission, and who can both operate independently and help a team be more than the sum of its parts.

These are some of the specific characteristics we are looking for
Strong technical and people skills.
Excellent written and verbal communication skills.
Experience with data engineering/data warehousing/analytics engineering.
Experience with building SQL-based data pipelines and data models.
Experience with database design and implementation.
Experience with gathering and documenting reporting requirements.
Experience with data reporting and visualization tools.
Experience with appropriate programming languages and technologies.

Preferred experience with existing toolsets:
Familiarity with Snowflake is a prerequisite/requirement for this position.
Any SQL DB as a primary data source; NoSQL is a plus.
Fivetran, Snowflake, DBT, Hightouch.
Apache Superset, Tableau, or a similar interactive data visualization software.
Basic knowledge of Python, R, or another standard data analysis language/framework.
BS in Computer Science / STEM / Economics.
Ability to thrive in a fast-paced work environment, where change is constant, and flexibility is key.

What We Offer
Comprehensive health benefits flexible schedule. 100% work from home 401k matching. Paid time off. 12 company paid holidays. Gym membership reimbursement. Birthday time off child care expense reimbursement. DeleteMe provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
Show Less
Report",-1,51 to 200 Employees,2009,Company - Public,-1,-1,Unknown / Non-Applicable
DATA ENGINEER,-1,Sonny's Enterprises LLC3.8 ★,United States,"Overview:
The Data Engineer will play a critical role in designing, developing, and maintaining robust data infrastructure and pipelines to support our data-driven initiatives. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities:
Design, develop and implement data packages for data warehousing or applications
Help with the design architecture of data platforms, warehouses and pipelines
Research and develop data related tools to fit within the company needs
Customize and manage integration tools, database and analytical systems
Develop and suggest best practices policies/procedures across the team for all data related processes.
Monitor the overall performance and stability of the data systems
Test and maintain efficiency and reliability of data pipelines and data warehouses
Build the infrastructure and data pipelines required to extract, transform, clean, and move data and metadata to be loaded into a data warehouse, data mart, or operational data store
Create and maintain technical documentation to support the business application and data related processes
Provide technical support and problem resolution for related data service teams
Collect and help mapping the organization’s data landscape to provide decision-makers with strategical insights to made effective decisions
Strong at understanding requirements and transforming business requirements into appropriate technical solutions
Other duties as assigned.
Qualifications:
Bachelor’s degree in Computer Science, IT, or related field/experience
5+ years of Data Engineering experience, working with large-scale data processing and ETL pipelines
Hands-on experience with data models, data management and data quality
Experience with Relational Database Systems, Data Design, RDBMS Concepts, ETL
Experience working with data in cloud environments such as AWS (preferred), Azure, GCP
Data engineering certification (a plus)

Sonny's Enterprises is the world's largest manufacturer of conveyorized car wash equipment, parts, and supplies. We are the industry leader, recognized and awarded by the International Car Wash Association for innovating new technologies to advance the industry with products proudly designed and built in the USA. Our culture thrives on finding new and better ways to accelerate what’s next. We embrace change and the opportunity it produces to maximize the potential of our most valuable resource — our PEOPLE! We invite you to explore our opportunities and grow your career with us.

We offer 100% employer paid medical plan. Other optional benefit programs are available to our employees and their families which include: 401(k) match, additional medical plans, dental, vision, flex spending account, short-term and long-term disability & life insurance coverage.

EEO Statement

Equal Opportunity Employer

Sonny’s is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive considerations for employment without regard to race, color, religion, sex, age, disability, marital status, familial status, sexual orientation, pregnancy, genetic information, gender identity, gender expression, national origin, ancestry, citizenship status, veteran status, and any other legally protected status under federal, state, or local anti-discrimination laws.
Show Less
Report",3.8,501 to 1000 Employees,1949,Company - Private,Consumer Product Manufacturing,Manufacturing,$100 to $500 million (USD)
Azure Data Engineer,$65.00 - $70.00 Per Hour (Employer est.),Apptad Inc4.4 ★,Remote,"Job Title: Azure Data Engineer
Job Location: Remote
Job Duration: Long-Term
Job Description
Snowflake- data masking
Snowflake-security
Snowflake- data ingestion from diff sources
Snowflake- data model, scalability
Fivetran- as orchestration
How to bring data from salesforce to snowflake – there is a new connector in preview , explore more
How to build data mesh architecture.
A strong foundation in designing and building data pipelines, integrating diverse data sources, and implementing efficient data processing frameworks.
Proficiency in programming languages Python, SQL, and used snowflake (data masking, security etc)
ADF for creating data pipelines.
Strong problem-solving abilities, attention to detail in data engineering needs.
*
Job Type: Full-time
Salary: $65.00 - $70.00 per hour
Experience level:
9 years
Experience:
Snowflake: 6 years (Preferred)
Azure: 6 years (Preferred)
Python: 3 years (Preferred)
SQL: 3 years (Preferred)
Work Location: Remote
Speak with the employer
+91 6093639912
Show Less
Report",4.4,201 to 500 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
Data Engineer,$78K - $116K (Glassdoor est.),American Power & Gas2.6 ★,"Largo, FL","Because of expansive growth American Power & Gas is seeking a Data Engineer to add to our technical team. This is a fulltime permanent on-site role.
We have been offering Green Energy solutions to both residential and small commercial customers for over 20 years and have won the award for fastest growing company in the Tampa Bay Business Journal as well as being featured in Forbes and the Huffington Post.
**
**
Key Responsibilities
Support operational executives in solving business problems by designing, developing, troubleshooting, and implementing data driven solutions to complex technical objectives.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Deploy sophisticated analytics programs, machine learning and statistical models to predict business outcomes and continually optimize performance through data science.
Gather and summarize technical requirements associated with strategic business initiatives.
Document, maintain, support and enhance company technology platforms and analytics applications.
Look for opportunities to streamline and automate various reporting processes to help support internal teams.
Assist with the research and development of internal business case studies to support onboarding new data tools.
Extract, manipulate and cleanse raw data from various data sources – call center, web, and CRM systems.
Work independently as well as collaboratively with other team members and key stakeholders as required to troubleshoot and resolve data issues.
Provide comprehensive technical and consultative services to support development and maintenance of internal and third-party platforms.
Create functional test cases/criteria to verify all functionality adheres to specifications and create end user manuals.
Routinely represent the Analytics department in cross-functional status & data strategy meetings.
Assist business analysts in provision of regular performance trends reporting, forecasts, and insights for marketing and sales team leaders as well as senior executive management to maintain a tight finger on the pulse of emerging performance trends and opportunities.
Partner with internal cross functional teams to identify business needs and analytics opportunities, developing tools and techniques to analyze and provide performance-improving recommendations.
Partner with the Operations team to optimize data workflows from sourcing to storage to reporting to deliverables to maximize for value-added and time-efficiency.
Develop, enhance, and manage various analytical solutions in support of business objectives.
Determine what data is needed and how to consume and store this data to support reporting needs and ad hoc performance-improving analysis for internal stakeholders.
Ensure deliverables are adapted properly to stakeholder audience; adapting terminology and visuals as needed to “speak the stakeholder’s language”, thus communicating with maximum effectiveness.
Troubleshoot and QA data, reporting, and tracking anomalies as needed, with proactive communication to stakeholders.
Relentlessly challenge the status quo. Always be critical of how we can be more effective or more efficient as an individual, as a team, and as a business.
Provide ongoing and proactive client service to your internal customers as required to continue elevating the performance of the business.
Regularly work with and analyze data across marketing channels and the customer journey through website analytics, call center activity, and CRM systems.
Requirements
University degree or college graduate in Engineering, Computer Science, Mathematics, Statistics, related technical/programming discipline, or the equivalent hands on experience in Data Engineering or Software Development.
Proficiency with coding in SQL is required. Ability to also write in Python, R, Java, or similar programming languages is preferred.
Strong technical prowess, including an understanding of algorithms, systems architecture and end user experience.
Experience with modern source, build, and deploy tools such as Git, Grub, Maven, Yeoman, etc. is a plus.
Ability to think unconventionally to derive innovative and creative solutions.
Competency in accurately estimating development timelines.
Experience with data warehouse design, relational databases, SQL/NoSQL data modeling, RESTful API standards and large scale data processing solutions.
Demonstrated skill in database development with solid understanding of schema design, stored procedure development, query optimization and ETL processes.
Excellent troubleshooting ability. Must be able to resolve issues tied to capturing and processing data in a timely manner.
Excellent English written and verbal communication skills, especially explaining technical concepts to non-technical business leaders.
Exceptional critical thinking and problem-solving skills; able to distill overall objectives into the actionable steps required to achieve those objectives.
Capable of effectively managing projects, priorities, timelines, and working relationships.
Must possess the intellectual curiosity to succeed in a dynamic, entrepreneurial, fast paced, sales driven organizational culture.
Excited by the opportunity to disrupt the status quo and uncover the eureka moment insights that will take the business to the next level – proactively searching for problems to solve, knowing there is so much to learn.
We offer Health, Dental, Optical and Life Insurance, PTO (paid time off) and the opportunity for promotions and room to advance.
For immediate consideration please send a resume to Carl Schumacher the Manager of Recruiting CarlS@goapg.com
Job Type: Full-time
Show Less
Report",2.6,201 to 500 Employees,2010,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$100 to $500 million (USD)
Sr. Data Engineer:,$50.00 - $60.00 Per Hour (Employer est.),zettalogix.Inc,Remote,"Sr. Data Engineer:
Location: REMOTE
Experience: 13+Years only
Client is looking for only USC and GC candidates
Technical skills: SQL server – versions 2016 and 2019, SSIS – 2015 and 2019, AWS/GCP, Azure Data Factory and Python
Essential Functions
Capable of developing Data Warehouse solutions using cloud-based tools in an Azure (primary) and AWS (secondary) cloud environment
Develop automated processes using ETL tools such as SSIS and Azure Data Factory
Analyze data and create sound database designs using dimensional and relational models as needed
Experience integrating data processes with various web services technologies (RESTful API, SOAP, RPC)
Ability to triage issues and provide 3rd tier technical support in both production and non-production environments to resolution
Capable of communicating issues, developing plans and recommendations for corrective actions to mitigate future recurrences
Desire to become a subject matter expert in the data and processes utilized to maintain the platform with a keen eye towards improving
Design tables, stored procedures, views, and other database objects within SQL Server on prem and in the cloud
Advanced SQL-querying proficiency
Ability to troubleshoot and improve database performance issues including indexing strategies for various workloads
Perform unit testing and prepare data and processes for QA/UAT
Work with team manager to track progress on projects and update tracking systems.
Experience with Git repositories for source control and CI/CD pipelines
Collaborate with members of the BI team and operations to understand business requirements and develop effective solutions.
Special Skills
Strong Enterprise Data Warehousing skills and thorough understanding of methodologies including managing various flavors of slowly changing dimensions
Understanding of ETL Frameworks, utilization of templates, design patterns and metadata to build supportable data integration processes.
Proven experience using Cloud based technologies, especially with regard to ELT software or other similar tools
Must be well organized and able to clearly and concisely present documentation of issues and potential resolutions to technical and non-technical teams
Enthusiasm for exploring new technologies - with an eye on maintainability
Desire to deliver high quality customer service
Ability to adhere to organizational and departmental policies and goals
Excellent problem solving and interpersonal skills
Demonstrate the strong written and verbal communication skills required in a disparate work environment
Required Education and Experience
Bachelor’s degree in Computer Science or similar discipline or equivalent work experience
Experience executing in an Agile environment
Demonstrated expertise delivering technical solutions as per tickets, specified plans and deliverables
Experience developing Data Lakes / Data Warehouses
Exposure to data streaming technologies and methods, ESB, pub/sub models
Preferred Education and Experience
4-6 years’ experience with AWS/Azure
Strong automation and programming skills experience
Demonstrated knowledge of various scripting tools
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
11+ years
Schedule:
Monday to Friday
Experience:
Informatica: 10 years (Preferred)
SQL: 10 years (Preferred)
Data warehouse: 10 years (Preferred)
Work Location: Remote
Show Less
Report",-1,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Sr Cloud Data Engineer,$55.00 - $75.00 Per Hour (Employer est.),Prutech5.0 ★,Remote,"Role: Sr Kubernates/Data Engineer
Location - Remote
Duration: 12 + Months
The skill sets needed are -
Strong cloud experience
Strong devops (Kubernetes, Docker, Terraform etc)
Understanding of distributed data computing platforms - anyone or more - Hadoop, Databricks, Spark, EMR, HDInsight, Snowflake etc.
String Understanding of Authorization models (RBAC, ABAC etc.)
Working knowledge on Ingress Controller and Egress Controller
Key Words :
Kubernetes
Devops Engineers
Data Engineers + K8S
SRE Engineer
AKS, GKS, EKS
RBAC + K8S
Ingress Controller and Egress Controller
Job Type: Contract
Salary: $55.00 - $75.00 per hour
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Show Less
Report",5.0,201 to 500 Employees,1998,Company - Private,Software Development,Information Technology,$5 to $25 million (USD)
Sr. Data Engineer,$84K - $191K (Employer est.),GG tech global,Remote,-1,5.0,-1,-1,-1,-1,-1,-1
Data Engineer,$68K - $96K (Glassdoor est.),Alert Innovation3.8 ★,"Andover, MA","Alert Innovation, now powered by Walmart, is a fast-growing venture on a mission to reinvent retail through robotics. We've partnered with Walmart to develop our Alphabot® technology, which is currently being deployed at stores throughout North America.
We're looking for a Data Scientist / Engineer to join our Performance Intelligence Team. You'll play a hands-on role in the development of the data pipelines, transforming data, and creating dashboards for visualization, making data accessible, scalable, and actionable. The ideal candidate possesses a strong aptitude for data, enjoys problem-solving, and effectively manages priorities.
WHAT YOU'LL DO:
Create data models, reports & analyses to support business needs and convert raw data into meaningful insights through interactive and easy-to-understand dashboards and reports.
Work collaboratively with stakeholders to define internal and external metrics & system requirements.
Translate team's needs into data solutions using data processing software & visualization tools such as Power BI and Tableau.
Recommend a technical approach to data pipeline development and understand the correct schemas to future-proof data pipelines.
Develop custom queries, tables, dashboards, and reports.
Review, improve and optimize existing ETL/SQL queries, dashboard views, and procedures while implementing best practices for data extraction and storage.
Work on implementation of technology needed to facilitate the transfer of data and integrations with internal and third-party applications.
Propose, define, review and test data warehouse modifications to improve report design efficiency.
WHAT WE'RE LOOKING FOR:
8+ years experience working with databases, data warehouses, data lakes, ETL/ELT and reporting & analytic tools.
Hands-on experience in extracting data from various data sources and building data models (Star Schema/Snowflake).
Experience in requirements analysis, design, and prototyping.
Experience in the architecture and development of scale-able production software and or enterprise data solutions.
Experience resolving complex issues creatively and efficiently.
A strong working knowledge of KQL, SQL, Azure ADX, and or relational databases.
Demonstrated proficiency in transferring data between cross-platform applications.
Ability to write clean, readable, and maintainable software with documentation in Java and/or Python (but experience with both not required).
Excellent written & oral communication skills, and strong organizational & planning skills.
Experience with data visualization tools like Power BI or Tableau (3+ years preferred)
Experience with cloud data platforms (experience with Azure is a plus).
Experience with machine learning and or artificial intelligence is a plus.
B.S. in Computer Science, Data Engineering, or related field.
Alert Innovation's recent awards include:
FOR FULL TIME EMPLOYEES, WE OFFER:
Excellent benefits — multiple health plan options, Doctor on Demand service, vision and dental insurance for you and dependents
401(k) match, stock purchase plans, life insurance, disability insurance and more
RSU grants with generous annual refreshers
Unlimited paid time off and flexible work schedules
Up to 12 weeks of paid maternity/paternity leave
Walmart discounts in-store and online
Free lunch, coffee, and snacks provided daily!
Learn more about why we were named a 2022 Best Place to Work at alertinnovation.com/careers.
Alert Innovation is proud to be an Equal Employment Opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
Show Less
Report",3.8,201 to 500 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,$5 to $25 million (USD)
Data Engineer,$120K - $140K (Employer est.),Zelis3.8 ★,"Saint Petersburg, FL","Summary:
· Design, develop and implement scalable batch/real time data pipelines (ETLs) to integrate data from a variety of sources into Data Warehouse and Data Lake
· Design and implement data model changes that align with warehouse dimensional modeling standards.
· Proficient in Data Lake, Data Warehouse Concepts and Dimensional Data Model.
· Responsible for maintenance and support of all database environments, design and develop data pipelines, workflow, ETL solutions on both on-prem and cloud-based environments.
· Design and develop SQL stored procedures, functions, views, and triggers
· Design, code, test, document and troubleshoot deliverables
· Collaborate with others to test and resolve issues with deliverables
· Maintain awareness of and ensure adherence to Zelis standards regarding privacy.
· Create and maintain Design documents, Source to Target mappings, unit test cases, data seeding.
· Ability to perform Data Analysis and Data Quality tests and create audit for the ETLs.
· Perform Continuous Integration and deployment using Azure DevOps and Git
· Experience with Banking and Finance Processes specifically NACHA, BAI2 and Bank 822 Files required.
· This position will support the Payments Advanced Record Keeping Project
Requirements:
· Minimum of 5+ years experience in the following:
o 5+ years data engineering experience to include data analysis
o 2+ years working with an ETL tool (DBT preferred)
o 5+ years programming SQL objects (procedures, triggers, views, functions) in SQL Server. Plus experience optimizing SQL queries a plus
o 2+ years designing and developing Azure/AWS Data Factory Pipelines.
o 2+ years Columnar MPP Cloud data warehouse using Snowflake
· Advanced understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
· Experience designing and implementing Data Warehouse.
· Working Knowledge of Azure/AWS Architecture, Data Lake
Preferred Skills:
· Microsoft BI stack (SSIS/SSRS/SSAS)
· Working knowledge managing data in the Data Lake.
· Business analysis experience to analyze data to write code and drive solutions
· Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence.
· Healthcare and/or Payments experience
Job Type: Full-time
Pay: $120,000.00 - $140,000.00 per year
Schedule:
Monday to Friday
Application Question(s):
Do you have experience designing and developing with Data Factory Pipelines (Azure/AWS)?
How many years of work experience do you have with Snowflake Cloud?
Do you have experience analyzing data to write code and drive solutions?
Experience:
ETL: 2 years (Required)
SQL: 4 years (Required)
Work Location: In person
Show Less
Report",3.8,1001 to 5000 Employees,2016,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
Data Engineer,$130K - $165K (Employer est.),Fabletics4.4 ★,California,"Job Description
How Do You Fit In?
We are currently seeking a highly skilled and motivated Data Engineer to join our dynamic team. If you have a passion for data analysis, possess strong technical skills, and are excited about leveraging data to drive business insights, this position is for you. As a Data Engineer, you will join a tight knit group of key contributors who are actively working together to achieve aggressive goals and meet timelines to drive the business forward.

This position will report to the Manager, Data Engineering (EDW), Data Platforms.

What You Will Do:
Work closely with stakeholders to understand their data needs and provide data-driven solutions.
Design, develop, and maintain scalable data pipelines and ETL processes to collect, process, and analyze large volumes of structured and unstructured data.
Solid understanding and experience in data modeling, including designing and implementing efficient data models.
Build and optimize data models, data warehouses, and data marts to support reporting and analytics needs.
Perform exploratory data analysis to identify trends, patterns, and insights that contribute to business decision-making.
Stay up-to-date with the latest trends, tools, and technologies in data analytics and apply them to enhance data-driven decision-making processes.
Perform code reviews to ensure data engineering best practices, code quality, and maintainability.
Provide production support on a rotating basis to ensure the availability, reliability, and performance of data analytics systems and processes.

What You Can Bring:
Bachelor’s degree in Computer science, Engineering, Mathematics, Statistics, or a related field. A master’s degree is a plus.
4 years of experience in creating and managing data pipelines.
1 year of experience with Python or another scripting language.
Experience in performing code reviews and ensuring adherence to best practices.
Excellent communication and presentation skills, with the ability to effectively convey complex data findings to non-technical stakeholders.
Proven experience in data analytics, data engineering, or a related role.
Strong expertise in SQL, particularly in an analytics/reporting capacity, with significant experience in creating and maintaining reporting processes.
Nice to Have:
Familiarity with traditional data warehousing concepts (e.g., Kimball methodology).
Experience in data engineering on cloud platforms such as BigQuery, Redshift, Snowflake, Teradata, Vertica, etc.
Knowledge of Dbt and/or Airflow for data pipeline management.
Previous experience in e-commerce, retail, or internet industries.
Compensation & Total Rewards:
At TechStyleOS, we believe work and life should fit together! We continue to build a culture of flexibility, to empower you to do your best and put yourself first. Our Total Rewards program rewards employees for their hard work, supporting their health, well-being, families, and ultimately their life journey. Total Rewards at TechStyleOS includes:

Hybrid Work Schedule*
Unlimited Paid Time Off*
Summer Fridays*
Healthcare Plans
Employee Discounts
401k
Annual Bonus Program
Equity Program*
And More

Varied for retail and fulfillment roles

The annual base salary range for this position is from $130,000-$165,000. The range provided includes the base salary that TechStyleOS expects to pay for the role. Offered base salary will be dependent on factors including the scope and complexity of the role, candidate’s related work experience, subject matter expertise and work location.
#LI-GR1
#LI-TechStyleOS
About TechStyleOS
TechStyleOS is the globally integrated Operations and Services provider behind some of the fastest growing online fashion brands in history, including Fabletics, Savage X Fenty, JustFab, ShoeDazzle, and FabKids. With capabilities spanning technology, data science, supply chain management, fulfillment, customer service, and more, we help brands launch, scale and grow—across product categories and geographically. From predictive analytics to data-driven marketing and attribution, our unique approach is powered by our proprietary, end-to-end tech platform that enables the brands we serve to deliver a level of personalization, value, and satisfaction that are unrivaled in the fashion industry.
Fabletics, Inc. is an equal opportunity employer. We recruit, employ, compensate, develop, and promote regardless of race, national origin, religion, sex, sexual orientation, gender identity, age, disability, genetic information, veteran status, and other protected status as required by applicable. At Fabletics, Inc., we champion a vibrant workplace culture that thrives on diversity law and do not tolerate discrimination or harassment. We are one team from many backgrounds, innovating through diversity of individuals, who are driven by passion for creating an inclusive space for all. Fabletics, Inc. will continue to champion a workplace culture that prizes diversity and inclusivity.
We encourage you to apply regardless of meeting all qualifications and/or requirements.
Apply Now: click Apply Now
Show Less
Report",4.4,Unknown,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Azure Data Engineer,$84K - $191K (Employer est.),PSRTEK4.5 ★,"Berwick, ME","Job Title : Azure Data Engineer
Job Type: Contract
Job Description
Atleast 2 years of experience working at onsite closely with client.
Strong exp with Azure Cloud Technologies, including ADF, Databricks, SQL DB, ADLS
Experience with data transformation and manipulation using Azure Databricks
Working knowledge of Azure DevOps CI/CD tools and concepts Azure pipelines, GitHub
Scripting experience with Python and Rest API
Experience in implementing CDC (Change Data Capture) Data Virtualization
Experience in creating base view, Derived View and Data source connections using Denodo
Experience in Denodo caching in snowflake
Experience in creating data models by extracting from various data sources by connecting through JDBC, ODBC and Web API in Denodo Skills Required: Python ADF ADB SQL Denodo Snowflake
Job Type: Contract
Salary: $84,181.34 - $190,840.44 per year
Ability to commute/relocate:
Berwick, ME 03901: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 5 years (Preferred)
SQL: 6 years (Preferred)
Data warehouse: 5 years (Preferred)
Work Location: In person
Speak with the employer
+91 609-934-3291
Show Less
Report",4.5,Unknown,-1,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Data Engineer,$88K - $120K (Glassdoor est.),Praescient Analytics3.6 ★,"Fairfax, VA","Praescient Analytics, LLC is a Veteran-led, certified Woman-Owned Small Business (WOSB) founded in 2011 which specializes in implementing advanced analytics solutions across the defense, intelligence, and law enforcement communities. Praescient has extensive experience designing, developing, and integrating solutions for customers including the US Army, Special Operations Command (SOCOM), US Navy (USN), US Marine Corps (USMC), US Coast Guard (USCG), Department of Justice (DOJ), Drug Enforcement Administration (DEA), and Federal Bureau of Investigation (FBI), Immigration and Customs Enforcement (ICE), intelligence community, and local law enforcement agencies, among others.

Praescient Analytics is in search of a Data Engineer to help develop PRAC's best practices and providing consultative advice across a wide range of big data technologies and data management tools. The Data Engineer will be expected to perform duties such as evaluating the performance of current data integrations, data management tools, and associated processes and procedures within a cloud systems environment. This position is fully remote. US Citizenship and a Public Trust clearance is REQUIRED.

Primary Responsibilities:
Support the Enterprise Data Management team at the PRAC by providing a range of consultative and documentation support in accordance with PRAC priorities.
Evaluate and support data integration and data flow mapping efforts in accordance with enterprise data governance standards.
Work with PRAC technical and non-technical users including data scientists and other data engineers to optimize the data environment for counter fraud analysis.
Conduct evaluations and gap analyses across the enterprise data environment to include cloud architecture, databases, data integrations, security and networking, and provide recommendations and best practices.
Support data driven deliverables such as data dictionaries, data inventories, data modeling diagrams, and a range of other data management artifacts.
Provide input into the data security and privacy as it relates to enterprise data management efforts.
Required Qualifications:
Minimum Bachelor's degree in Data Science, Business Intelligence, Computer Science or related fields, or the equivalent combination of education, professional training, and work experience.
3-5 years of experience working with one or more languages commonly used for data operations including SQL, Python, Scala, and R.
3-5 years of experience of consultative experience in cloud big data analytic environments
Experience working with relational databases such as MS SQL, Oracle, and MySQL.
Desired Qualifications:
Experience with distributed systems utilizing tools such as Apache Hadoop and/or Spark.
Experience with Azure cloud offerings.
Experience with big data analytic tools such as Databricks or Azure Synapse.
Experience working with graph databases such as Neo4j.
Working experience in Agile Scrum environments.
Experience with source control tools such as GitHub or Azure DevOps
If you are a self-starter who is passionate about data analysis and eager to make a meaningful impact, we invite you to apply for this exciting opportunity with Praescient Analytics!

Very competitive salary based on qualifications and experience.
Comprehensive, Company paid healthcare for you (We pay your premiums and deductibles)
401(k) & 4% matching
Travel & performance incentives
26 days of paid time off
$5K annual training allowance
$500 book allowance
Tuition reimbursement program
Praescient Analytics, LLC is a certified Woman-Owned Small Business (WOSB) that was founded in 2011 and led by veterans. The company specializes in providing advanced analytics solutions to the defense, intelligence, and law enforcement sectors. Its expertise has been sought after by numerous clients across six continents, with more than 40 contracts executed. Over 70% of the company's workforce comprises individuals with veterans or intelligence/law enforcement backgrounds. Praescient Analytics is committed to identifying, evaluating, implementing, and improving commercial and proprietary technologies to offer comprehensive solutions that address the specific challenges of its clients. The company has a proven track record of developing and integrating solutions for a diverse range of customers, including the US Army, Special Operations Command (SOCOM), US Navy (USN), US Marine Corps (USMC), US Coast Guard (USCG), Department of Justice (DOJ), Drug Enforcement Administration (DEA), Federal Bureau of Investigation (FBI), Immigration and Customs Enforcement (ICE), intelligence community, and local law enforcement agencies.

Praescient Analytics is committed to providing equal employment opportunities for all. Employees and applicants without regard to race, color, age, religion, sex, sexual orientation, marital status, gender identity, national origin, legally protected physical or mental disability, genetic information, citizenship status, or status in the uniformed services of the United States, status as a disabled veteran or veteran of the Vietnam era, or on any other basis which is protected under applicable law. This includes a commitment to provide a work environment that is free from all forms of illegal harassment including sexual harassment.

This covers all terms and conditions of employment including (but not limited to):

Hiring, placement, promotion, transfer, or demotion of all job classifications;
Recruiting, advertising, or solicitation for employment;
Treatment during employment;
Rates of pay or other forms of compensation;
Benefits;
Selection for training;
Company sponsored social and recreational activities; and
Layoff or termination
Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information.
US Citizenship Required
Praescient Analytics is an Equal Opportunity Employer.
Interested Candidates: Please forward your resume to recruiting@praescientanalytics.com and please visit our website to apply online at www.praescientanalytics.applicantstack.com/x/openings.
Show Less
Report",3.6,51 to 200 Employees,2011,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
Data Engineer,$115K - $135K (Employer est.),Symmetry Lending3.9 ★,"Anaheim, CA","Job Description
This role will play a pivotal role in assisting the IT team with the evolution of the company data architecture. In addition to supporting extensions to the data warehouse design, this role will design and develop ETL required to onboard new types of business data. Finally, this role will also provide support for Analytics assignments as needed.
Responsibilities
Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment.
Designing and building new data pipelines that support business requirements.
Work with IT and other business partners to support the evolution of the enterprise data architecture.
Work with IT to build new ETLs to take data from various operational systems and extend existing data warehouse data model for analytics and reporting.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
Support the development of the company BI infrastructure, including the construction of scalable analytic solutions, and on-premises reporting tools.
Utilize Power BI development to build scalable reporting models to serve BI reporting needs.
Demonstrate the ability to gather requirements, extract and manipulate data residing in multiple disparate databases, and articulate solutions to support the business.
Design, develop, and maintain performant data models in Power BI utilizing best practices.
Serve as a subject matter expert for all data warehouse and enterprise systems
Use new or existing technologies to produce analytics solutions (in the form of excel spreadsheets, dashboards, etc.).
Work with IT and Analytics team to architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.
Gather requirements, assess gaps and build roadmaps to help the analytics driven organization achieve its goals.
Develop data-related Proof of Concepts in order to demonstrate feasibility and value to Symmetry business constituents.
Qualifications / Requirements
Bachelor’s degree in Information Systems, Computer Science, Finance, or similar education from an accredited college
2-3 years Accounting and Finance background preferred
Strong skillset in Microsoft Excel (2-3 years, financial formulas & VBA is a plus) and familiarity with Microsoft Office Productivity Suite (Excel, Word, etc.)
5+years’ experience with advanced SQL concepts and writing SQL statements (SQL Server or similar).
Prefer 2 years’ experience with Microsoft BI Suite, PowerBI, Power Query, PowerPivot, Reporting Service (SSRS), SharePoint
Experience in mentoring other team members in development best practice, and methodologies.
You are passionate about data quality control and know how and where to anticipate potential errors.
Knowledge of the software development lifecycle, agile methodologies, and structured software development methodologies.
Experience performing analysis with large datasets in a cloud-based environment.
Ability to work effectively with stakeholders at all levels within the organization
Strong communication and time management skills and a self-motivated approach
Ability to work independently, detail-oriented, and execution focused
Highly collaborative and team oriented
Tenacious (doesn’t give up easily)
Genuine passion for clean and reliable data
At least 5 years of work experience
About Symmetry
Symmetry Lending, specializes in providing mortgage fulfillment services to include origination, servicing, and capital markets needs to various Lenders across the country with whom we partner. We have offices in Atlanta, GA, Eden Prairie, MN, Denver, CO, Orlando, FL, and Anaheim, CA, and we do business from coast to coast. We take great pride in building a diverse team of motivated professionals that contribute to an exciting work atmosphere. We provide a competitive benefits package including medical, dental, and vision plan options, paid time off, and more.
California Disclosure - Employee Notice at Collection
This disclosure is intended to comply with the California Consumer Privacy Act (CCPA), which gives California residents who are applicants, employees, or contractors of Symmetry Lending (“Symmetry”) the right to know what categories of personal information Symmetry collects about them and the purposes for which Symmetry uses that information. As used in this Privacy Notice, “Personal Information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular individual or household. Personal Information includes, but is not limited to, the categories of personal information identified below if such information identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular individual or household.
The following is a list of the categories of Personal Information that we may collect about consumers:
Identifiers. This may include a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, Social Security number, driver's license number, passport number, or other similar identifiers.
Personal information described in the California Customer Records Statute (Cal. Civ. Code § 1798.80(e)). This may include a name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, or any other financial information, medical information, or health insurance information.
Characteristics of Protected Classification under California or Federal Law. This may include age, race, color, ancestry, national or ethnic origin, citizenship status, religion or belief, marital status, a childbirth or related medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth, and related medical conditions), sexual orientation, veteran or military status.
Biometric information. This may include voice and video recordings.
Sensory data. This may include audio, electronic, visual, or similar information, including photos.
Professional or employment-related information. This may include current or past job history, compensation data, performance evaluations, or employee benefits.
Beneficiaries, dependents, and emergency contact information. This may include the name, gender, phone number, and relationship of beneficiaries, dependents, and emergency contacts.
We may use the categories of Personal Information for the following business or commercial purposes:
To perform background checks necessary to comply with licensing requirements, to perform reference checks, to verify eligibility to work in the United States, for contact purposes, to assess your qualification for employment, to conduct performance evaluations, for payrolls and budgeting purposes, for implementation of employee benefits, for internal organizational purposes to establish proper accommodations for sick time, PTO, leaves of absences, or emergency situations, and to conduct health screenings as allowed by OSHA and the CDC to protect the safety of our employee during pandemic situations.
For internal use, such as tracking access into buildings, timekeeping, activity logs, etc.
To comply with laws and regulations, including but not limited to applicable tax, health and safety, anti-discrimination, immigration, labor and employment, and social welfare laws.
For security or the prevention, detection, or investigation of fraud, suspected or actual illegal activity, violations of company policy or rules, or other misconduct.
To comply with civil, criminal, judicial, or regulatory inquiries, investigations, subpoenas, or summons.
To comply with all licensing requirements necessary for our business operations, including state exam audits.
To exercise or defend the legal rights of Symmetry and its employees, affiliates, customers, contractors, and agents.
To seek advice from lawyers, auditors, or other professional advisors.
If Symmetry uses Personal Information of an applicant, employee, or contractor for a purpose materially different than those disclosed in this notice, Symmetry will notify the employee and obtain explicit consent from the employee to use the Personal Information for this new purpose.
Job Type: Full-time
Pay: $115,000.00 - $135,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Paid time off
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Accounting and Finance: 2 years (Preferred)
advanced SQL concepts and writing SQL statements: 5 years (Required)
Work Location: Hybrid remote in Anaheim, CA 92807
Show Less
Report",3.9,51 to 200 Employees,-1,Company - Public,Banking & Lending,Financial Services,$5 to $25 million (USD)
Data Engineer,$78K - $106K (Glassdoor est.),"World Wide Technology Holding, LLC4.1 ★","Saint Louis, MO","Qualifications:
At least 5 years of experience
Ability to translate a logical data model into a relational or non-relational solution
Hands on experience in setting up end to end cloud based data lakes.
Experience in one scripting language; ie; python, java or scala.
Experience with at least one traditional data warehousing technology (e.g., Teradata, Oracle, SQL Server) and/or modern platforms (e.g., AWS, GCP, Azure)
Expert in one or more of the following ETL tools: SSIS, Azure Data Factory, AWS Glue, Matillion, Talend, Informatica, Fivetran
Experience in SQL scripting, tuning, indexing, partitioning, data access patterns, and scaling strategies

Experience with data integrations and data processing for business intelligence and analytics workloads
Ability to translate complex business problems into data-driven solutions
Experience as a key player in developing an enterprise data warehouse
Ability to identify data quality issues that could affect business outcomes
Flexibility in working across different database technologies and propensity to learn new platforms on-the-fly
Strong interpersonal skills
Team player prepared to lead or support depending on situation
Added bonus experience
Certification in any of the modern ETL tools (e.g. SSIS, Azure Data Factory, AWS Glue, Matillion, Talend, Informatica, Fivetran)
Hands-on ETL development experience with Matillion
The well-being of WWT employees is essential. So, when it comes to our benefits package, WWT has one of the best. We offer the following benefits to all full-time employees:
Health and Wellbeing: Heath, Dental, and Vision Care, Onsite Health Centers, Employee Assistance Program, Wellness program
Financial Benefits: Competitive pay, Profit Sharing, 401k Plan with Company Matching, Life and Disability Insurance, Tuition Reimbursement
Paid Time Off: PTO & Holidays, Parental Leave, Sick Leave, Military Leave, Bereavement
Additional Perks: Nursing Mothers Benefits, Voluntary Legal, Pet Insurance, Employee Discount Program
Diversity, Equity, and Inclusion is more than a commitment at WWT - it is the foundation of what we do. Through diverse networks and pipelines, we have a clear vision: to create a Great Place to Work for All. We believe inclusion includes U. Be who U are at WWT!
Equal Opportunity Employer Minorities/Women/Veterans/Individuals with Disabilities
Locations Include: MO, IL, GA, TX, FL, SC, KY, TN, KS and potentially others
Some WWT customers have a COVID-19 vaccine requirement. In order to work on projects for these customers, employees must be fully vaccinated or have an appropriate religious or medical accommodation.
Requirements:
Why WWT?
Fueled by creativity and ideation, World Wide Technology strives to accelerate our growth and nurture future innovation. From our world class culture, to our generous benefits, to developing cutting edge technology solutions, WWT constantly works towards its mission of creating a profitable growth company that is a great place to work. We encourage our employees to embrace collaboration, get creative and think outside the box when it comes to delivering some of the most advanced technology solutions for our customers.
At a glance, WWT was founded in 1990 in St. Louis, Missouri. We employ over 9,000 individuals and closed nearly $17 Billion in revenue. We have an inclusive culture and believe our core values are the key to company and employee success. WWT is proud to announce that it has been named on the FORTUNE ""100 Best Places to Work For®"" list for the 12th consecutive year!
Want to work with highly motivated individuals that come together to form high performance team? Come join WWT today!
What is the Solutions Consulting & Engineering (SC&E) Team and why join?
Solutions Consulting & Engineering is an organization that is Customer Focused and Solutions Led. We deliver end-to-end (E2E) and emerging solutions to drive customer satisfaction, increase profitability and growth. Our success is enabled by our world-class management consulting, delivery excellence and engineering brilliance. We embody the OneWWT mindset by bringing the right talent at the right time from anywhere within WWT to solve our customer's problems. Our goal is to bring together business acumen with full-stack technical know-how to develop innovative solutions for our clients' most complex challenges.
What will you be doing?
Role Overview: The Data Engineer role requires experience in both traditional warehousing technologies (e.g. Teradata, Oracle, SQL Server) and modern database/data warehouse technologies (e.g., AWS Redshift, Azure Synapse, Google Big Query, Snowflake), as well as expertise in ETL tools and frameworks (e.g. SSIS, Azure Data Factory, AWS Glue, Matillion, Talend), with a focus on how these technologies affect business outcomes. This person should have experience with both on-premise and cloud deployments of these technologies and in transforming data to adhere to logical and physical data models, data architectures, and engineering a dataflow to meet business needs. In this role, the Data Engineer will support engagements such as data lake design, data management, migrations of data warehouses to the cloud, and database security models, and ideally should have experience in a large enterprise in these areas.
Key Responsibilities:
Develop high performance distributed data warehouses, distributed analytic systems and cloud architectures
Participate in developing relational and non-relational data models designed for optimal storage and retrieval
Develop, test, and debug batch and streaming data pipelines (ETL/ELT) to populate databases and object stores from multiple data sources using a variety of scripting languages; provide recommendations to improve data reliability, efficiency and quality
Work along-side data scientists, supporting the development of high-performance algorithms, models and prototypes
Implement data quality metrics, standards, guidelines; automate data quality checks / routines as part of data processing frameworks; validate flow of information
Ensure that Data Warehousing and Big Data systems meet business requirements and industry practices including but not limited to automation of system builds, security requirements, performance requirements and logging/monitoring requirements
Apply Now: click Easy Apply
Show Less
Report",4.1,5001 to 10000 Employees,1990,Company - Private,Computer Hardware Development,Information Technology,$10+ billion (USD)
"Senior / Principal Software Engineer, Data Platform",$140K - $190K (Employer est.),NewLimit,"San Francisco, CA",-1,4.1,-1,-1,-1,-1,-1,-1
Data Science Platform Engineer,-1,"Iodine Software, LLC4.3 ★",Remote,"Data Science Platform Engineer

Company Overview
Iodine is an enterprise AI company that is championing a radical rethink of how to create value for healthcare professionals, leaders, and their organizations: automating complex clinical tasks, generating insights and empowering intelligent care. Powered by the largest set of clinical data and use cases available, our groundbreaking clinical machine-learning engine, Cognitive ML, constantly ingests the patient record to generate real-time, highly focused, predictive insights that clinicians and hospital administrators can leverage to dramatically augment the management of care delivery.

What You’ll Do
Iodine thrives on innovation. We are looking for a Data Science Platform Engineer that will help us build out the platform that supports the training and application of our predictive models and performs deep analysis to extract machine consumable meaning from unstructured clinical documentation. You will be joining a small team that has become one of the top players in our field in just two years. Because we work on the cutting edge of a lot of technologies, we need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively. You will be accessing our enormous amount of data to help drive our future innovation.
Work with the rest of the data platform team to design, implement, and test our high-throughput, event-based data pipeline for real-time data processing
Design and build tools and frameworks to develop and enhance our predictive model suite and scalably support real-time predictions in production
Design and implement highly performing and highly scalable applications
Work with other team members to develop a complete and integrated solution
Deliver quality software and documentation on time
Understand and comply with development standards to ensure consistency across the larger development team
Conduct in-depth technical and performance analyses in support of production issue troubleshooting
Monitor and maintain production systems

What You'll Need

Minimum Requirements (Education, certifications and experience):
Bachelor’s degree in Computer Science or related area
3+ years of professional experience writing Java or Python code
Intermediate proficiency with SQL
Experience with CI/CD and automated testing
Comfort working in a Linux environment
Passion for exploring, applying and following the evolution of cutting edge technologies related to AI, machine learning, NLP and large scale data processing
Professional experience with Docker, Kubernetes, relational databases (PostgreSQL preferred), REST API design, and microservices application architectures
Experience with public cloud solutions, such as AWS or Azure
Team player DNA with a positive, self-starter attitude
Attention to detail, highly organized, with an absolute focus on quality of work

Preferred Requirements:
Professional experience with MLOps, Kafka
Familiarity with ClearML, Triton, PyTorch, and TensorFlow
Familiarity with statistics and healthcare domain
Proven expertise in successful large project/build management and execution
Demonstrated ability to manage multiple work streams simultaneously and efficiently

Why should you join Iodine?
This is a unique opportunity to join a close-knit, rapidly growing team and help us improve a key piece of the organization. You will have the opportunity to drive smarter healthcare processes through technology, so hospitals can stay focused on patient care. You will join a passionate and ambitious team, with a proven record of success building multiple companies. Learn more about our company culture on Built In Austin and on our website at www.iodinesoftware.com.

+++++

At Iodine Software, we don’t just accept DIFFERENCE — we celebrate it, we support it, and we flourish on it for the benefit of our employees, our products, and our community. We pride ourselves to be an equal opportunity workplace and are an affirmative action employer.

**You must be currently authorized to work full-time in the United States**
Show Less
Report",4.3,51 to 200 Employees,2010,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
Azure Data Engineer,$100K - $104K (Employer est.),ProIT Inc.4.9 ★,"Bellevue, WA","Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004
Show Less
Report",4.9,51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Senior Data Engineer,$107K - $179K (Employer est.),Wizards of the Coast3.2 ★,"Renton, WA","At Wizards of the Coast, we connect people around the world through play and imagination. From our genre-defining games like Magic: The Gathering® and Dungeons & Dragons® to our growing multiverse, we continue to innovate and build new ways to foster friendship and connection. That's where you come in!
As a Senior Data Engineer for Wizards of the Coast, you will be a technical specialist for our scalable cloud data platform! You will work with a team of engineers in an agile work environment to design and develop data pipelines, tools and infrastructure used by our games and products so that they can handle the volume, variety, and velocity of message types and traffic flowing through it.
You'll also be working to deliver performance optimization, virtualization, and other value adds to our data pipes and ecosystem. You will work with a diverse set of product and engineering teams to design and build innovative data systems that exceed industry standards and provide our analysts, data scientists, marketers, and business teams with clear, accurate, and actionable insights.
What You'll Do:
Help design highly scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of outstanding proprietary and open-source platforms and tools.
Collaborate with engineers, producers, business partners and analysts to elicit, translate, and prescribe requirements for their needs.
Work on technical projects to simplify tough problems, touch many moving parts across functional teams and business domains and influence strategic direction.
Stay ahead of the technology curve and make recommendations about technologies to build current and future products.
Supports the progression of technology solutions from conception through release and into live ops.
Follow and Implement processes, procedures, and documentation standards to ensure compliance with internal Technology and Corporate IT regulations.
What You'll Bring:
3+ years of experience with Cloud data Warehouses such as Synapse, Redshift, or Snowflake.
2+ years of experience working with Cloud Technologies.
Proficiency programming in python and SQL.
Experience with data architecture and data modeling.
Excellent problem-solving and troubleshooting skills.
Experience building scalable data pipelines and ETL/ELT processes.
Experience working in a DevOps environment responding to a system with 24x7 uptime.
Experience with data streaming management.
Experience integrating with third-party APIs.
Nice to Have:
Experience with data pipeline integration tools such as Airflow, or other standalone or integrated pipeline toolsets
Familiarity with Kubernetes or other container orchestration.
Experience working with AWS.
Experience with KAFKA.
We are an Equal Opportunity / Affirmative Action Employer
The above is intended to describe the general content of and the requirements for satisfactory performance in this position. It is not to be construed as an exhaustive statement of the duties, responsibilities, or requirements of the position.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. If you are selected to move forward in our application process and need to request an accommodation, please let your recruiter or coordinator know.
In compliance with local law, we are disclosing the compensation range for this role. The range listed is just one component of Wizards of the Coast’s total compensation package for employees. Employees may also be eligible for annual and long-term incentives. In addition, Wizards of the Coast provides a variety of benefits to employees. Here’s a look at what your benefits package may include: Medical, Dental & Vision Insurance, Paid Vacation Time & Holidays, Generous 401(k) match, Paid Parental Leave, Volunteer Program, Employee Giving & Matching Gifts Programs, Tuition Reimbursement, Product Discounts, and more.
Compensation Range
$106,560—$178,800 USD
Show Less
Report",3.2,1001 to 5000 Employees,1999,Subsidiary or Business Segment,Culture & Entertainment,"Arts, Entertainment & Recreation",$1 to $5 billion (USD)
Sr Data Engineer (10+ Years experience),$70.00 - $80.00 Per Hour (Employer est.),Violet ink4.0 ★,"Dallas, TX",-1,3.2,-1,-1,-1,-1,-1,-1
Sr. Data Engineer,$95K - $115K (Employer est.),Newbold Advisors4.1 ★,"Omaha, NE","Role - Sr. Data Engineer
Location: Dallas, TX (Hybrid)
C2H
SQL, Spark, Analytics, Hadoop, Scala, data pipelines, tableau/PBI, Airflow, Kafka,
Job Description:
Designs, develops, and implements Hadoop eco-system based applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation.
Required Qualifications –
Option 1: Bachelor’s degree in Computer Science and 4 years' experience in software engineering or related field.
Option 2: 6 years’ experience in software engineering or related field.
Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field. 3 years' experience in data engineering, database engineering, business intelligence, or business analytics.
Nice to have soft skills –
7+ years of experience with 3+ years of Big data development experience
Experience in HDFS, Hive, Hive UDF’s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix.
Demonstrates expertise in writing complex, highly optimized queries across large data sets
Retail experience and knowledge of commercial data is a huge plus
Experience with BI Tool Tableau or Looker is a plus
Main Skills:
strong experience in Scala, Spark, SQL, Python , Tableau , Power BI and more.
Design, develop and build database to power Big Data analytical systems.
Design pipelines from a wide variety of data sources using Spark, SQL, HQL and other
technologies.
Build robust and scalable applications using SQL, Scala/Python and Spark.
Create real time data streaming and processing using Kafka and/or Spark streaming.
Build dashboards using Tableau, Power BI and other reporting tools.
Hybrid role - will need to go into the office 1 - 2 days a week.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75204: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Dallas, TX 75204
Show Less
Report",4.0,1 to 50 Employees,2007,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Data Engineer,$77K - $108K (Glassdoor est.),Union Home Mortgage3.7 ★,"Strongsville, OH","Overview:
The Data Engineer will contribute to the development of enterprise analytics products and data management solutions in support of UHM’s strategic initiatives. Their job is to optimize databases, develop ETL solutions, and manage automatized data flow. The Data Engineer will supervise database content and vouch for data quality. This position will function as a technical leader, making sure that the data architecture under your watch is secure and sustainable. You will work together with other teams on developing various data integration strategies. The Data Engineer will work directly with our IT Data Analytics partners to implement the next generation of data visualization and insights.

At UHM, we understand diversity comes in many different forms. It’s our commitment to improve inclusion in the workplace through programs and policies that establish a positive and inclusive environment where every Partner, regardless of their background, can grow and excel. We value diversity, educate on equity, and create inclusive partner opportunities to ensure that you know #UBelongAtUHM!
Responsibilities:
Work closely with the Enterprise Data Warehouse Manager and Team Leads to implement a modern data and analytics platform.
Lead the design, construction, and implementation of robust, fault tolerant data pipelines to clean, transform and aggregate unorganized and messy data into data products
Determine tools and strategies to deliver solutions that demonstrate effective Business and IT alignment.
Ensure data integrity with data profiling, cleansing, and audits
Ensures enterprise data is structured for ease of use and adheres to data modeling standards and best practices
Provide testing and production support as needed
Maintain a detailed knowledge of research, trends, and innovation in the Data & Analytics technology space. Makes recommendations or offers alternative solutions for the improvement and growth of Enterprise analytic capabilities
Provide guidance and training to other team members in the form of feedback, lunch and learns, and day-to-day mentoring
Other duties may be assigned.
Education & Qualifications:
Bachelor's Degree in Computer Science, Data Analytics, or related field
5+ years of experience architecting, designing, developing, and maintaining large scale cloud data solutions utilizing a mixture of Big Data and Relational database platforms
Proven experience and proficiency with SQL
Experience staging and loading data to an Enterprise Data Warehouse
Knowledge of data integration tools, data governance, and enterprise grade reporting systems
Experience driving technical ideas and communication clearly with technical, as well as not-technical audiences

Union Home Mortgage Corp. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Show Less
Report",3.7,1001 to 5000 Employees,1970,Company - Private,Banking & Lending,Financial Services,$100 to $500 million (USD)
Data Engineer,$120K - $150K (Employer est.),"Pallidus, Inc3.8 ★",Remote,"Pallidus, Inc. is a fast-growing producer of large diameter Silicon Carbide (SiC) wafers for high power and high frequency semiconductor applications in transportation, green energy, telecommunications, and Industrial markets. Pallidus’ M-SiC™ technology delivers step change improvements in SiC quality and cost effectiveness to drive wafer to device yield performance. Pallidus is a high energy, technology driven and innovative startup company looking for like-minded people to join our team.

SCOPE
Pallidus is looking for a Data Engineer to deliver business value through the design and integration of information models that originate from multiple data sources. You will be responsible for expanding and optimizing our data and data pipeline architecture and data flow and collection for cross-functional teams. The Data Engineer will be part of the growing the data science team and be responsible for the day-to-day activities related to the implementation of new services and support for existing services.

RESPONSIBILITIES
Partner with key business stakeholders to create blueprints for data solutions & services that align with overall corporate objectives and support enterprise scale reporting capabilities
Translate simple to complex user requirements into functional and actionable analytics
Provide guidance on architecture, data-store selection, data modeling, and query optimization for existing and future applications
Execute the design, development and support of data solutions including configuration, administration, monitoring, performance tuning, and debugging for efficient data handling & analytical reporting
Ensure an appropriate infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources into cloud computing platforms such as Snowflake and Databricks
Support data quality issue resolution while managing SLAs for services owned
Participate in the development lifecycle using Agile / DevOps methodologies and support testing & deployment as part of the full release cycle.
Become an internal subject matter expert on various datasets and support other departments on usage of those datasets
Collaborate with applications teams and business partners to assist in developing data governance framework & data retention policies
REQUIREMENTS
Bachelor's degree in computer science, engineering, or related field of study
Minimum of 5 years of experience in a data engineering role working with diverse data sets ideally with modern data warehousing/data lake technologies
A successful history of ETL manipulating/processing/extracting value from large disconnected and diverse datasets
Comfortable building pipelines with industry best practices to load huge volumes of data into a Data Warehouse
Strong analytic skills related to working with unstructured datasets
Experience with relational SQL and NoSQL databases, and capable of writing complex queries in SQL, spark, pandas, etc.
Knowledge of data modeling, star schema, incremental load
Ability to quickly build effective working relationships across the organization
Solid software engineering skills and advanced knowledge of at least one language, preferably Python. Appreciation of and dedication to clean coding principles is important
Effective communication skills, including the ability to articulate complex technical requirements to business and end-users in a clear and non-technical manner
Must be a US Citizen or be a lawful permanent resident of the U.S. to meet Export Control requirements
Show Less
Report",3.8,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Sr. Data Engineer,-1,MindBoard3.5 ★,Remote,-1,3.8,-1,-1,-1,-1,-1,-1
Senior Data Engineer,$120K - $152K (Employer est.),iSpot.tv4.4 ★,Remote,"Position: Remote Data Engineer
Duration: 24+ Months
Note: Required Travel for Client meeting.
JD:
The Data Engineer will play a key role in the design, development, and deployment of our data platform. The ideal candidate will have a technical background in data engineering and have experience in building and maintaining large-scale data systems. And The Data Engineer will work closely with other teams, such as data science and product development, to understand their data needs and ensure initiatives are driven to completion.
YOU HAVE:
· Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
· At least 6 to 10 years of experience in data engineering.
· Strong programming skills in Python, SQL
· Familiarity with datalakes and warehouses, ETL processes, and data pipelines.
· Knowledge of cloud computing platforms such as GCP (preferred) or AWS
· Strong problem-solving and analytical skills.
· Attention to detail and ability to work well under pressure.
· Good communication and collaboration skills.
· Ability to learn quickly and adapt to new technologies.
· A passion for working with data and technology.
· Design, build, and maintain data pipelines to ensure data is stored, processed, and analyzed efficiently.
· Develop and implement data warehousing and ETL processes.
· Troubleshoot and resolve data-related issues.
· Collaborate with cross-functional teams to ensure data is accessible and usable by stakeholders.
Job Types: Full-time, Contract
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 5 years (Preferred)
Data Engineer: 10 years (Preferred)
Shift availability:
Day Shift (Preferred)
Work Location: Remote
Show Less
Report",3.5,1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
Sr. Azure Data Engineer(Databricks),$80.00 - $100.00 Per Hour (Employer est.),Health,Remote,-1,3.5,-1,-1,-1,-1,-1,-1
Senior Data Engineer (Azure),$93K - $135K (Glassdoor est.),Momentum Consulting4.4 ★,"Miami, FL","Momentum takes pride in establishing a team of highly skilled professionals to deliver IT consulting services to our Fortune 500 clients. We provide successful technology solutions and solve critical business challenges to meet our client’s needs. Our alliances with industry-leading technology organizations have been instrumental to our success and allow us to offer robust solutions that are highly scalable and supportable using the best technologies available.

We are seeking to hire a seasoned Senior Data Engineer to join our team. To succeed in this data engineering position, the cloud engineer should have strong analytical skills and the ability to combine data from different sources. You will use various methods to develop raw data into useful data systems. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods. Overall, you’ll strive for efficiency by aligning data systems with business goals.

About the job:
Responsible for the development and management of a Microsoft Azure cloud data platform leveraging Azure Data Lake Storage Gen2, Azure Synapse Analytics, and Azure Data Factory
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
Work with data and analytics experts to strive for greater functionality in our data systems.

About You:
Bachelor’s Degree in Computer Science or a related discipline
Minimum of 5-7 years of data warehousing, data lifecycle management, and computer programming experience
MS Azure: Azure Data Lake Store, Azure Synapse
Azure Data Factory/SSIS or related tool
Business visualization tools, Semantic Layer – Views and Dimensional Modeling – Power BI knowledge is a plus
Experience in database design and data modeling
Experience with structured query language (SQL)
Strong learning orientation and curiosity; comfortable learning new systems/software applications
Strong analytical thinking skills and problem-solving skills
Solid written and oral communication skills
Strong proficiency in Python with an emphasis in building data pipelines
Knowledge of scheduling, logging, monitoring and alert frameworks
Knowledge of Data infrastructure

About us:
Recently honored for the eleventh consecutive year as one of the “Top 10 Best Places to Work” by the South Florida Business Journal. We offer a great working environment and provide our team members with the opportunity to enhance and expand their professional development. Come join our team and take part in our excellent compensation package that includes a competitive salary, health benefits, and a 401K plan. We look forward to hearing from you!

Momentum Consulting offers a great working environment and provide our team members with the opportunity to enhance and expand their professional development. Come join our team and take part in our excellent compensation package that includes competitive salary, health benefits, and a 401K plan. We look forward to hearing from you!

Momentum Consulting Corp. is an Equal Opportunity Employer.
Show Less
Report",4.4,51 to 200 Employees,1994,Private Practice / Firm,Business Consulting,Management & Consulting,$25 to $100 million (USD)
Expert Data Engineer (Remote),$82K - $119K (Glassdoor est.),Experian4.2 ★,"Costa Mesa, CA","Company Description

Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been named in the 100 “World’s Most Innovative Companies” by Forbes Magazine. Experian Consumer Information Services is redefining the way our clients do business within all aspects of the customer credit lifecycle. Fueled by best-in-class data and innovative technology we help businesses make smarter decisions, identify consumers, make decisions on loans, market to prospects and collect.

Job Description

We are looking for an Expert ETL Data Engineer who will be responsible for building data pipelines, data warehouse solutions, and analytics processing tools to democratize data.
About us, but we’ll be brief
Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been named in the 100 “World’s Most Innovative Companies” by Forbes Magazine.
This position will be supporting the Experian Consumer Services - a passionate and innovative team with a mission to provide Financial Power to All™. Our portfolio offers credit education and identity protection solutions to consumers and helps businesses manage the impact of a data breach.
What you’ll be doing
Design and develop foundational components of the BI platform which includes Data Engineering, Data Quality, and Data Catalog framework.
Partnering with Program Managers, Subject Matter Experts, Architects, Engineers, and Data Scientists across the organization where appropriate to understand customer requirements, design prototypes, and optimize existing data services/products.
Design and maintain data warehouse solutions that allow for large-scale analytics processing.
Build scalable ETL data pipelines, ingesting high volume of data from internal and external sources.
Build and maintain Data Engineering solutions that support self-service BI Platform, partnering with BI Engineers to deliver end to end solutions.
Work closely with product teams to understand business success criteria, translate business needs into technical requirements, worked with other Data Engineers and enable project success.
Ability to develop high-performing data engineering team with an inspiring leadership style.
Curious, detail oriented, and highly motivated self-starter. Able to work independently and with the team to formulate innovative solutions
Troubleshoot and resolve data, system, and performance issues.
Participant in production support on-call rotation
#LI-REMOTE

Qualifications

What your background looks like
Minimum 10 years of experience in Data Engineering development and support
5 years of experience within big data domain
5 years of experience in Python scripting
5 years of experience with AWS ecosystem (Redshift, EMR, MWAA, S3, etc.)
2 years of experience with BI tools such as Tableau, Alteryx
5 years of experience in Agile development methodology
Excellent communication skills
Ability to understand complex metrics and translate business requirements to technical requirements
Ability to multitask and prioritize an evolving workload in a fast pace environment.
Proven track record of leading large-scale project
Education: BS degree or higher in computer science or related fields

Additional Information

All your information will be kept confidential according to EEO guidelines.
Our compensation reflects the cost of labor across several U.S. geographic markets. The base pay range for this position is listed above. Within this range, individual pay is determined by work location and additional factors such as job-related skills, experience and education. This position is also eligible for a variable pay opportunity and a comprehensive benefits package which includes health, life and disability insurance, generous paid time off including paid parental and family care leave, an employee stock purchase plan and a 401(k) plan with a company match.
Experian is proud to be an Equal Opportunity and Affirmative Action employer. We’re passionate about unlocking the power of data to transform lives and create opportunities for consumers, businesses, and society. For more than 125 years, we’ve helped people and economies flourish – and we’re not done.
We take our people’s agenda very seriously. We focus on what truly matters; diversity and inclusion, work/life balance, flexible working, development, collaboration, wellness, reward & recognition, volunteering, making an impact... the list goes on. See our DEI work in action!
The power of YOU. We are building a culture where everyone is comfortable bringing their whole self to work. A place where we not only respect our differences and values but celebrate them in a positive and supportive environment.
Find out what is like to work for Experian and discover the Unexpected!
Apply Now: click Easy Apply
Show Less
Report",4.2,10000+ Employees,1980,Company - Public,Business Consulting,Management & Consulting,$5 to $10 billion (USD)
Senior Data Engineer,$43.96 - $70.65 Per Hour (Employer est.),DiamondPick4.5 ★,"Edison, NJ","Hi ,
Greetings from Diamond pick inc.
We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP..
Role:Data engineer
Location: Berkley Heights, NJ(Onsite)(locals only)
9+ years of experience is must
Description
Skills: strong Java,Azure,Spark & sql
Company Description:
Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
QualificationsBachelor's Degree in Computer Science or Computer Engineering is required
Job Type: Contract
Salary: $43.96 - $70.65 per hour
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Java (Required)
Azure (Required)
Work Location: In person
Show Less
Report",4.5,1001 to 5000 Employees,2018,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
Data Engineer,$120K - $140K (Employer est.),Zelis3.8 ★,"Saint Petersburg, FL","Summary:
· Design, develop and implement scalable batch/real time data pipelines (ETLs) to integrate data from a variety of sources into Data Warehouse and Data Lake
· Design and implement data model changes that align with warehouse dimensional modeling standards.
· Proficient in Data Lake, Data Warehouse Concepts and Dimensional Data Model.
· Responsible for maintenance and support of all database environments, design and develop data pipelines, workflow, ETL solutions on both on-prem and cloud-based environments.
· Design and develop SQL stored procedures, functions, views, and triggers
· Design, code, test, document and troubleshoot deliverables
· Collaborate with others to test and resolve issues with deliverables
· Maintain awareness of and ensure adherence to Zelis standards regarding privacy.
· Create and maintain Design documents, Source to Target mappings, unit test cases, data seeding.
· Ability to perform Data Analysis and Data Quality tests and create audit for the ETLs.
· Perform Continuous Integration and deployment using Azure DevOps and Git
· Experience with Banking and Finance Processes specifically NACHA, BAI2 and Bank 822 Files required.
· This position will support the Payments Advanced Record Keeping Project
Requirements:
· Minimum of 5+ years experience in the following:
o 5+ years data engineering experience to include data analysis
o 2+ years working with an ETL tool (DBT preferred)
o 5+ years programming SQL objects (procedures, triggers, views, functions) in SQL Server. Plus experience optimizing SQL queries a plus
o 2+ years designing and developing Azure/AWS Data Factory Pipelines.
o 2+ years Columnar MPP Cloud data warehouse using Snowflake
· Advanced understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
· Experience designing and implementing Data Warehouse.
· Working Knowledge of Azure/AWS Architecture, Data Lake
Preferred Skills:
· Microsoft BI stack (SSIS/SSRS/SSAS)
· Working knowledge managing data in the Data Lake.
· Business analysis experience to analyze data to write code and drive solutions
· Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence.
· Healthcare and/or Payments experience
Job Type: Full-time
Pay: $120,000.00 - $140,000.00 per year
Schedule:
Monday to Friday
Application Question(s):
Do you have experience designing and developing with Data Factory Pipelines (Azure/AWS)?
How many years of work experience do you have with Snowflake Cloud?
Do you have experience analyzing data to write code and drive solutions?
Experience:
ETL: 2 years (Required)
SQL: 4 years (Required)
Work Location: In person
Show Less
Report",3.8,1001 to 5000 Employees,2016,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
"Software Engineer, Data Platform",$226K - $275K (Employer est.),"Grammarly, Inc.4.5 ★",United States,"Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Every day, tens of millions of people and 50,000 professional teams worldwide trust Grammarly's AI and human expertise to help ideate, compose, revise, and comprehend communications. Our team members have the autonomy to take on exciting challenges in pursuit of our mission to improve lives by improving communication. Together, we're building on more than a decade of steady growth and profitability. We're defining the communication assistance category with our tailored service offerings: Grammarly Free, Grammarly Premium, Grammarly Business, and Grammarly for Education. Our latest product offering, GrammarlyGO, brings the power of generative AI to our users. It all begins with our team collaborating in an inclusive, values-driven, and learning-oriented environment.
To achieve our ambitious goals, we’re looking for a Software Engineer to join our Data Engineering Platform team. This person will build back-end systems to enable data management and create full-stack software tools to help data engineers and end users work with data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure.
Your impact
As a Software Engineer on our Data Engineering Platform team, you will:
Install, provision, and manage data pipeline orchestration software as a service for all of Grammarly to use.
Install, provision, and manage container management software that will enable seamless scaling as our data volume grows.
Create alerting and monitoring services that allow engineers to accurately and efficiently debug and resolve failures in real time.
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re looking for someone who
.
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries.
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation and benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. . If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States:
United States:
Zone 1: $226,000 - $275,000/year (USD)
Zone 2: $203,000 – $245,000/year (USD)
Zone 3: $192,000 – $225,000/year (USD)
Zone 4: $181,000 – $215,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
#LI-DT1
#LI-Hybrid
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
To apply to this job, click Apply Now
Show Less
Report",4.5,501 to 1000 Employees,2009,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Data Engineer,$130K - $165K (Employer est.),Fabletics4.4 ★,California,"Job Description
How Do You Fit In?
We are currently seeking a highly skilled and motivated Data Engineer to join our dynamic team. If you have a passion for data analysis, possess strong technical skills, and are excited about leveraging data to drive business insights, this position is for you. As a Data Engineer, you will join a tight knit group of key contributors who are actively working together to achieve aggressive goals and meet timelines to drive the business forward.

This position will report to the Manager, Data Engineering (EDW), Data Platforms.

What You Will Do:
Work closely with stakeholders to understand their data needs and provide data-driven solutions.
Design, develop, and maintain scalable data pipelines and ETL processes to collect, process, and analyze large volumes of structured and unstructured data.
Solid understanding and experience in data modeling, including designing and implementing efficient data models.
Build and optimize data models, data warehouses, and data marts to support reporting and analytics needs.
Perform exploratory data analysis to identify trends, patterns, and insights that contribute to business decision-making.
Stay up-to-date with the latest trends, tools, and technologies in data analytics and apply them to enhance data-driven decision-making processes.
Perform code reviews to ensure data engineering best practices, code quality, and maintainability.
Provide production support on a rotating basis to ensure the availability, reliability, and performance of data analytics systems and processes.

What You Can Bring:
Bachelor’s degree in Computer science, Engineering, Mathematics, Statistics, or a related field. A master’s degree is a plus.
4 years of experience in creating and managing data pipelines.
1 year of experience with Python or another scripting language.
Experience in performing code reviews and ensuring adherence to best practices.
Excellent communication and presentation skills, with the ability to effectively convey complex data findings to non-technical stakeholders.
Proven experience in data analytics, data engineering, or a related role.
Strong expertise in SQL, particularly in an analytics/reporting capacity, with significant experience in creating and maintaining reporting processes.
Nice to Have:
Familiarity with traditional data warehousing concepts (e.g., Kimball methodology).
Experience in data engineering on cloud platforms such as BigQuery, Redshift, Snowflake, Teradata, Vertica, etc.
Knowledge of Dbt and/or Airflow for data pipeline management.
Previous experience in e-commerce, retail, or internet industries.
Compensation & Total Rewards:
At TechStyleOS, we believe work and life should fit together! We continue to build a culture of flexibility, to empower you to do your best and put yourself first. Our Total Rewards program rewards employees for their hard work, supporting their health, well-being, families, and ultimately their life journey. Total Rewards at TechStyleOS includes:

Hybrid Work Schedule*
Unlimited Paid Time Off*
Summer Fridays*
Healthcare Plans
Employee Discounts
401k
Annual Bonus Program
Equity Program*
And More

Varied for retail and fulfillment roles

The annual base salary range for this position is from $130,000-$165,000. The range provided includes the base salary that TechStyleOS expects to pay for the role. Offered base salary will be dependent on factors including the scope and complexity of the role, candidate’s related work experience, subject matter expertise and work location.
#LI-GR1
#LI-TechStyleOS
About TechStyleOS
TechStyleOS is the globally integrated Operations and Services provider behind some of the fastest growing online fashion brands in history, including Fabletics, Savage X Fenty, JustFab, ShoeDazzle, and FabKids. With capabilities spanning technology, data science, supply chain management, fulfillment, customer service, and more, we help brands launch, scale and grow—across product categories and geographically. From predictive analytics to data-driven marketing and attribution, our unique approach is powered by our proprietary, end-to-end tech platform that enables the brands we serve to deliver a level of personalization, value, and satisfaction that are unrivaled in the fashion industry.
Fabletics, Inc. is an equal opportunity employer. We recruit, employ, compensate, develop, and promote regardless of race, national origin, religion, sex, sexual orientation, gender identity, age, disability, genetic information, veteran status, and other protected status as required by applicable. At Fabletics, Inc., we champion a vibrant workplace culture that thrives on diversity law and do not tolerate discrimination or harassment. We are one team from many backgrounds, innovating through diversity of individuals, who are driven by passion for creating an inclusive space for all. Fabletics, Inc. will continue to champion a workplace culture that prizes diversity and inclusivity.
We encourage you to apply regardless of meeting all qualifications and/or requirements.
Apply Now: click Apply Now
Show Less
Report",4.4,Unknown,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Azure Data Engineer,$84K - $191K (Employer est.),PSRTEK4.5 ★,"Berwick, ME","Job Title : Azure Data Engineer
Job Type: Contract
Job Description
Atleast 2 years of experience working at onsite closely with client.
Strong exp with Azure Cloud Technologies, including ADF, Databricks, SQL DB, ADLS
Experience with data transformation and manipulation using Azure Databricks
Working knowledge of Azure DevOps CI/CD tools and concepts Azure pipelines, GitHub
Scripting experience with Python and Rest API
Experience in implementing CDC (Change Data Capture) Data Virtualization
Experience in creating base view, Derived View and Data source connections using Denodo
Experience in Denodo caching in snowflake
Experience in creating data models by extracting from various data sources by connecting through JDBC, ODBC and Web API in Denodo Skills Required: Python ADF ADB SQL Denodo Snowflake
Job Type: Contract
Salary: $84,181.34 - $190,840.44 per year
Ability to commute/relocate:
Berwick, ME 03901: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 5 years (Preferred)
SQL: 6 years (Preferred)
Data warehouse: 5 years (Preferred)
Work Location: In person
Speak with the employer
+91 609-934-3291
Show Less
Report",4.5,Unknown,-1,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Senior Data Engineer,$130K - $160K (Employer est.),ReUp Education3.4 ★,Remote,"The Role
We are looking for an experienced data engineer to help manage, utilize, and build upon ReUp’s data infrastructure. This role will be a key member of the team creating and delivering against a data engineering strategy that will support ReUp’s tech-enabled service as we continue to grow. If you are a curious engineer strong in Python, SQL, AWS, and business acumen, then we want to hear from you!
As a Data Engineer, you’ll maintain critical data architecture, process large volumes of student data, and work collaboratively to create useful reports, dashboards, and data products that support internal teams and university partners. Data is at the heart of everything we do, so we’re always looking to improve these systems to better address a variety of open-ended questions related to student success. Along the way, you’ll help improve inefficiencies, ensure we follow best practices around data, and work on interesting data science projects in areas like predictive analytics and natural language processing. Through this critical role in our data operations, you will better equip us to support our students all the way to graduation.
What we do
Founded in 2015, ReUp Education is the only organization that focuses exclusively on helping colleges and universities engage and re-enroll the more than 40% of US students who have “stopped out” and support them until graduation, through our technology-enabled service. To date, we have re-enrolled nearly 22,500 students, assisted over 8,000 to graduate, and recaptured over $85 million in tuition for our university partners.
What you’ll do
Develop, construct, test and maintain data architectures and pipelines
Build and refine Data Integration Products (such as APIs, integrations, and other data tools)
Leverage (and expand) Python codebase to launch university partnerships, ingest student lists, process partner data, and perform validation
Support data security, compliance, and governance initiatives
Manage (and expand) daily monitoring automation for key systems/processes
Partner with stakeholders (marketing, product, coaching, partner success, finance, etc.) to optimize our systems and develop novel solutions
Contribute to our data strategy to ensure it is aligned with company goals
Manage and setup new ETLs, automated jobs, and workflows
Troubleshoot and resolve issue escalations pertaining to data infrastructure
Protect end users from risk of faulty or inaccurate data
Identify ways to improve data reliability, efficiency, and quality
Collaborate with data scientists to prepare data for machine learning applications (predictive analytics, natural language processing, etc.)
Collaborate with data analysts to create business intelligence assets (analytics reports, dashboards, KPIs, research etc.)
Qualifications
Research shows that women and people from underrepresented groups only apply to jobs if they meet all of the qualifications. However, no one ever meets 100% of the qualifications. ReUp encourages you to break that statistic and to apply. We look forward to your application.
10+ years of related work experience in data engineering required
Experience building and maintaining APIs required
Strong Python and SQL skills required
Experience building within Amazon Web Services (certification a plus)
Experience with Airflow and other workflow management tools
Experience with large scale production environments
Experience with modern development processes and practices
Experience with machine learning technologies
Tableau / Mode Analytics / BI experience a plus
Salesforce CRM experience a plus
Certification in CISA, CISSP or similar data security credential a plus
Higher ed, ed tech, and/or startup experience a plus
Bachelor’s or Master’s degree in Engineering, Computer Science, Data Science, or other quantitative focused field of study a plus
You believe in ReUp’s mission and are committed to helping us create new opportunities for millions of stopout students
Additional
You are a first-principle thinker and creative problem solver
Your attention to detail leaves no stone unturned
You are able to communicate complex ideas effectively
You proactively look for ways to automate and optimize tasks
You enjoy learning in a fast-paced environment
You are collaborative and open to feedback
You have a passion to support data driven decisions while recognizing the humans behind the numbers
Compensation & Benefits
Compensation: $130,000-160,000 annual salary commensurate with experience
Medical, dental, and vision insurance for employees
We pay 100% of the employee's premium and 50% of any dependents' premiums
FSA or HSA available
Company paid short term disability, long term disability, and life insurance for employee
Flexible time off and remote work opportunity
15 paid holidays per year (including Juneteenth and the last week of the calendar year)
Company wellness days (2 per year)
Day of Service (Paid day for volunteering)
401(k) plan
Paid parental leave (12 weeks primary parental leave, 6 weeks secondary parental leave)
A diverse team that fosters a high level of collaboration despite being highly distributed
We provide your choice of a Mac or PC laptop

Location
ReUp is a remote organization with a geographically distributed team. This position will be based remotely in one of the states listed:AL, AZ, CA, FL, GA, IL, MA, MI, MO, NC, NH, NY, OH, OR, PA, SC, TN, TX, VA, WA and WI.
Company Culture
TEAMWORK * RESULTS * CONSTANT LEARNING * AGENCY * DIVERSITY, EQUITY & INCLUSION * JOY

ReUp employees share a passion for improving outcomes for stopout students. We support students to get Results as they embark on finishing what they started. We believe in the power of human potential and that supporting an individual’s Agency acts as a catalyst for positive change and resiliency. We support Diversity, Equity & Inclusion, for both the students we work with and in our hiring practices. We value Teamwork and strive to create a safe and supportive environment where trust, communication, creativity, and humility are valued as highly as technical skills. We tackle hard problems with curiosity and take action towards continuous improvement and Constant Learning. Approaching our work with open hearts, open minds, and seeking collective success creates Joy. If that sounds like your dream work environment, we look forward to hearing from you.

ReUp Education is an equal opportunity employer. Our company values diversity and believes diverse teams make innovation possible. We encourage all qualified applicants from any race, color, religion, sex, gender identity, sexual orientation, national origin, disability status, protected veteran status, or other characteristics to apply.

ogDwCwY1YZ
Show Less
Report",3.4,51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Data Engineer,$110K (Employer est.),Capitol Federal2.9 ★,"Topeka, KS","Job Description:
Pay: up to $110,000 Annually
Job Type: Full Time
The Data Engineer assists in setting overall development roadmap and standards for the Bank and helps evaluate and architect the use of data solutions, using industry best practices. This position works as part of a collaborative team to design, code, and implement data solutions to support internal business requirements or external customers and vendors. An innovative mindset and an ability to translate complex business scenarios into a technical solution is required. This position performs a variety of tasks under general supervision. The position reports directly to an IT manager and requires regular, predictable and timely attendance at work to meet department workload demands.
Paid time off and holiday available on your first day! Benefits available to anyone working 20 hours or more per week!
CapFed® is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Job Type: Full-time
Work Location: In person
Show Less
Report",2.9,501 to 1000 Employees,1893,Company - Public,Banking & Lending,Financial Services,$100 to $500 million (USD)
Data Engineer,$130K - $170K (Employer est.),ASSURANCE3.2 ★,"Seattle, WA","About Assurance
Assurance IQ is a technology company headquartered in Seattle. We were acquired by Prudential (NYSE: PRU) to further the joint mission of improving financial wellness across the world.

Our team of world class software engineers, data scientists, and business professionals work every day to expand our product offerings and the reach of our platform. We simplify the complex world of insurance and financial services into straightforward, valuable solutions to improve people's lives. We start by asking customers a few questions, so our system can learn about their needs; from there, our ground-breaking, proprietary platform takes over and analyzes the thousands of data points that make customers unique. This is how we create custom-tailored plans for each customer; plans built precisely for their needs and budget. Our platform serves as the intersection between customer and seller, technology, and the human touch.

At Assurance, we are innovative, persevering, collaborative, calculated, and authentic, and we're working together to improve the lives of millions!

About the Position
As we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.
To be successful in this role, you must possess the following:
Experience with Python and SQL
Experience in data modelling
Business Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.
Comfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct
Comfort with learning new technologies to help the team explore new solutions to existing problems
Excellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.
Enthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.
Bachelors degree in mathematics, statistics, data science or related field of study.
The following additional experience is desired:
You have a proven ability to drive business results by building the right infrastructure that enables data-based insights.
You are comfortable working with a wide range of stakeholders and functional teams.
The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes.
We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail.
You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity.
You can work independently, with little oversight or guidance.

Note: Assurance is required by multiple state and city laws to include the salary range on position postings when hiring in those specific locals. The salary range for this position will be between $130,000-$170,000 and may be eligible for additional bonus or commission plans + benefits. Eligibility to participate in the bonus or commission plans is subject to the rules governing those programs, whereby an award, if any, depends on various factors including, without limitation, individual and/or organizational performance. In addition, employees are eligible for standard benefits package including paid time off, medical, dental and retirement.
Start your job application: click Apply Now
Show Less
Report",3.2,1001 to 5000 Employees,2016,Company - Public,Insurance Agencies & Brokerages,Insurance,$100 to $500 million (USD)
Software/Data Engineer,$130K - $150K (Employer est.),Trio Health Advisory Group Inc.,"Louisville, CO",-1,3.2,-1,-1,-1,-1,-1,-1
Sr. Data Engineer,$84K - $191K (Employer est.),GG tech global,Remote,-1,3.2,-1,-1,-1,-1,-1,-1
Data Engineer,$100K - $140K (Employer est.),GVEC4.8 ★,"Seguin, TX","Who We Are
GVEC is a member-owned cooperative that is proud to provide Electric, Internet, and Beyond the Meter services to the Guadalupe Valley area and beyond. With the mission of exceeding the expectations of those we serve, we are looking for talented professionals interested in providing value for our customers and coworkers. GVEC wants team members who can deliver the unexpected through attitude, results, and a willingness to continuously develop themselves, their department, and our organization.

What We Are Looking For
GVEC is looking to hire a Data Engineer to join our dedicated Analytics and Data Science team! As a Data Engineer, you will play a key, hands-on role across multiple business segments providing technical support and expertise for data-enabled products, solutions, delivery models, business models and end-to-end data solutions. The Data Engineer identifies necessary source or processed data for analysis, data acquisition, data storage, data cleaning and data preparation. They make data available in the form needed for analysis and visualization, starting with data as it is stored in native systems, data warehouses, and other sources. The Data Engineer collaborates with industry and solutions teams to identify opportunities to design and deliver data and analytics capabilities and offerings that stay on top of emerging data trends and methodologies.

We will consider remote applicants for this position; however, you MUST reside in Texas. During GVEC’s training and development programs, you will be expected to attend on-site classes, as needed.

How You’ll Help
Analyzes, designs, and develops data and analytics solutions
Leverage TimeXtender to manage data pipeline activities, such as:
ETL (Extract/Transform/Load) activities
Clean/standardize data; build database and create meaning for business
Develop underlying framework and initial semantic model
Add new data, fields, tables to semantic model
Develop and/or maintain customer identity matching process in Azure Data Factory
Develop assets in data visualization tools such as Power BI and/or SQL Server Report Services to conduct data model testing and validation
Perform application development to provide data where necessary
Prioritizes and executes simultaneously across multiple projects
Stays on top of emerging data and analytics trends and methodologies such as best practices for organizing and managing data (e.g., data warehouses, cloud architecture, etc.)
Plans, designs, and implements customized solution development and delivery plans with minimal oversight
Helps to optimize data, data architecture, data flow and collection for cross-functional teams
Clearly documents data solutions so that they may be reliably maintained
Communicates clearly and proactively to Analytics team and internal stakeholders
Experience/Skills You’ll Need
Experience with Azure technology stack: Azure Data Factory, Azure Data Lake Gen2, Azure SQL Data Warehouse, Azure Synapse Analytics, Azure Analysis Services
Ability to write DDL (Data Definition Language) and DML (Data Manipulation Language) in addition to DQL (Data Query Language), or significant, comparable work experience
Successful history of manipulating, processing, and extracting value from large, disconnected datasets
Familiarity with system and software development methodologies
Preferred Experience/Skills
2 or more years of experience developing data solutions for business users, including code management and all aspects of the software development life cycle
Experience with data and analytics visualization software (e.g., Excel, Power BI, Tableau, etc.)
Experience with data extract, transform and load, data storage, data models, etc. using working knowledge of SQL, Python, or comparable languages
Undergraduate degree required, with a preference of specializing in quantitative methodology (e.g., BS in Mathematics, Economics, Computer Science, Information Management or Statistics)
Deadline to submit your application: Open until filled.
Commensurate with experience, the wage range for this position: $100,000 - $140,000/year
Start your job application: click Easy Apply
Show Less
Report",4.8,201 to 500 Employees,1938,Self-employed,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
Azure Data Engineer,$100K - $104K (Employer est.),ProIT Inc.4.9 ★,"Bellevue, WA","Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004
Show Less
Report",4.9,51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
"Software Engineer, Data Platform",$120K - $185K (Employer est.),SentiLink5.0 ★,Remote,"SentiLink is building the future of identity verification in the United States. The existing ways to determine if somebody is who they claim to be are too clunky, ineffective, and expensive, but we believe strongly that the future will be 10x better.
We’ve had tremendous traction and are growing extremely quickly. Already our real-time APIs have helped verify hundreds of millions of identities, beginning with financial services. In 2021, we raised a $70M Series B round, led by Craft Ventures to rapidly scale our best in class products. We’ve earned coverage and awards from TechCrunch, CNBC, Bloomberg, Forbes, Business Insider, PYMNTS, American Banker, LendIt, and have recently landed on the Forbes Fintech 50, 2023. Last but not least, we’ve even been a part of history-we were the first company to go live with the eCBSV and testified before the United States House of Representatives.
Role:
As a software engineer on the Data Platform team at SentiLink, you will own the data infrastructure components to support the SentiLink suite of products. You will work with product, engineering, and data science teams across the company to build, enhance and modify the data platform that powers our fraud detection products. You have outstanding programming skills and are proficient in our technology stack and pick up new technologies quickly as we evolve.
Responsibilities:
Build, expand, and optimize data architecture in order to create the most accurate dataset of identities and their relationships
Develop and operate scalable and resilient data stores and distributed data processing infrastructures to meet business requirements
Design, develop, test and support a suite of API-based data products
Enable the data science team by ensuring data availability at scale and efficiency
Partner with product management to drive agile delivery of both existing and new offerings
Ensure data platform and services meet SLA and quality requirements; on call rotation for production issues, along with the rest of engineering
Develop functional subject matter expertise within various areas of identity fraud domain
Requirements:
5+ years of software product development experience
3+ years of development experience in python or golang and related technologies and frameworks
Strong expertise in big data technologies and distributed data processing frameworks such as Spark, Kafka, Hadoop etc
Experience with public cloud platforms such as AWS, Microsoft Azure or GCP
Deep understanding of different database technologies including but not limited to RDBMS (e.g. postgres), NoSQL (Elasticsearch, vector DB), Columnar data stores etc. and experience with writing efficient queries and optimization techniques.
Proven track record of building and delivering enterprise grade, scalable, data-intensive backend services on Kubernetes or similar platforms.
Excellent analytical and problem solving skills, interpersonal skills and a sense of humor (enjoy the journey)
Experience working in a scrum / Agile development environment
Bonus points if you have..
experience working with Spark/EMR
built real time streaming applications
experience with AWS technologies such as EKS, sqs/sns, emr, redshift, s3 etc
Infrastructure-as-code (IAC) such as terraform, CloudFormation etc
prior experience working in a fintech startup
Candidates must be legally authorized to work in the United States and must live in the United States
Salary Range:
$120,000/year - $185,000/year
Perks:
Remote (or hybrid, if specified) work with home office stipend, and regular company-wide in-person events
Lunch, coffee and snacks during the work day are fully reimbursable
Employer paid group health insurance for you and your dependents
. . . and other typical start-up benefits (e.g. 401(k) plan with employer match, flexible paid time off, etc.)
Corporate Values:
Follow Through
Deep Understanding
Whatever It Takes
Do Something Smart
Show Less
Report",5.0,51 to 200 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
Senior Data Engineer,$100K - $179K (Employer est.),RVO Health4.0 ★,"Minneapolis, MN","AT A GLANCE
RVO Health is looking for a talented Senior Data Engineer to join our team! As a Senior Data Engineer at RVO Health, you will have the chance to build technology that drives real improvements to consumer health outcomes and has the potential to have widespread impact across the healthcare industry. You will design, develop, test, and maintain big data pipelines for ingestion, segmentation, and reporting to drive our vision!
What You'll Do:
Provide technology ownership for data solutions for projects that the team has been tasked with.
Work with a cross functional team of business analysts, architects, engineers, data analysts and data scientists to formulate both business and technical requirements.
Design and build data pipelines from various data sources to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.
Conceptualizing and generating infrastructure that allows data to be accessed and analyzed effectively.
Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.
Perform periodic code reviews and test plans to ensure data quality and integrity.
Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.
Execute on proof of concepts, where appropriate, to help improve our technical processes.
What We're Looking For:
5+ years of Data Engineering experience
4+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines.
4+ years of experience in the big data space
Experience in translating business requirements into technical data solutions on a large scale.
2+ years of experience working on AWS (Kinesis / Kafka / S3 / RedShift) Or Azure.
Able to research and troubleshoot potential issues presented by stakeholders within the data ecosystem.
Experience with GitHub and CI/CD processes
Experience with Compute technologies like EMR and Databricks
Experience working with job orchestration (eg., Airflow / AWS Step Function)
Experience with Data Modeling, Data warehousing
Working with Kubernetes is a plus
Strong analytical and interpersonal skills.
Enthusiastic, highly motivated and ability to learn quickly.
Able to work through ambiguity in a fast-paced, dynamically changing business environment.
Ability to manage multiple tasks at the same time with minimal supervision.
This position may occasionally require travel for training and other work-related duties.
""Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.""
Starting Salary: $100,000 - $178,500*
Note actual salary is based on geographic location, qualifications and experience
Health Insurance Coverage (medical, dental, and vision)
Life Insurance
Short and Long-Term Disability Insurance
Flexible Spending Accounts
Paid Time Off
Holiday Pay
401(k) with match
Employee Assistance Program
Paid Parental Bonding Benefit Program
Who We Are:
Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group's Optum Health. Together we're focused on delivering on our vision of a stronger and healthier world.
RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.
We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.
RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications.
We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.
We do not provide visa sponsorship at this time.
RVO Health Privacy Policy: https://rvohealth.com/legal/privacy
Show Less
Report",4.0,1001 to 5000 Employees,2022,Company - Private,Hospitals & Health Clinics,Healthcare,Unknown / Non-Applicable
Data Engineer,$87K - $129K (Glassdoor est.),N J Malin & Associates3.7 ★,"Addison, TX","Data Engineer Duties and Responsibilities
Assemble large, complex sets of data that meet non-functional and functional business requirements
Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues
Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
Skills and Qualifications
Ability to build and optimize data sets, ‘big data' data pipelines and architectures
Ability to understand and build complex data models for Power BI reporting.
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Must Have experience in (in order of importance)
10 + years experience as a Data Engineer or similar role.
SQL Server
T-SQL code
Administrator functions
Data Factory
Python
Power BI
DAX code
M code
Azure
Malin is an Equal Opportunity Employer - M/F/Veteran/Disability/Sexual Orientation/Gender Identity
Show Less
Report",3.7,501 to 1000 Employees,1971,Company - Private,Shipping & Trucking,Transportation & Logistics,$100 to $500 million (USD)
"Senior Software Engineer, Data Platform",$121K - $162K (Employer est.),Rockstar Games4.2 ★,"New York, NY","At Rockstar Games, we create world-class entertainment experiences.
A career at Rockstar Games is about being part of a team working on some of the most creatively rewarding and ambitious projects to be found in any entertainment medium. You would be welcomed to a dedicated and inclusive environment where you can learn, and collaborate with some of the most talented people in the industry.
Rockstar is seeking a Senior Software Engineer, Data Platform to join a team focused on building a cutting-edge game analytics platform and tools to better understand our players and enhance their experience in our games. This is a full-time permanent position based out of Rockstar's New York City, NY office.
The ideal candidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability and performance. In collaboration with other data engineers, machine learning engineers, and software engineers, the candidate will empower the team of analysts and data scientists to deliver data driven insights and applications to company stakeholders.
WHAT WE DO
The Rockstar Analytics team provide insights and actionable results to a wide variety of stakeholders across the organization in support of their decision making.
We are currently adding team members to help build out our platform and service capabilities to fully serve our customers and partners teams more extensively.
RESPONSIBILITIES
Implement and maintain the underlying analytics platform and other services related to helping ingest and deliver data to/from multiple sources.
Build and maintain microservices for both on-prem and cloud systems.
Design and implement APIs for both data access and other services.
Assist in the development of frontends for internal tooling.
Work with the operations teams to maintain infrastructure both on-prem and in Azure.
Maintain and extend CI/CD pipelines and documentation.
Adhere to and enforce rigorously automated testing.
Assist with the integration of data services into other tools for other teams within Rockstar.
Assist in the development of automation and operational support strategies.
QUALIFICATIONS
5+ years of work experience in enterprise-level software engineering and microservice environments.
Experience with API design and development through REST APIs, RPC, and/or Web APIs.
Comfortable with relational and NoSQL databases.
Comfort with Docker and Docker Swarm.
Familiar with CI/CD pipelines.
Experience with on-prem datacenters and cloud-based ecosystems.
Experience with C# and JavaScript.
Prior experience creating frontends with React, Redux, Typescript, npm, and/or single page applications (SPA).
Experience developing, productionizing workloads, and maintaining production services in a .NET environment.
SKILLS
Ability to manage numerous projects concurrently and strategically, prioritizing when necessary.
Good communication skills.
Dynamic team player.
A drive for not just accepting technology marketing, but truly understanding how new technologies actually work.
A strong penchant for optimization problems and a strong eye for scaling concerns.
A strong proponent of documentation.
A passion for technology.
PLUSES
Please note that these are desirable skills and are not required to apply for the position.
Experience with Azure.
Experience with the Hadoop ecosystem (HDFS, Spark, Impala, etc.) and other big data tooling (Kafka, Cassandra, etc.).
Familiarity with SQL languages such as T-SQL, Spark SQL, or PL/SQL.
Ability to develop in Python and Java.
Familiarity with developing in a Linux environment.
Experience working in support of large-scale data ecosystems at the terabyte or petabyte scale
Familiarity with Apache Airflow.
Experience with Databricks.
Experience with online game service APIs, particularly Xbox Live, Sony NP, Steam, and Epic Games.
Knowledge of the video game industry.
HOW TO APPLY
Please apply with a resume and cover letter demonstrating how you meet the skills above. If we would like to move forward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through the process.
Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and compensating employees based on their qualifications and demonstrated ability to perform job responsibilities.
If you've got the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates regardless of age, disability, gender identity, sexual orientation, religion, belief, or race.
The pay range for this position in New York City at the start of employment is expected to be between the range below* per year. However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations. Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards, in addition to a full range of medical, financial, and/or other benefits. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an ""at-will position"" and the company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, company or individual department/team performance, and market factors.

NYC Pay Range
$121,400—$161,800 USD
Show Less
Report",4.2,1001 to 5000 Employees,1998,Subsidiary or Business Segment,Video Game Publishing,Media & Communication,$5 to $25 million (USD)
Data Engineer I,$80K (Employer est.),VirginPulse3.4 ★,Remote,"Overview:
Now is the time to join us!
At Virgin Pulse we value and celebrate diversity and we are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Why? Because diversity inspires innovation, collaboration, and challenges us to produce better solutions. But more than this, diversity is our strength, and a catalyst in our ability to #changelivesforgood.
Responsibilities:
Who are you?
Data Engineer I perform development activities with the guidance of another member of the data engineering team. You will work closely with account management, ETL, data warehouse, business intelligence, and reporting teams as you develop data pipelines and enhancements and investigate and troubleshoot issues.

In this role you will wear many hats, but your knowledge will be essential in the following:
Extracting, cleansing, and loading data.
Building data pipelines using SQL, Python, and other technologies.
Triage incoming bugs and incidents.
Perform technical operation tasks.
Investigate and troubleshoot issues with data and data pipelines.
Participation in sprint refinement, planning, and kick-off to help estimate stories, raise awareness and additional implementation details.
Help monitor areas of the data pipeline and raise awareness to team when issues arise.
Performing quality assurance work to verify the accuracy of code and data results
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Qualifications:
What you bring to the Virgin Pulse team
In order to represent the best of what we have to offer you come to us with a multitude of positive attributes including:
1– 2 years or less experience in data engineering
SW certification or degree in IT related field
You also take pride in offering the following Core Skills, Competencies, and Characteristics:
Solid grasp of modern relational and non-relational models and differences between them.
Proficiency in writing SQL, the use of Excel, and some analytical tools.
Understanding of REST API.
Understanding of JSON.
Detail oriented and able to examine data and code for quality and accuracy.
Knowledge of Agile environments, including Scrum and Kanban methodologies
Python / R / programming language experience preferred
ETL experience preferred
AWS Lambda / Console experience preferred
Git experience preferred
No candidate will meet every single desired qualification. If your experience looks a little different from what we’ve identified and you think you can bring value to the role, we’d love to learn more about you!

#LI-REMOTE

Why work at Virgin Pulse?
We believe a career should provide competitive pay and benefits, a collaborative and supportive culture and cutting-edge technology and services. Virgin Pulse is an equal opportunity organization and is committed to diversity, inclusion, equity and social justice. To that end, we make a particular effort to recruit candidates from minoritized backgrounds to apply for open positions.

In compliance with all states and cities that require transparency of pay, the base compensation for this position ranges up to $80,000 annually. Note that salary may vary based on location, skills, and experience. This position is eligible for [10%] target bonus/variable compensation as well as health, dental, vision, mental health and other benefits.
Show Less
Report",3.4,501 to 1000 Employees,2004,Company - Private,Computer Hardware Development,Information Technology,$100 to $500 million (USD)
"Software Engineer, Data Products",$119K - $163K (Glassdoor est.),LaunchDarkly4.1 ★,"Oakland, CA","Please note, before progressing to our application, this position is based in the San Francisco Bay Area and not suitable for remote candidates.
About the Job:
We are looking for exceptional Software Engineers to make a profound impact on how data products will be integrated into companies' software in the future. We are integrating data into everything LaunchDarkly offers on top of our unrivaled feature management platform.
As a Data Products - Software Engineer, you will help us architect and write fast, reliable, and scalable data processing tools to process data from our thousands of customers and their hundreds of millions of users around the world. We're looking for someone who knows what it takes to deliver value to customers and takes pride in the quality of their work.
The primary technologies we use daily include Golang, Scala, Kinesis, and Flink. If working as a part of such a poly-functional team to bring to change how experimentation is done forever appeals to you then come join the Experimentation team at LaunchDarkly.
Responsibilities:
Build and expand our data platform and services
Help us identify the best technologies for our evolving data needs
Collaborate with product team to spec and deliver user-facing features
Monitor and improve data pipeline performance
Actively participate in code reviews
Improve engineering standards, tooling, and processes
Qualifications:
Proven experience and fluency in a JVM or functional language
Experience building data platforms (e.g. using Flink, Kafka, DataFlow, Hadoop, Spark)
Strong communication skills, a positive attitude, and empathy
You write code that can be easily understood by others, with an eye towards maintainability
You hold yourself and others to a high bar when working with production systems
You value high code quality, automated testing, and other engineering best practices
Pay:
Target pay range for a Level P3 in San Francisco/Bay Area: $144,000 - $169,000*
Restricted Stock Units (RSUs), health, vision, and dental insurance, and mental health benefits in addition to salary.
LaunchDarkly operates from a place of high trust and transparency; we are happy to state the pay range for our open roles to best align with your needs. Exact compensation may vary based on skills, experience, degree level, and location.
About LaunchDarkly:
LaunchDarkly is a Feature Management Platform that serves trillions of feature flags daily to help software teams build better software, faster. Feature flagging is an industry standard methodology of wrapping a new or risky section of code or infrastructure change with a flag. Each flag can easily be turned off independent of code deployment (aka ""dark launching""). LaunchDarkly has SDKs for all major web and mobile platforms. We are building a diverse team so that we can offer robust products and services. Our team culture is dynamic, friendly, and supportive. Our headquarters are in Oakland.
At LaunchDarkly, we believe in the power of teams. We're building a team that is humble, open, collaborative, respectful and kind. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status.
One of our company values is 'Widen the Circle'. Which means we seek out diversity of perspectives to get better results. We understand everyone has their own unique talents and experiences. We encourage you to apply to this role even if you don’t think you meet 100% of the qualifications outlined above. We can find out together if it's the right match for your skillset.
We've partnered with KeyValues to help demonstrate the amazing culture we've built here at LaunchDarkly, find more info at https://www.keyvalues.com/launchdarkly.
LaunchDarkly is also committed to giving back to our community and is a part of Pledge 1%, an organization that helps companies make this a priority. Through this initiative and its charitable arm, the LaunchDarkly Foundation, the company is committed to such causes as supporting education for the underserved, homelessness relief and moving towards having a net-zero carbon footprint. You can find more about the LaunchDarkly Foundation and the organizations we serve at https://launchdarkly.com/foundation/.
Do you need a disability accommodation?
Fill out this accommodations request form and someone from our People Operations team will contact you for assistance.
Show Less
Report",4.1,501 to 1000 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD)
Sr. Data Engineer - Remote,-1,Chamberlain Group3.7 ★,Illinois,"Chamberlain Group is a global leader in access solutions with top brands, such as LiftMaster and Chamberlain, found in millions of homes, businesses, and communities worldwide.
As a leader in the Smart Home industry, we boast one of the largest IoT install bases, with innovative products consisting of cameras, locks, card readers, garage door openers, gates and more, all powered by our myQ digital ecosystem.
This role is responsible for providing technical expertise and leadership to design and deliver end-to-end data engineering solutions to support advanced analytics capabilities and drive innovation and decision-making
across Chamberlain.
Essential Duties and Responsibilities
Build and maintain real-time and batch data pipelines across the advanced analytics platform.
Design, develop and orchestrate highly robust and scalable ETL pipelines.
Design and implement Dimensional and NoSQL data modelling as per the business requirements.
Develop highly optimal codebase and perform Spark optimizations for Big Data use cases.
Design, develop and deploy optimal monitoring and testing strategy for the data products.
Collaborate with stakeholders and advanced analytics business partners to understand business needs and translate requirements into scalable data engineering solutions.
Collaborate with data scientists to prepare data for model development and production.
Collaborate with data visualization and reporting application developers to ensure the sustainability of production applications and reports.
Collaborate with data architects on the enhancement of Chamberlain’s enterprise data architecture and platforms.
Provide leadership to third-party contractors.
Comply with health and safety guidelines and rules.
Protect CGI’s reputation by keeping information confidential.
Maintain professional and technical knowledge by attending educational workshops, professional publications, establishing personal networks, and participating in professional societies.
Minimum Qualifications
Education/Certifications:
Bachelor’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
Knowledge, Skills, and Abilities:
Natural sense of urgency, teamwork, and collaboration reflected in daily work ethic.
Proficient in Spark or Databricks, Cloud Data Engineering Services preferably Azure, Streaming frameworks like Event Hubs or Kafka.
Proficient in Microsoft Office.
Familiarity with modern Machine Learning Operationalization techniques.
Agile methodologies.
Familiarity with Data visualization tools, such as Qlik or Power BI.
Preferred Qualifications
Education/Certifications:
Master’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
2+ years of professional experience delivering engineering for advanced analytics or data science solutions
Knowledge, Skills, and Abilities:
Agile methodologies
Experience with IoT Data Architecture.
Machine Learning Operationalization (MLOps) proficiency.
REST API design and development.
Proficiency with streaming design patterns.

The pay range for this position is $103,300.00 to $177.475.00; base pay offered may vary depending on a number of factors including, but not limited to, the position offered, location, education, training, and/or experience. In addition to base pay, also offered is a comprehensive benefits package and 401k contribution (all benefits are subject to eligibility requirements).
This position is eligible for participation in a short-term incentive plan subject to the terms of the applicable plans and policies.
#LI-Remote
We're an organization who values its human capital and provides support to assist its employees succeed.

Chamberlain Group is proud to be an Equal Opportunity Employer. You will be considered for this position based upon your experience and education, without regard to race, color, religion, sex, national origin, age, sexual orientation, ancestry; marital, disabled or veteran status. We are committed to creating and maintaining a workforce environment that is free from any form of discriminations or harassment.

Persons with disabilities who anticipate needing accommodations for any part of the application process may contact, in confidence
Recruiting@Chamberlain.com
.

NOTE: Staffing agencies, headhunters, recruiters, and/or placement agencies, please do not contact our hiring managers via email or phone or other methods.
Apply Now: click Apply Now
Show Less
Report",3.7,1001 to 5000 Employees,1900,Company - Private,Consumer Product Manufacturing,Manufacturing,$500 million to $1 billion (USD)
Senior Data Engineer,$107K - $179K (Employer est.),Wizards of the Coast3.2 ★,"Renton, WA","At Wizards of the Coast, we connect people around the world through play and imagination. From our genre-defining games like Magic: The Gathering® and Dungeons & Dragons® to our growing multiverse, we continue to innovate and build new ways to foster friendship and connection. That's where you come in!
As a Senior Data Engineer for Wizards of the Coast, you will be a technical specialist for our scalable cloud data platform! You will work with a team of engineers in an agile work environment to design and develop data pipelines, tools and infrastructure used by our games and products so that they can handle the volume, variety, and velocity of message types and traffic flowing through it.
You'll also be working to deliver performance optimization, virtualization, and other value adds to our data pipes and ecosystem. You will work with a diverse set of product and engineering teams to design and build innovative data systems that exceed industry standards and provide our analysts, data scientists, marketers, and business teams with clear, accurate, and actionable insights.
What You'll Do:
Help design highly scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of outstanding proprietary and open-source platforms and tools.
Collaborate with engineers, producers, business partners and analysts to elicit, translate, and prescribe requirements for their needs.
Work on technical projects to simplify tough problems, touch many moving parts across functional teams and business domains and influence strategic direction.
Stay ahead of the technology curve and make recommendations about technologies to build current and future products.
Supports the progression of technology solutions from conception through release and into live ops.
Follow and Implement processes, procedures, and documentation standards to ensure compliance with internal Technology and Corporate IT regulations.
What You'll Bring:
3+ years of experience with Cloud data Warehouses such as Synapse, Redshift, or Snowflake.
2+ years of experience working with Cloud Technologies.
Proficiency programming in python and SQL.
Experience with data architecture and data modeling.
Excellent problem-solving and troubleshooting skills.
Experience building scalable data pipelines and ETL/ELT processes.
Experience working in a DevOps environment responding to a system with 24x7 uptime.
Experience with data streaming management.
Experience integrating with third-party APIs.
Nice to Have:
Experience with data pipeline integration tools such as Airflow, or other standalone or integrated pipeline toolsets
Familiarity with Kubernetes or other container orchestration.
Experience working with AWS.
Experience with KAFKA.
We are an Equal Opportunity / Affirmative Action Employer
The above is intended to describe the general content of and the requirements for satisfactory performance in this position. It is not to be construed as an exhaustive statement of the duties, responsibilities, or requirements of the position.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. If you are selected to move forward in our application process and need to request an accommodation, please let your recruiter or coordinator know.
In compliance with local law, we are disclosing the compensation range for this role. The range listed is just one component of Wizards of the Coast’s total compensation package for employees. Employees may also be eligible for annual and long-term incentives. In addition, Wizards of the Coast provides a variety of benefits to employees. Here’s a look at what your benefits package may include: Medical, Dental & Vision Insurance, Paid Vacation Time & Holidays, Generous 401(k) match, Paid Parental Leave, Volunteer Program, Employee Giving & Matching Gifts Programs, Tuition Reimbursement, Product Discounts, and more.
Compensation Range
$106,560—$178,800 USD
Show Less
Report",3.2,1001 to 5000 Employees,1999,Subsidiary or Business Segment,Culture & Entertainment,"Arts, Entertainment & Recreation",$1 to $5 billion (USD)
"Senior Engineer I, Data",$105K - $168K (Employer est.),Exact Sciences Corporation3.5 ★,Remote,"Position Overview
At Exact Sciences, we are cancer fighters. We are united by our mission to change lives by providing earlier, smarter answers. Through advances in cancer detection and treatment guidance, we will help eradicate the disease and the suffering it causes.
As a Senior Engineer I, Data, this seasoned professional will demonstrate competence and creativity in a wide range of technical areas. This role will have a lead role in the design, development, and testing of software applications for Exact Sciences, creating expert design concepts and foolproof debugging. This role will be able to resolve most issues and problems effectively with minimal oversight, communicate effectively with business stakeholders and mentor junior team members. This role will routinely make key decisions for the team, be accountable for application quality, and be highly productive.
Working in our Data Engineering teams, you will collect and analyze data to develop robust IT solutions that deliver advanced data analytics capabilities to the organization. You will develop database architectures to address business requirements, ensuring system scalability, security, performance, and reliability. Additionally, you will design and document database applications such as interfaces, data transfer mechanisms, and data partitions to enable efficient access of the generic database structure.
Essential Duties
Include, but are not limited to, the following:
Has complete knowledge, skills, and understanding of practices for complex programs and initiatives demonstrating creativity and substantial understanding of specialized techniques, processes, procedures.
Troubleshoot issues and problems of medium complexity for major software applications; break down complex tasks, make reasonable decisions, investigate and fix bugs, and operate independently.
Initiate independent designs that impact and influence the department’s delivery and approach. Lead reviewing design alternatives, problems, and solutions with an eye on limiting the need for later problem solving.
Consistently influence and make significant decisions within a project, demonstrating good judgment in selecting methods and techniques for obtaining solutions based in a solid understanding of business strategy and best practices with minimal instructions for difficult or unpredictable situations.
Contribute individual capabilities to the achievement of group objectives, and work effectively with others in a group setting. This includes mentoring, coaching, and technical guidance for junior members of the team.
Build successful internal and external partnerships with peers, SMEs, stakeholders, and decision-makers.
Communicate clear and concise project plans/designs and status updates for a project.
Ability to work nights and/or weekends, as needed.
Uphold company mission and values through accountability, innovation, integrity, quality, and teamwork.
Support and comply with the company’s Quality Management System policies and procedures.
Maintain regular and reliable attendance.
Ability to act with an inclusion mindset and model these behaviors for the organization.
Minimum Qualifications
Bachelor’s Degree in Data Science, Computer Science, Information Systems, Mathematics, or Engineering; or High School Diploma/General Education Degree and 4 years of relevant experience as outlined in the essential duties in lieu of Bachelor’s Degree.
Complete knowledge and full understanding of software development design and development, and relevant domain specific skills (see below).
Spark on Snowflake or Databricks.
Python, Scala, SQL development.
ETL data pipelines.
Designing and implementing data modeling solutions using relational, dimensional, and/or NoSQL databases.
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Big Data file formats (Parquet, Avro, Delta Lake).
Cloud Infrastructure services (i.e., AWS, SQS, S3, and GitLab).
Agile development tools; including, but not limited to, JIRA, Confluence repository.
RestAPI development.
Tableau, ideally including performance optimization.
Demonstrated ability to perform the essential duties of the position with or without accommodation.
Authorization to work in the United States without sponsorship.
#LI-VZ1
Salary Range:
$105,000.00 - $168,000.00
The annual base salary shown is a national range for this position on a full-time basis and may differ by hiring location. In addition, this position is bonus eligible, and is eligible to receive company stock upon hire as well as annually. Benefits offered include a retirement savings plan, paid vacation, holiday and personal days, paid caregiver/parental leave, and health benefits to include medical, prescription drug, dental and vision coverage in accordance with the terms, conditions, and eligibility requirements of the applicable plans.

If you require a reasonable accommodation with the application process, please email
hr@exactsciences.com
.
We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to age, color, creed, disability, gender identity, national origin, protected veteran status, race, religion, sex, sexual orientation, and any other status protected by applicable local, state or federal law. Applicable portions of the Company’s affirmative action program are available to any applicant or employee for inspection upon request.
To view the Right to Work, E-Verify Employer, and Pay Transparency notices and Federal, Federal Contractor, and State employment law posters, refer to
this link
. These documents summarize important details of the law and provide key points that you have a right know.
Start your job application: click Apply Now
Show Less
Report",3.5,1001 to 5000 Employees,1995,Company - Public,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,$100 to $500 million (USD)
Sr. Data Engineer,$78K - $167K (Employer est.),appsintegration,"Atlanta, GA",-1,3.5,-1,-1,-1,-1,-1,-1
Data Engineer,$125K - $140K (Employer est.),ZOLL LifeVest3.4 ★,"Broomfield, CO","Location: Remote, CO, United States of America
Data Management

Who We Are: A leader in emergency software for medics, firefighters, and other lifesavers. We have the resources and stability of a large company, but the nimbleness, ambition, and determination of a startup, because we used to be one.
Who You Are: Someone who enjoys discovering new insights from large volumes of data and takes pride in creating scalable processes around it. The Data Engineer will build pipelines and integrate our data sources across the entire ZOLL Data organization, providing best practices in ETL and data management and design, ensuring our SaaS Data platforms are fully automated, monitored, and stable.
Responsibilities
Key contributor for building our next data platform, including:
Automated discovery and mapping of all data sets
Implementing scalable ETL workflows from a variety of operational data stores to our data warehouse
Participate in the design of standardized data models and APIs
Identify solutions and mechanism to ensure compliance with data rights and regulatory constraints
Create data pipelines for data management and in preparation for statistical analysis
Work effectively with large sets of unstructured and structured data
Thoroughly understand how ZOLL products work and implement data solutions with meaningful insights
Collaborate with data analysts, scientists, and architects on multiple projects
Work with various stakeholders, including product management, to get requirements and implement appropriate data-driven solutions
Stay on top of modern data technologies
Desired Skills and Experience:
Be passionate about data!
BS in Computer Science/Applied Math- MS (preferred)
Equivalent job or military experience will be considered
Advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies, and ETL concepts
Experience with streaming and messaging platforms (Kafka, Rabbit MQ)
5+ years of experience with data engineering, software engineering with data focus, business intelligence, data ETL, and data modeling
2+ years of hands on (daily) ETL tool experience, such as Talend, Glue, Airflow or similar
3+ years of experience with AWS services, such S3, Glue, Kinesis, and RDS.
5+ years of software development experience with C#/C++ and Python including distributed systems
Machine Learning experience strongly desired
Experience with IaC and CI/CD (Terraform) in a cloud environment desired
Experience with HIPAA and GDPR a plus
Salary for this position is $125,000-$140,000 annually. Final compensation will be determined by various factors such as a candidate’s relevant work experience, skills, certifications, and location.

Perks & Benefits
ZOLL provides a casual work environment, competitive salaries, and a compensation structure that rewards contribution and hard work. Our benefits package includes 401(k), medical, dental, vision, life, AD&D, flex spending accounts, STD/LTD, Paid Time Off, tuition assistance, and home office technology allowance.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities will be reviewed periodically as duties and responsibilities change with business necessity. Essential and marginal job functions are subject to modification.
Show Less
Report",3.4,1001 to 5000 Employees,1980,Company - Private,Health Care Products Manufacturing,Manufacturing,$100 to $500 million (USD)
Senior Data Lakehouse Engineer,-1,ProServeIT4.3 ★,Florida,"Do you want to make a big impact on a fast-growing IT organization? Do you want to be part of a team that truly supports employee growth and development? Are you someone with Data Engineering and Business Intelligence experience, combined with the strong interpersonal skills? Then, we want you!

The Senior Data Engineer will help clients define their information strategy, architecture, and governance, get the most value from business intelligence and analytics, and implement enterprise content and data management solutions to enable business insights, reduce cost and complexity, increase trust and integrity, and improve operational effectiveness. The ideal candidate will have a strong combination of technical skills, combined with the interpersonal skills required to work effectively with key technical and non-technical stakeholders in a multi-cultural environment.

Life at ProServeIT is fast paced, performance-driven, rewarding, and fun! We value and support our team members' career growth and ongoing professional development. And we recognize their achievements and outstanding results on a regular basis. We work hard and play hard.

""People Matter. Be like gumby. Do it right."" These are the three values we follow every day. These truly represent who we are and what we care about.

Excited? Read on and apply! Looking forward to hearing from you.

Responsibilities
Define client’s information strategy, architecture and governance
Help client’s get the most value from business intelligence and analytics
Implement enterprise content and data management solutions to enable business insights, reduce cost and complexity, increase trust and integrity, and improve operational effectiveness.
Deliver ETL, BI Data models and Analytic solutions using Microsoft SQL Server Business Intelligence tools, Power BI and Tableau.
Work with teams to deliver effective, high-value reporting solutions by leveraging an established delivery methodology.
Perform data mining and analysis to uncover trends and correlations to develop insights that can materially improve our decisions.
Writing analytics programs (transformations/calculations) using T-SQL and Python or comparable
Qualifications
3+ years Business Intelligence / Data Lakehouse development experience
3+ Years of experience in Data Engineering and ELT development (T-SQL, SparkSQL and PySpark)
Knowledge of the Kimball dimensional modeling process
Experience with:
1. Data Lakehouse
2. Medallion Architecture
3. Autoloader
4. Change Data Feed
5. Delta Lake
6. Azure Data Factory; knowledge on SSIS/SSAS/SSRS
7. Azure DevOps
8. Databricks
9. Synapse
10. Python and R is a plus
Experience visualizing data in business intelligence tools such as Power BI and Tableau, Business Objects or Hana Analytics
Experience creating test plans, testing and resolving data discrepancies
Must be a self-motivated, energetic, detail-oriented team player passionate about producing high quality BI & Analytics deliverables
Must be an enthusiastic, self-motivated, team player in an evolving, dynamic environment
Experience conducting requirements analysis, meeting with business owners, and performing current state system analysis
Ability to manage own time effectively, work with key stakeholders, and provide ongoing deliverables with minimal supervision
Strong communication skills (written & spoken)
Solution-oriented, with the imagination to identify workarounds for critical problems.
Diversity, Inclusion and Accessibility
ProServeIT values diversity of thought and is proud to be an equal opportunity employer. We are committed to creating a diverse and inclusive environment where all people feel supported, connected, and belonging at work. All applications will receive consideration for employment without regard to race, colour, religion, gender, gender identity or expression, national origin, disability, or age. Please let us know if you require any accommodations or support during the recruitment process.
Show Less
Report",4.3,51 to 200 Employees,2002,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
Sr. Azure Data Engineer,$75.00 - $80.00 Per Hour (Employer est.),Kairos Technologies4.6 ★,"Dallas, TX","Hello,
Hope you are doing great!
If you are comfortable with the requirement, kindly respond with below information and your updated resume( word formatted ) ASAP
Full Name:
Current Location:
Contact No Primary:
Work Authorization:
S.S.N No( Last 4 digits ):
D.O.B( MM/DD ):
Availability( Any notice period required to join in project ):
Currently working(Yes/No):
LinkedIn:
Employer details( If on C2C ):
Please check the below requirement
Direct Client Requirement
Position: Sr. Data Engineer with Azure Cloud and Strong SQL( Hybrid/ Onsite for 2 days a week )
Location: Dallas TX or VA
Duration: 12+ Months Long Term Contract
Work Authorization: US Citizen/GC/GC-EAD/TN Visa( on C2C is fine )
Need Locals Candidates..
Job Summary:
We are seeking an experienced and highly skilled Senior Database Engineer with strong hands-on experience in modernizing and automating on-premises SQL databases, as well as a solid track record of successful migration projects from on-premises to Azure cloud. As a Senior Database Engineer, you will play a critical role in driving our database modernization efforts, ensuring scalability, performance, and security in the Azure cloud environment.
Responsibilities:
Lead the modernization and automation of on-premises SQL databases, implementing best practices and efficient processes to enhance performance, scalability, and reliability.
Design and execute successful migration strategies from on-premises databases to Azure cloud, ensuring seamless data transfer and minimal downtime.
Develop and implement automation solutions using tools such as Python, PowerShell, and Azure DevOps (ADO) to streamline database management tasks, including provisioning, backup and recovery, monitoring, and deployment processes.
Collaborate closely with cross-functional teams to gather requirements, understand business needs, and propose effective database solutions that align with the overall technical architecture in the Azure cloud environment.
Utilize tools like Liquibase or similar to manage database schema changes and version control.
Implement effective monitoring and logging solutions using tools like Splunk and Datadog to ensure database performance, availability, and security.
Perform thorough assessments and evaluations of existing on-premises databases, identifying areas for improvement, optimization, and consolidation prior to migration to Azure cloud.
Ensure data integrity, security, and compliance with industry standards and regulations throughout the database modernization and migration process in the Azure cloud environment.
Provide guidance and mentorship to junior database engineers, fostering their technical growth and promoting best practices in database management, automation, and migration in the Azure cloud.
Stay up to date with the latest trends, tools, and technologies in database management, Azure cloud services, and DevOps practices, evaluating their potential impact and relevance to our organization.
Collaborate with vendors and third-party providers to assess and implement new technologies, tools, and services that can enhance our Azure cloud database environment and support our modernization goals.
Qualifications:
Bachelor's degree in computer science, information technology, or a related field.
Minimum of 10 years of hands-on experience in database engineering, with a strong focus on SQL databases and Azure cloud.
Extensive experience in modernizing and automating on-premises SQL databases, including performance optimization, scalability improvements, and process automation.
Proven expertise in successfully migrating on-premises databases to Azure cloud, ensuring data integrity and minimal disruption to operations.
Strong proficiency in SQL scripting, database performance tuning, backup and recovery, and database security practices in the Azure cloud environment.
Experience with database migration tools and technologies, such as Azure Database Migration Service, Azure Data Factory, or similar tools.
Solid understanding of Azure cloud database services, including Azure SQL Database, Azure SQL Managed Instance, and Azure Cosmos DB.
Familiarity with automation and configuration management tools like Python, PowerShell, and Azure DevOps (ADO) for streamlining database management and deployment processes in Azure.
Experience in using tools like Liquibase or similar for managing database schema changes and version control.
Knowledge of monitoring and logging solutions like Splunk and Datadog for database performance and security monitoring in Azure cloud.
Strong analytical and problem-solving skills, with the ability to troubleshoot complex database issues and propose effective solutions in the Azure cloud environment.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.
Tech Stack:
Databases: On-premises SQL databases (e.g., Oracle, MySQL, PostgreSQL)
Cloud Platform: Microsoft Azure (Azure SQL Database, Azure SQL Managed Instance, Azure Cosmos DB)
Scripting and Automation: Python, PowerShell
Version Control: Git
CI/CD: Azure DevOps (ADO)
Database Migration: Azure Database Migration Service, Azure Data Factory
Database Schema Management: Liquibase or similar tools
Monitoring and Logging: Splunk, Datadog
Thanks and Regards
Prasad Mamidela | Kairos Technologies Inc.
Direct Number: 972.777.9484 | Mobile: 201.613.3664
433 E Las Colinas Blvd, # 1240, Irving, TX 75039 USA
http://www.kairostech.com
LinkedIn: https://www.linkedin.com/in/prasadmamidela
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 4 years (Required)
SQL: 6 years (Required)
On-premises: 2 years (Required)
Total IT: 10 years (Required)
Azure DevOps: 2 years (Required)
Work Location: In person
Show Less
Report",4.6,51 to 200 Employees,2003,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
Cloud Data Platform Engineer,$88K - $126K (Glassdoor est.),Genoa Employment Solutions Inc4.7 ★,"Ann Arbor, MI","Position Summary:
Reporting to the Data Platform Manager, and dotted line into the Director of Transformation, the Cloud Data Platform Engineer is responsible for the technical strategy, design, planning, implementation, performance tuning and recovery procedures for cloud database systems in the enterprise and will serve as a technical expert in the area of modern database technologies and operations.
Duties and Responsibilities:
Develops Database strategy to support the cloud and containerization roadmap for cloud and on-premise environments
Defines and executes on automation activities related to environment setup, configuration, deployment, and recovery
Develops a methodology for the ongoing assessment of database performance and the identification of problem areas
Define and design global solutions for multi region high availability, cloud failover and disaster recovery, including in the future multi-cloud
Develop and rollout a hybrid cloud/on premise database model that supports hot/hot failover
Coordinate with the Cloud and Containerization team to analyze the business drivers that determine key architecture requirements and solutions
Provides leadership during the development and enhancement of production applications including working with applications, technical support and operations during the design, development, and implementation of applications
Provides guidance and mentoring across the organization in all aspects of the supported areas.
Maintain awareness of industry developments, technology, and trends
Reviews design tools, technologies, process, and approaches to make best in practice and innovative recommendations
Provides technical support for the database environment and provides guidance to the development team on new service implementations
Leads database performance benchmarks and implements performance enhancements
Responsible for day-to-day database support and maintenance activities, such as database backups, restores for development env to mission-critical production systems to meet or exceed business and IT needs
Develop, manage and support of 24x7 proactive monitoring & alerting of databases environments, working with both cloud and development teams to resolve issues and part of on-call support rotation
Logical and physical database design, and documentation where applicable
Translation of log files to actionable and measurable tasks
Planning and execution of database or database asset migrations, combined with modernization, upgrades, transformation and optimization
Responsible for Database Server installation, configuration, and set-up
Infrastructure troubleshooting in partnership with the cloud team
Qualifications:
Bachelor's Degree in computer science or other business-related field or equivalent experience
8-10 years of experience as a DBA in a mixed Operating System environment
5+ years of experience on cloud technologies in AWS, Azure or GCP
5+ years of working experience in large enterprise(s)
Very strong knowledge and experience in DB performance tuning
5+ years of experience in any three of these respective areas at minimum - MS SQL Server, Azure SQL, Cosmos DB, Couchbase
Hands On experience with SQL Always-On technology
Hands On experience with NoSQL
Experience of OS administration and Active Directory
Knowledge of data modeling concepts
Excellent written and verbal communication skills
Experience with successful project management
Ancillary Qualifications:
Qualifications listed below, while not necessarily a requirement for this position in each circumstance, provide additional depth and value to the role.
Masters degree in related field.
MS SQL certifications
Cloud Certifications (AWS, Azure, GCP)
Show Less
Report",4.7,Unknown,-1,Company - Private,Staffing & Subcontracting,Human Resources & Staffing,Unknown / Non-Applicable
Sr. Azure Data Engineer(Databricks),$80.00 - $100.00 Per Hour (Employer est.),Health,Remote,-1,4.7,-1,-1,-1,-1,-1,-1
"Senior / Principal Software Engineer, Data Platform",$140K - $190K (Employer est.),NewLimit,"San Francisco, CA",-1,4.7,-1,-1,-1,-1,-1,-1
Data Engineer,$130K - $165K (Employer est.),Fabletics4.4 ★,California,"Job Description
How Do You Fit In?
We are currently seeking a highly skilled and motivated Data Engineer to join our dynamic team. If you have a passion for data analysis, possess strong technical skills, and are excited about leveraging data to drive business insights, this position is for you. As a Data Engineer, you will join a tight knit group of key contributors who are actively working together to achieve aggressive goals and meet timelines to drive the business forward.

This position will report to the Manager, Data Engineering (EDW), Data Platforms.

What You Will Do:
Work closely with stakeholders to understand their data needs and provide data-driven solutions.
Design, develop, and maintain scalable data pipelines and ETL processes to collect, process, and analyze large volumes of structured and unstructured data.
Solid understanding and experience in data modeling, including designing and implementing efficient data models.
Build and optimize data models, data warehouses, and data marts to support reporting and analytics needs.
Perform exploratory data analysis to identify trends, patterns, and insights that contribute to business decision-making.
Stay up-to-date with the latest trends, tools, and technologies in data analytics and apply them to enhance data-driven decision-making processes.
Perform code reviews to ensure data engineering best practices, code quality, and maintainability.
Provide production support on a rotating basis to ensure the availability, reliability, and performance of data analytics systems and processes.

What You Can Bring:
Bachelor’s degree in Computer science, Engineering, Mathematics, Statistics, or a related field. A master’s degree is a plus.
4 years of experience in creating and managing data pipelines.
1 year of experience with Python or another scripting language.
Experience in performing code reviews and ensuring adherence to best practices.
Excellent communication and presentation skills, with the ability to effectively convey complex data findings to non-technical stakeholders.
Proven experience in data analytics, data engineering, or a related role.
Strong expertise in SQL, particularly in an analytics/reporting capacity, with significant experience in creating and maintaining reporting processes.
Nice to Have:
Familiarity with traditional data warehousing concepts (e.g., Kimball methodology).
Experience in data engineering on cloud platforms such as BigQuery, Redshift, Snowflake, Teradata, Vertica, etc.
Knowledge of Dbt and/or Airflow for data pipeline management.
Previous experience in e-commerce, retail, or internet industries.
Compensation & Total Rewards:
At TechStyleOS, we believe work and life should fit together! We continue to build a culture of flexibility, to empower you to do your best and put yourself first. Our Total Rewards program rewards employees for their hard work, supporting their health, well-being, families, and ultimately their life journey. Total Rewards at TechStyleOS includes:

Hybrid Work Schedule*
Unlimited Paid Time Off*
Summer Fridays*
Healthcare Plans
Employee Discounts
401k
Annual Bonus Program
Equity Program*
And More

Varied for retail and fulfillment roles

The annual base salary range for this position is from $130,000-$165,000. The range provided includes the base salary that TechStyleOS expects to pay for the role. Offered base salary will be dependent on factors including the scope and complexity of the role, candidate’s related work experience, subject matter expertise and work location.
#LI-GR1
#LI-TechStyleOS
About TechStyleOS
TechStyleOS is the globally integrated Operations and Services provider behind some of the fastest growing online fashion brands in history, including Fabletics, Savage X Fenty, JustFab, ShoeDazzle, and FabKids. With capabilities spanning technology, data science, supply chain management, fulfillment, customer service, and more, we help brands launch, scale and grow—across product categories and geographically. From predictive analytics to data-driven marketing and attribution, our unique approach is powered by our proprietary, end-to-end tech platform that enables the brands we serve to deliver a level of personalization, value, and satisfaction that are unrivaled in the fashion industry.
Fabletics, Inc. is an equal opportunity employer. We recruit, employ, compensate, develop, and promote regardless of race, national origin, religion, sex, sexual orientation, gender identity, age, disability, genetic information, veteran status, and other protected status as required by applicable. At Fabletics, Inc., we champion a vibrant workplace culture that thrives on diversity law and do not tolerate discrimination or harassment. We are one team from many backgrounds, innovating through diversity of individuals, who are driven by passion for creating an inclusive space for all. Fabletics, Inc. will continue to champion a workplace culture that prizes diversity and inclusivity.
We encourage you to apply regardless of meeting all qualifications and/or requirements.
Start your job application: click Apply Now
Show Less
Report",4.4,Unknown,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Data Engineer,$150K - $180K (Employer est.),"Amino, Inc.4.4 ★","San Francisco, CA","About Amino - What We Do
At Amino, we are a leading innovator in healthcare, empowering individuals to take charge of their healthcare journey. Our powerful digital tools help navigate and unlock the full potential of health plan options. With Amino's cutting-edge solutions, health plan members can easily understand and make informed decisions about their benefits, considering factors such as available programs, provider network quality, and cost.
We are excited to announce that we have recently secured additional funding to fuel our growth and continue developing market-leading navigation tools. As part of this journey, we are looking for a talented and driven Data Engineer to join us. You will play a vital role in helping millions of people understand and optimize their health plan options.
The Role
As a Data Engineer at Amino, you will be at the heart of our organization. Your primary responsibilities will involve processing, transforming, and organizing large and complex datasets that power our products. We are expanding our data sources and enhancing our current offerings, and we need a Data Engineer to help us build competitive and compliant healthcare guidance products.
Key Priorities and Projects:
Collaborate with Product Managers, Architects, and senior leadership to translate high-level requirements into detailed plans.
Integrate new data sources from vendors and internal systems into our ETL (Extract, Transform, Load) pipelines.
Enhance our editorial tools for creating and maintaining accurate data about health entities.
Modernize our orchestration infrastructure using open-source tools like dbt and Dagster.
Impact you will have:
Revamp and expand critical data pipelines that drive our existing and future products.
Mentor other junior data engineers and share your expertise.
Collaborate closely with data scientists and software engineers in related teams.
Influence the foundational aspects of data modeling and warehousing, benefiting various teams across the company.
Utilize modern data stack technologies to build the foundation for Amino's next-generation data assets.
Technologies you will work with:
Snowflake, Python 3, dbt, Docker, AWS, Looker, Databricks, Spark, Jenkins CI/CD, Airflow, Dagster, Terraform, PostgreSQL, RDS, Elasticsearch, and Kinesis.
Skills and Experience you should possess:
Familiarity with various relational and non-relational databases.
Proficiency in writing clean and well-tested code using Python.
Experience handling and working with large datasets and the associated tools.
Previous exposure to ETL pipelines and familiarity with data modeling and data warehousing best practices (experience with dbt is preferred).
Strong collaboration and feedback skills, with a willingness to seek and incorporate input from others.
Excellent documentation and verbal communication skills, capable of conveying technical concepts to peers and non-technical stakeholders.
Bonus Skills:
Experience handling PHI or PII
Deep understanding of the healthcare domain, particularly with healthcare claims data.
We offer
We're committed to helping you achieve your best work in a supportive, growth-oriented environment. We have seriously big goals, and expectations are high and we'll equip you with the tools and resources you need to be successful.
Expected base salary: $150k to $180k plus standard company benefits and a generous option grant. Amino values transparency and has included the reasonable estimate of the base salary range for this full-time role at any approved US location. Individual pay is determined by a range of factors, including job-related skills, experience, relevant education or training, licensure or certifications, and other business and organizational needs. Amino does not typically hire at or near the top of a salary range.
We offer full-time employees 100% paid employee healthcare premiums; dependent premium coverage depends on the plan.
401(k) and FSA programs
This position, like all roles at the firm, will have a good deal of autonomy. We're a remote-first team and have designed our culture for a balance of synchronous and asynchronous work with people operating from all over the country. To support your remote office, we provide every new Amino with a generous office set-up allowance plus a monthly stipend for internet/phone.
PTO is non accrual and we expect Amino's to take a minimum of 15 days a year.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We know the reputation and track record that the tech industry has, and work hard to be exceptional in this regard.
Our Culture
We are a small team who believes that success is a group activity. You should expect to learn from everyone at Amino, and be excited to share your knowledge. You will play a big part in influencing the shape of the product and be empowered to provide your thoughts and ideas.
We believe in collaboration, respect, and curiosity. We believe in having a growth mindset, and have a passion for solving problems that have never been faced before. Everyone's input is valued, be it about code, data models, business models, or product ideas.
Show Less
Report",4.4,1 to 50 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Senior Data Engineer (Azure),$93K - $135K (Glassdoor est.),Momentum Consulting4.4 ★,"Miami, FL","Momentum takes pride in establishing a team of highly skilled professionals to deliver IT consulting services to our Fortune 500 clients. We provide successful technology solutions and solve critical business challenges to meet our client’s needs. Our alliances with industry-leading technology organizations have been instrumental to our success and allow us to offer robust solutions that are highly scalable and supportable using the best technologies available.

We are seeking to hire a seasoned Senior Data Engineer to join our team. To succeed in this data engineering position, the cloud engineer should have strong analytical skills and the ability to combine data from different sources. You will use various methods to develop raw data into useful data systems. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods. Overall, you’ll strive for efficiency by aligning data systems with business goals.

About the job:
Responsible for the development and management of a Microsoft Azure cloud data platform leveraging Azure Data Lake Storage Gen2, Azure Synapse Analytics, and Azure Data Factory
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
Work with data and analytics experts to strive for greater functionality in our data systems.

About You:
Bachelor’s Degree in Computer Science or a related discipline
Minimum of 5-7 years of data warehousing, data lifecycle management, and computer programming experience
MS Azure: Azure Data Lake Store, Azure Synapse
Azure Data Factory/SSIS or related tool
Business visualization tools, Semantic Layer – Views and Dimensional Modeling – Power BI knowledge is a plus
Experience in database design and data modeling
Experience with structured query language (SQL)
Strong learning orientation and curiosity; comfortable learning new systems/software applications
Strong analytical thinking skills and problem-solving skills
Solid written and oral communication skills
Strong proficiency in Python with an emphasis in building data pipelines
Knowledge of scheduling, logging, monitoring and alert frameworks
Knowledge of Data infrastructure

About us:
Recently honored for the eleventh consecutive year as one of the “Top 10 Best Places to Work” by the South Florida Business Journal. We offer a great working environment and provide our team members with the opportunity to enhance and expand their professional development. Come join our team and take part in our excellent compensation package that includes a competitive salary, health benefits, and a 401K plan. We look forward to hearing from you!

Momentum Consulting offers a great working environment and provide our team members with the opportunity to enhance and expand their professional development. Come join our team and take part in our excellent compensation package that includes competitive salary, health benefits, and a 401K plan. We look forward to hearing from you!

Momentum Consulting Corp. is an Equal Opportunity Employer.
Show Less
Report",4.4,51 to 200 Employees,1994,Private Practice / Firm,Business Consulting,Management & Consulting,$25 to $100 million (USD)
Expert Data Engineer (Remote),$82K - $119K (Glassdoor est.),Experian4.2 ★,"Costa Mesa, CA","Company Description

Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been named in the 100 “World’s Most Innovative Companies” by Forbes Magazine. Experian Consumer Information Services is redefining the way our clients do business within all aspects of the customer credit lifecycle. Fueled by best-in-class data and innovative technology we help businesses make smarter decisions, identify consumers, make decisions on loans, market to prospects and collect.

Job Description

We are looking for an Expert ETL Data Engineer who will be responsible for building data pipelines, data warehouse solutions, and analytics processing tools to democratize data.
About us, but we’ll be brief
Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been named in the 100 “World’s Most Innovative Companies” by Forbes Magazine.
This position will be supporting the Experian Consumer Services - a passionate and innovative team with a mission to provide Financial Power to All™. Our portfolio offers credit education and identity protection solutions to consumers and helps businesses manage the impact of a data breach.
What you’ll be doing
Design and develop foundational components of the BI platform which includes Data Engineering, Data Quality, and Data Catalog framework.
Partnering with Program Managers, Subject Matter Experts, Architects, Engineers, and Data Scientists across the organization where appropriate to understand customer requirements, design prototypes, and optimize existing data services/products.
Design and maintain data warehouse solutions that allow for large-scale analytics processing.
Build scalable ETL data pipelines, ingesting high volume of data from internal and external sources.
Build and maintain Data Engineering solutions that support self-service BI Platform, partnering with BI Engineers to deliver end to end solutions.
Work closely with product teams to understand business success criteria, translate business needs into technical requirements, worked with other Data Engineers and enable project success.
Ability to develop high-performing data engineering team with an inspiring leadership style.
Curious, detail oriented, and highly motivated self-starter. Able to work independently and with the team to formulate innovative solutions
Troubleshoot and resolve data, system, and performance issues.
Participant in production support on-call rotation
#LI-REMOTE

Qualifications

What your background looks like
Minimum 10 years of experience in Data Engineering development and support
5 years of experience within big data domain
5 years of experience in Python scripting
5 years of experience with AWS ecosystem (Redshift, EMR, MWAA, S3, etc.)
2 years of experience with BI tools such as Tableau, Alteryx
5 years of experience in Agile development methodology
Excellent communication skills
Ability to understand complex metrics and translate business requirements to technical requirements
Ability to multitask and prioritize an evolving workload in a fast pace environment.
Proven track record of leading large-scale project
Education: BS degree or higher in computer science or related fields

Additional Information

All your information will be kept confidential according to EEO guidelines.
Our compensation reflects the cost of labor across several U.S. geographic markets. The base pay range for this position is listed above. Within this range, individual pay is determined by work location and additional factors such as job-related skills, experience and education. This position is also eligible for a variable pay opportunity and a comprehensive benefits package which includes health, life and disability insurance, generous paid time off including paid parental and family care leave, an employee stock purchase plan and a 401(k) plan with a company match.
Experian is proud to be an Equal Opportunity and Affirmative Action employer. We’re passionate about unlocking the power of data to transform lives and create opportunities for consumers, businesses, and society. For more than 125 years, we’ve helped people and economies flourish – and we’re not done.
We take our people’s agenda very seriously. We focus on what truly matters; diversity and inclusion, work/life balance, flexible working, development, collaboration, wellness, reward & recognition, volunteering, making an impact... the list goes on. See our DEI work in action!
The power of YOU. We are building a culture where everyone is comfortable bringing their whole self to work. A place where we not only respect our differences and values but celebrate them in a positive and supportive environment.
Find out what is like to work for Experian and discover the Unexpected!
Apply Now: click Easy Apply
Show Less
Report",4.2,10000+ Employees,1980,Company - Public,Business Consulting,Management & Consulting,$5 to $10 billion (USD)
Data Engineer I,$80K (Employer est.),VirginPulse3.4 ★,Remote,"Overview:
Now is the time to join us!
At Virgin Pulse we value and celebrate diversity and we are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Why? Because diversity inspires innovation, collaboration, and challenges us to produce better solutions. But more than this, diversity is our strength, and a catalyst in our ability to #changelivesforgood.
Responsibilities:
Who are you?
Data Engineer I perform development activities with the guidance of another member of the data engineering team. You will work closely with account management, ETL, data warehouse, business intelligence, and reporting teams as you develop data pipelines and enhancements and investigate and troubleshoot issues.

In this role you will wear many hats, but your knowledge will be essential in the following:
Extracting, cleansing, and loading data.
Building data pipelines using SQL, Python, and other technologies.
Triage incoming bugs and incidents.
Perform technical operation tasks.
Investigate and troubleshoot issues with data and data pipelines.
Participation in sprint refinement, planning, and kick-off to help estimate stories, raise awareness and additional implementation details.
Help monitor areas of the data pipeline and raise awareness to team when issues arise.
Performing quality assurance work to verify the accuracy of code and data results
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Qualifications:
What you bring to the Virgin Pulse team
In order to represent the best of what we have to offer you come to us with a multitude of positive attributes including:
1– 2 years or less experience in data engineering
SW certification or degree in IT related field
You also take pride in offering the following Core Skills, Competencies, and Characteristics:
Solid grasp of modern relational and non-relational models and differences between them.
Proficiency in writing SQL, the use of Excel, and some analytical tools.
Understanding of REST API.
Understanding of JSON.
Detail oriented and able to examine data and code for quality and accuracy.
Knowledge of Agile environments, including Scrum and Kanban methodologies
Python / R / programming language experience preferred
ETL experience preferred
AWS Lambda / Console experience preferred
Git experience preferred
No candidate will meet every single desired qualification. If your experience looks a little different from what we’ve identified and you think you can bring value to the role, we’d love to learn more about you!

#LI-REMOTE

Why work at Virgin Pulse?
We believe a career should provide competitive pay and benefits, a collaborative and supportive culture and cutting-edge technology and services. Virgin Pulse is an equal opportunity organization and is committed to diversity, inclusion, equity and social justice. To that end, we make a particular effort to recruit candidates from minoritized backgrounds to apply for open positions.

In compliance with all states and cities that require transparency of pay, the base compensation for this position ranges up to $80,000 annually. Note that salary may vary based on location, skills, and experience. This position is eligible for [10%] target bonus/variable compensation as well as health, dental, vision, mental health and other benefits.
Show Less
Report",3.4,501 to 1000 Employees,2004,Company - Private,Computer Hardware Development,Information Technology,$100 to $500 million (USD)
Sr. Data Engineer - Remote,-1,Chamberlain Group3.7 ★,Illinois,"Chamberlain Group is a global leader in access solutions with top brands, such as LiftMaster and Chamberlain, found in millions of homes, businesses, and communities worldwide.
As a leader in the Smart Home industry, we boast one of the largest IoT install bases, with innovative products consisting of cameras, locks, card readers, garage door openers, gates and more, all powered by our myQ digital ecosystem.
This role is responsible for providing technical expertise and leadership to design and deliver end-to-end data engineering solutions to support advanced analytics capabilities and drive innovation and decision-making
across Chamberlain.
Essential Duties and Responsibilities
Build and maintain real-time and batch data pipelines across the advanced analytics platform.
Design, develop and orchestrate highly robust and scalable ETL pipelines.
Design and implement Dimensional and NoSQL data modelling as per the business requirements.
Develop highly optimal codebase and perform Spark optimizations for Big Data use cases.
Design, develop and deploy optimal monitoring and testing strategy for the data products.
Collaborate with stakeholders and advanced analytics business partners to understand business needs and translate requirements into scalable data engineering solutions.
Collaborate with data scientists to prepare data for model development and production.
Collaborate with data visualization and reporting application developers to ensure the sustainability of production applications and reports.
Collaborate with data architects on the enhancement of Chamberlain’s enterprise data architecture and platforms.
Provide leadership to third-party contractors.
Comply with health and safety guidelines and rules.
Protect CGI’s reputation by keeping information confidential.
Maintain professional and technical knowledge by attending educational workshops, professional publications, establishing personal networks, and participating in professional societies.
Minimum Qualifications
Education/Certifications:
Bachelor’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
Knowledge, Skills, and Abilities:
Natural sense of urgency, teamwork, and collaboration reflected in daily work ethic.
Proficient in Spark or Databricks, Cloud Data Engineering Services preferably Azure, Streaming frameworks like Event Hubs or Kafka.
Proficient in Microsoft Office.
Familiarity with modern Machine Learning Operationalization techniques.
Agile methodologies.
Familiarity with Data visualization tools, such as Qlik or Power BI.
Preferred Qualifications
Education/Certifications:
Master’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
2+ years of professional experience delivering engineering for advanced analytics or data science solutions
Knowledge, Skills, and Abilities:
Agile methodologies
Experience with IoT Data Architecture.
Machine Learning Operationalization (MLOps) proficiency.
REST API design and development.
Proficiency with streaming design patterns.

The pay range for this position is $103,300.00 to $177.475.00; base pay offered may vary depending on a number of factors including, but not limited to, the position offered, location, education, training, and/or experience. In addition to base pay, also offered is a comprehensive benefits package and 401k contribution (all benefits are subject to eligibility requirements).
This position is eligible for participation in a short-term incentive plan subject to the terms of the applicable plans and policies.
#LI-Remote
We're an organization who values its human capital and provides support to assist its employees succeed.

Chamberlain Group is proud to be an Equal Opportunity Employer. You will be considered for this position based upon your experience and education, without regard to race, color, religion, sex, national origin, age, sexual orientation, ancestry; marital, disabled or veteran status. We are committed to creating and maintaining a workforce environment that is free from any form of discriminations or harassment.

Persons with disabilities who anticipate needing accommodations for any part of the application process may contact, in confidence
Recruiting@Chamberlain.com
.

NOTE: Staffing agencies, headhunters, recruiters, and/or placement agencies, please do not contact our hiring managers via email or phone or other methods.
Start your job application: click Apply Now
Show Less
Report",3.7,1001 to 5000 Employees,1900,Company - Private,Consumer Product Manufacturing,Manufacturing,$500 million to $1 billion (USD)
Software Engineer | Data Platform,$153K - $180K (Employer est.),Ramp Financial4.5 ★,"New York, NY","Location
New York, Miami, Remote
Type
Full time
Department
Engineering

About Ramp
Ramp is building the next generation of finance tools—from corporate cards and expense management, to bill payments and accounting integrations—designed to save businesses time and money with every click. Over 12,000 customers cut their expenses by 3.5% per year and close their books 8x faster by switching to the Ramp platform.
Founded in 2019, Ramp powers the fastest-growing corporate card and bill payment software in America and enables billions of dollars of purchases each year. Ramp continues to grow quickly, more than doubling its revenue run rate in the first half of 2022.
Valued at $8.1 billion, Ramp's investors include Founders Fund, Stripe, Citi, Goldman Sachs, Coatue Management, D1 Capital Partners, Redpoint Ventures, General Catalyst, and Thrive Capital, as well as over 100 angel investors who were founders or executives of leading companies. The Ramp team comprises talented leaders from leading financial services and fintech companies—Stripe, Affirm, Goldman Sachs, American Express, Mastercard, Visa, Capital One—as well as technology companies such as Meta, Uber, Netflix, Twitter, Dropbox, and Instacart. Ramp was named Fast Company’s #1 Most Innovative Company in North America in 2023 and #5 on LinkedIn Top Startups 2022.
About the Role
The Data Platform team develops and owns the systems that enable Ramp's reporting and strategic decision-making, as well as integrating machine learning models into our Risk systems and the product. As a member of the Data Platform team, you’ll build and maintain the infrastructure that enables Ramp to realize value from data. You’ll also partner with Ramp’s analytics engineers, data scientists, and other data professionals to build internally and externally facing data products.
Our ideal candidate is excited about building systems for data collection, processing, storage, and retrieval, and is also passionate about making these systems observable, reliable, scalable, and highly automated.
What You’ll Do
Build and integrate the components of Ramp's Analytics Platform and Machine Learning Platform
Build tools that improve the agility and data experience of Ramp's Data Scientists, Analytics Engineers, Engineers, and Operations teams
Build the batch and streaming data pipelines critical to Ramp’s daily operations using Airflow, Snowflake, Materialize, and other data processing technologies
Collaborate with stakeholder teams on building and productionizing analytical products and machine learning models
Build reliable, scalable, maintainable, and cost-efficient systems across the stack
What You Need
Minimum 2 years of experience with workflow orchestrators like Airflow, Dagster, or Prefect
Minimum 2 years of experience building infrastructure on AWS, GCP, or Azure
Knowledge of SQL and experience with Snowflake, Redshift, BigQuery, or similar databases
Intuition around analytics and machine learning
Strong Python programming skills
Track record of building highly-reliable infrastructure for data storage and processing
Nice to Haves
Expertise with AWS
Expertise with the Modern Data Stack - dbt, Looker, Snowflake, and Fivetran
Expertise with building and deploying machine learning systems.
Experience with Terraform and Datadog
Compensation
The annual salary/OTE range for the target level for this role is $153,000-$180,000 + target equity + benefits (including medical, dental, vision, and 401(k)
Ramp Benefits (for U.S. based employees)
100% medical, dental & vision insurance coverage for you
Partially covered for your dependents
One Medical annual membership
401k (including employer match)
Please note only 401k contributions made while employed by Ramp are eligible for an employer match
Flexible PTO
Fertility HRA (up to $5,000 per year)
WFH stipend to support your home office needs
Wellness stipend
Parental Leave
Relocation support
Pet insurance
Show Less
Report",4.5,201 to 500 Employees,2019,Company - Private,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
Data Engineer,$110K (Employer est.),Capitol Federal2.9 ★,"Topeka, KS","Job Description:
Pay: up to $110,000 Annually
Job Type: Full Time
The Data Engineer assists in setting overall development roadmap and standards for the Bank and helps evaluate and architect the use of data solutions, using industry best practices. This position works as part of a collaborative team to design, code, and implement data solutions to support internal business requirements or external customers and vendors. An innovative mindset and an ability to translate complex business scenarios into a technical solution is required. This position performs a variety of tasks under general supervision. The position reports directly to an IT manager and requires regular, predictable and timely attendance at work to meet department workload demands.
Paid time off and holiday available on your first day! Benefits available to anyone working 20 hours or more per week!
CapFed® is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Job Type: Full-time
Work Location: In person
Show Less
Report",2.9,501 to 1000 Employees,1893,Company - Public,Banking & Lending,Financial Services,$100 to $500 million (USD)
"Software Engineer, Data Platform",$120K - $185K (Employer est.),SentiLink5.0 ★,Remote,"SentiLink is building the future of identity verification in the United States. The existing ways to determine if somebody is who they claim to be are too clunky, ineffective, and expensive, but we believe strongly that the future will be 10x better.
We’ve had tremendous traction and are growing extremely quickly. Already our real-time APIs have helped verify hundreds of millions of identities, beginning with financial services. In 2021, we raised a $70M Series B round, led by Craft Ventures to rapidly scale our best in class products. We’ve earned coverage and awards from TechCrunch, CNBC, Bloomberg, Forbes, Business Insider, PYMNTS, American Banker, LendIt, and have recently landed on the Forbes Fintech 50, 2023. Last but not least, we’ve even been a part of history-we were the first company to go live with the eCBSV and testified before the United States House of Representatives.
Role:
As a software engineer on the Data Platform team at SentiLink, you will own the data infrastructure components to support the SentiLink suite of products. You will work with product, engineering, and data science teams across the company to build, enhance and modify the data platform that powers our fraud detection products. You have outstanding programming skills and are proficient in our technology stack and pick up new technologies quickly as we evolve.
Responsibilities:
Build, expand, and optimize data architecture in order to create the most accurate dataset of identities and their relationships
Develop and operate scalable and resilient data stores and distributed data processing infrastructures to meet business requirements
Design, develop, test and support a suite of API-based data products
Enable the data science team by ensuring data availability at scale and efficiency
Partner with product management to drive agile delivery of both existing and new offerings
Ensure data platform and services meet SLA and quality requirements; on call rotation for production issues, along with the rest of engineering
Develop functional subject matter expertise within various areas of identity fraud domain
Requirements:
5+ years of software product development experience
3+ years of development experience in python or golang and related technologies and frameworks
Strong expertise in big data technologies and distributed data processing frameworks such as Spark, Kafka, Hadoop etc
Experience with public cloud platforms such as AWS, Microsoft Azure or GCP
Deep understanding of different database technologies including but not limited to RDBMS (e.g. postgres), NoSQL (Elasticsearch, vector DB), Columnar data stores etc. and experience with writing efficient queries and optimization techniques.
Proven track record of building and delivering enterprise grade, scalable, data-intensive backend services on Kubernetes or similar platforms.
Excellent analytical and problem solving skills, interpersonal skills and a sense of humor (enjoy the journey)
Experience working in a scrum / Agile development environment
Bonus points if you have..
experience working with Spark/EMR
built real time streaming applications
experience with AWS technologies such as EKS, sqs/sns, emr, redshift, s3 etc
Infrastructure-as-code (IAC) such as terraform, CloudFormation etc
prior experience working in a fintech startup
Candidates must be legally authorized to work in the United States and must live in the United States
Salary Range:
$120,000/year - $185,000/year
Perks:
Remote (or hybrid, if specified) work with home office stipend, and regular company-wide in-person events
Lunch, coffee and snacks during the work day are fully reimbursable
Employer paid group health insurance for you and your dependents
. . . and other typical start-up benefits (e.g. 401(k) plan with employer match, flexible paid time off, etc.)
Corporate Values:
Follow Through
Deep Understanding
Whatever It Takes
Do Something Smart
Show Less
Report",5.0,51 to 200 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
Senior Data Engineer (Remote),$170K (Employer est.),Home Depot / THD3.8 ★,"Atlanta, GA","Position Purpose:
The Senior Data Engineer will lead the approach and execution of integrating new datasets that support analytics and data science initiatives for Home Depot's Customer Experience organization. They will combine, organize, and automate data sets from the contact center, e-commerce, transactional, supply chain, and operational sources. Furthermore, the role entails stitching and migrating various customer-related data sources into Google BigQuery, maintaining the data warehouse, designing and performing QA, and providing ad-hoc reporting.

Key Responsibilities:
50% Design and develop robust, user friendly applications, reports and dashboards using BI tools like Microstrategy and SAS
25% Partner with leaders to identify needs and gather requirements. Provide solutions by building tools, reports and predictive models. Automate reporting as needed
25% Research and document best practices and standards for using our BI tools. Provide insight on industry trends

Direct Manager/Direct Reports:
Position reports to Sr Manager or Manager, Online Business Intelligence.
Position has no direct reports.

Travel Requirements:
Typically requires overnight travel less than 10% of the time.

Physical Requirements:
Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.

Working Conditions:
Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.

Minimum Qualifications:
Must be eighteen years of age or older.
Must be legally permitted to work in the United States.

Preferred Qualifications:
Expert in SQL (preferably BigQuery)
Strong knowledge of Python
Strong knowledge of Airflow
Prior experience with data stitching process (the process of combining data sets from different sources and devices to get deep insights into customer behavior and journey - such as stitching data from call centers to customer orders)
Demonstrates strong ability in translating business needs into technical specifications towards building BI solutions
Prior experience in migrating data on a regular cadence from one platform to another
Prior experience in researching and analyzing company data to support statistical analysis by the business
Prior retail industry experience is preferred


Minimum Education:
The knowledge, skills and abilities typically acquired through the completion of a bachelor's degree program or equivalent degree in a field of study related to the job.

Preferred Education:
No additional education

Minimum Years of Work Experience:
3

Preferred Years of Work Experience:
No additional years of experience

Minimum Leadership Experience:
None

Preferred Leadership Experience:
None

Certifications:
None

Competencies:
Strong decision making and problem solving skills
Proficiency in Microsoft Excel and Access
Ability to lead and manage cross functionally
Strong organizational, analytical and customer service skills
Positive, upbeat, can-do, professional and responsible attitude, independent and self-directed yet also team oriented
Influential; practiced in negotiating with others in ways that result in win-win outcomes.
To apply to this job, click Apply Now
Show Less
Report",3.8,10000+ Employees,1978,Company - Public,Home Furniture & Housewares Stores,Retail & Wholesale,$10+ billion (USD)
"Software Engineer, Data Platform",$115K - $175K (Glassdoor est.),Notion4.8 ★,"San Francisco, CA","About Us:
We're on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us can't build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft.
We've been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, we're growing fast and excited for new teammates to join us who are the best at what they do. We're passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide.
About The Role:
You'll join a team of talented engineers who will design and own foundational data products that are key to the company's business and product. Notion's data platform and infrastructure are vital to both empowering every team at Notion to make decisions using data, and are increasingly used in our product features, like search, user notifications, workspace analytics and Notion AI.
What You'll Achieve:
You'll work cross-functionally with partners from the Data Science, Data Engineering, AI, Product, Go-to-Market, Legal and Finance organizations to deliver short- and long-term impact
You'll help in executing the roadmap for data infrastructure and systems to power high volume product features using Notion's data.
You'll play a pivotal role in the development of tools and infrastructure that democratize data access and enable analytics capabilities across the organization
You'll determine the best ways to handle Notion's unique data model and usage patterns to derive insights and bring intelligence to product features like search and discovery.
Skills You'll Need to Bring:
You have worked cross-functionally to establish the right overarching data architecture for a company's needs, to build data ingestion (real-time & batch), and to provide guidance on best data practices for the business.
You have worked on data or infrastructure-focused engineering teams, particularly ones that own a wide swath of software platforms (hosted or built in-house).
You've experienced the challenges of scaling and re-architecting data platforms and infrastructure through orders of magnitude of growth and scaling data volume.
You have a deep background with big data compute, storage, and best practices that you can ask the right questions of your team, balance technology and people concerns, and make hard tradeoffs.
Nice to Haves:
You've built out data infrastructure from, or nearly from, scratch at a fast-growing startup.
You've led or managed a Data Engineering / Platform / Infrastructure Team.
You have experience building MLOps and ML serving infrastructure.
Our customers come from all walks of life and so do we. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. If you share our values and our enthusiasm for small businesses, you will find a home at Notion.
Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation made due to a disability, please let your recruiter know.
#LI-Onsite
Show Less
Report",4.8,201 to 500 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
Data Engineer,$130K - $170K (Employer est.),ASSURANCE3.2 ★,"Seattle, WA","About Assurance
Assurance IQ is a technology company headquartered in Seattle. We were acquired by Prudential (NYSE: PRU) to further the joint mission of improving financial wellness across the world.

Our team of world class software engineers, data scientists, and business professionals work every day to expand our product offerings and the reach of our platform. We simplify the complex world of insurance and financial services into straightforward, valuable solutions to improve people's lives. We start by asking customers a few questions, so our system can learn about their needs; from there, our ground-breaking, proprietary platform takes over and analyzes the thousands of data points that make customers unique. This is how we create custom-tailored plans for each customer; plans built precisely for their needs and budget. Our platform serves as the intersection between customer and seller, technology, and the human touch.

At Assurance, we are innovative, persevering, collaborative, calculated, and authentic, and we're working together to improve the lives of millions!

About the Position
As we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.
To be successful in this role, you must possess the following:
Experience with Python and SQL
Experience in data modelling
Business Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.
Comfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct
Comfort with learning new technologies to help the team explore new solutions to existing problems
Excellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.
Enthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.
Bachelors degree in mathematics, statistics, data science or related field of study.
The following additional experience is desired:
You have a proven ability to drive business results by building the right infrastructure that enables data-based insights.
You are comfortable working with a wide range of stakeholders and functional teams.
The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes.
We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail.
You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity.
You can work independently, with little oversight or guidance.

Note: Assurance is required by multiple state and city laws to include the salary range on position postings when hiring in those specific locals. The salary range for this position will be between $130,000-$170,000 and may be eligible for additional bonus or commission plans + benefits. Eligibility to participate in the bonus or commission plans is subject to the rules governing those programs, whereby an award, if any, depends on various factors including, without limitation, individual and/or organizational performance. In addition, employees are eligible for standard benefits package including paid time off, medical, dental and retirement.
Apply Now: click Apply Now
Show Less
Report",3.2,1001 to 5000 Employees,2016,Company - Public,Insurance Agencies & Brokerages,Insurance,$100 to $500 million (USD)
Senior Data Engineer,$150K - $190K (Employer est.),Curology2.6 ★,Remote,"As a team, Data Engineering enables teams throughout the company to make decisions with data they feel confident in. Data underpins virtually everything we do at Curology—from a truly individualized patient experience, to efficient business operations, to cutting edge Marketing workflows. Data Engineering is the foundation a data driven company is built on. For this role, we believe in the following:

Data as a product. We believe the true potential of Foundation teams lies in a product-oriented mindset and that this is even more relevant to data.

Exceptional impact. Data Engineers are force-multipliers that enable others to work better and faster. Data is deeply integrated into what we do, this role and team are key to our continued success.

Modern data stack. We use the best tools for the job and you will be part of growing and cultivating our modern data stack. AWS, Snowflake, our S3 Data Lake, we build for the future.

A talented and passionate team. Our small team of data engineers has achieved outsize results by maintaining a high bar for ownership and product quality.
In this role, you will:
Design, manage and optimize the flow of data throughout the organization.
Utilize a modern stack to build a cloud-first product.
Work closely with application and site reliability engineers to ensure data quality, integrity and availability.
Work with the team to integrate consistent and high coding standards.
Automate manual processes by working closely with teams like Marketing, BizOps, and Product to discover opportunities for programmatic efficiency.
Work on initiatives to keep our system elegant and productivity high — such as improving our metrics, analytics, and experimentation infrastructure.
Keep Privacy and Data Protection (PDP) the first-order consideration of data.
Respond quickly to data strike team requests.
You will be successful if you have:
At least 5 years experience building software with Python.
Write idiomatic python.
Write object oriented and reusable code.
Familiar with industry standard coding and documentation best practices.
At least 6 years experience modeling SQL and noSQL data.
Understand the tradeoffs between different database models.
Given an analysis problem, suggest data models and key reasons for choices.
Know major data warehousing tools and concepts.
Used Airflow or other job schedulers to develop and monitor batch data pipelines.
Strong expertise in data architecture for efficiency of storage and retrieval of data, especially in modern usage-based systems.
Experience with AWS data services.
Worked with modern database systems, especially Snowflake.
Worked with permissions and regulated or controlled data (HIPAA/GDPR/CCPA/FDA).
Experience with the following technologies: Python, MySQL, Snowflake, Airflow, Terraform, AWS
Passionate about getting the right data to the right person.
High emotional intelligence and a kind demeanor.
Willing to lead in areas of strength and learn new skills when needed.
Has earned the trust and respect of other members of the Data Engineering Team.
Engages in significant thought leadership within the Data Team.
Reaches out to other departments and areas of the company to build relationships.
Anticipates problems before they arise and devises solutions to those problems.
Takes a leading role in documenting post-mortems after problems arise.
Is able to fully architect, build and deploy a solution with no supervision.
Handles Strike Team tickets quickly and efficiently.
Is an expert in all of the technologies that Data Engineering works with on a weekly basis.
Understands advanced Snowflake capabilities, as well as when and how to use them.

Additional Optional Skills
Worked with Apache Spark or other Big Data tools.
Built systems at scale with AWS Lambda or a similar serverless technology.
Worked closely with Marketing teams and having a deep understanding of paid channel automation.
You will love working at Curology because:
Competitive salary and equity packages
Comprehensive benefits: medical, dental, and vision insurance for employees; flexible spending account; 401k; mental health & wellness programs
$75 WFH stipend (remote employees)
Home office setup stipend (remote employees)
Minimum Time Off policy (unlimited PTO, with at least 3 weeks off) for exempt employees
11 company observed holidays
Additional holidays: Curology days off (1 per quarter), 1 annual floating holiday (employee’s choice), and Gratitude Week (employees take the full week of Thanksgiving off; business critical teams observe different days)
Paid parental leave
pre-tax commuter benefits
Employee donation matching program
Company-sponsored events
Free subscription to Curology or Agency (for you and another VIP of your choice!)

The base salary for this position will be between $150,000 to $190,000 depending on your experience, skillset, and geographic location.
#LI-EH1
#LI-Remote
To apply to this job, click Apply Now
Show Less
Report",2.6,201 to 500 Employees,2014,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
Sr. Azure Data Engineer,$75.00 - $80.00 Per Hour (Employer est.),Kairos Technologies4.6 ★,"Dallas, TX","Hello,
Hope you are doing great!
If you are comfortable with the requirement, kindly respond with below information and your updated resume( word formatted ) ASAP
Full Name:
Current Location:
Contact No Primary:
Work Authorization:
S.S.N No( Last 4 digits ):
D.O.B( MM/DD ):
Availability( Any notice period required to join in project ):
Currently working(Yes/No):
LinkedIn:
Employer details( If on C2C ):
Please check the below requirement
Direct Client Requirement
Position: Sr. Data Engineer with Azure Cloud and Strong SQL( Hybrid/ Onsite for 2 days a week )
Location: Dallas TX or VA
Duration: 12+ Months Long Term Contract
Work Authorization: US Citizen/GC/GC-EAD/TN Visa( on C2C is fine )
Need Locals Candidates..
Job Summary:
We are seeking an experienced and highly skilled Senior Database Engineer with strong hands-on experience in modernizing and automating on-premises SQL databases, as well as a solid track record of successful migration projects from on-premises to Azure cloud. As a Senior Database Engineer, you will play a critical role in driving our database modernization efforts, ensuring scalability, performance, and security in the Azure cloud environment.
Responsibilities:
Lead the modernization and automation of on-premises SQL databases, implementing best practices and efficient processes to enhance performance, scalability, and reliability.
Design and execute successful migration strategies from on-premises databases to Azure cloud, ensuring seamless data transfer and minimal downtime.
Develop and implement automation solutions using tools such as Python, PowerShell, and Azure DevOps (ADO) to streamline database management tasks, including provisioning, backup and recovery, monitoring, and deployment processes.
Collaborate closely with cross-functional teams to gather requirements, understand business needs, and propose effective database solutions that align with the overall technical architecture in the Azure cloud environment.
Utilize tools like Liquibase or similar to manage database schema changes and version control.
Implement effective monitoring and logging solutions using tools like Splunk and Datadog to ensure database performance, availability, and security.
Perform thorough assessments and evaluations of existing on-premises databases, identifying areas for improvement, optimization, and consolidation prior to migration to Azure cloud.
Ensure data integrity, security, and compliance with industry standards and regulations throughout the database modernization and migration process in the Azure cloud environment.
Provide guidance and mentorship to junior database engineers, fostering their technical growth and promoting best practices in database management, automation, and migration in the Azure cloud.
Stay up to date with the latest trends, tools, and technologies in database management, Azure cloud services, and DevOps practices, evaluating their potential impact and relevance to our organization.
Collaborate with vendors and third-party providers to assess and implement new technologies, tools, and services that can enhance our Azure cloud database environment and support our modernization goals.
Qualifications:
Bachelor's degree in computer science, information technology, or a related field.
Minimum of 10 years of hands-on experience in database engineering, with a strong focus on SQL databases and Azure cloud.
Extensive experience in modernizing and automating on-premises SQL databases, including performance optimization, scalability improvements, and process automation.
Proven expertise in successfully migrating on-premises databases to Azure cloud, ensuring data integrity and minimal disruption to operations.
Strong proficiency in SQL scripting, database performance tuning, backup and recovery, and database security practices in the Azure cloud environment.
Experience with database migration tools and technologies, such as Azure Database Migration Service, Azure Data Factory, or similar tools.
Solid understanding of Azure cloud database services, including Azure SQL Database, Azure SQL Managed Instance, and Azure Cosmos DB.
Familiarity with automation and configuration management tools like Python, PowerShell, and Azure DevOps (ADO) for streamlining database management and deployment processes in Azure.
Experience in using tools like Liquibase or similar for managing database schema changes and version control.
Knowledge of monitoring and logging solutions like Splunk and Datadog for database performance and security monitoring in Azure cloud.
Strong analytical and problem-solving skills, with the ability to troubleshoot complex database issues and propose effective solutions in the Azure cloud environment.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.
Tech Stack:
Databases: On-premises SQL databases (e.g., Oracle, MySQL, PostgreSQL)
Cloud Platform: Microsoft Azure (Azure SQL Database, Azure SQL Managed Instance, Azure Cosmos DB)
Scripting and Automation: Python, PowerShell
Version Control: Git
CI/CD: Azure DevOps (ADO)
Database Migration: Azure Database Migration Service, Azure Data Factory
Database Schema Management: Liquibase or similar tools
Monitoring and Logging: Splunk, Datadog
Thanks and Regards
Prasad Mamidela | Kairos Technologies Inc.
Direct Number: 972.777.9484 | Mobile: 201.613.3664
433 E Las Colinas Blvd, # 1240, Irving, TX 75039 USA
http://www.kairostech.com
LinkedIn: https://www.linkedin.com/in/prasadmamidela
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 4 years (Required)
SQL: 6 years (Required)
On-premises: 2 years (Required)
Total IT: 10 years (Required)
Azure DevOps: 2 years (Required)
Work Location: In person
Show Less
Report",4.6,51 to 200 Employees,2003,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
"Software Engineer, Data Products",$119K - $163K (Glassdoor est.),LaunchDarkly4.1 ★,"Oakland, CA","Please note, before progressing to our application, this position is based in the San Francisco Bay Area and not suitable for remote candidates.
About the Job:
We are looking for exceptional Software Engineers to make a profound impact on how data products will be integrated into companies' software in the future. We are integrating data into everything LaunchDarkly offers on top of our unrivaled feature management platform.
As a Data Products - Software Engineer, you will help us architect and write fast, reliable, and scalable data processing tools to process data from our thousands of customers and their hundreds of millions of users around the world. We're looking for someone who knows what it takes to deliver value to customers and takes pride in the quality of their work.
The primary technologies we use daily include Golang, Scala, Kinesis, and Flink. If working as a part of such a poly-functional team to bring to change how experimentation is done forever appeals to you then come join the Experimentation team at LaunchDarkly.
Responsibilities:
Build and expand our data platform and services
Help us identify the best technologies for our evolving data needs
Collaborate with product team to spec and deliver user-facing features
Monitor and improve data pipeline performance
Actively participate in code reviews
Improve engineering standards, tooling, and processes
Qualifications:
Proven experience and fluency in a JVM or functional language
Experience building data platforms (e.g. using Flink, Kafka, DataFlow, Hadoop, Spark)
Strong communication skills, a positive attitude, and empathy
You write code that can be easily understood by others, with an eye towards maintainability
You hold yourself and others to a high bar when working with production systems
You value high code quality, automated testing, and other engineering best practices
Pay:
Target pay range for a Level P3 in San Francisco/Bay Area: $144,000 - $169,000*
Restricted Stock Units (RSUs), health, vision, and dental insurance, and mental health benefits in addition to salary.
LaunchDarkly operates from a place of high trust and transparency; we are happy to state the pay range for our open roles to best align with your needs. Exact compensation may vary based on skills, experience, degree level, and location.
About LaunchDarkly:
LaunchDarkly is a Feature Management Platform that serves trillions of feature flags daily to help software teams build better software, faster. Feature flagging is an industry standard methodology of wrapping a new or risky section of code or infrastructure change with a flag. Each flag can easily be turned off independent of code deployment (aka ""dark launching""). LaunchDarkly has SDKs for all major web and mobile platforms. We are building a diverse team so that we can offer robust products and services. Our team culture is dynamic, friendly, and supportive. Our headquarters are in Oakland.
At LaunchDarkly, we believe in the power of teams. We're building a team that is humble, open, collaborative, respectful and kind. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status.
One of our company values is 'Widen the Circle'. Which means we seek out diversity of perspectives to get better results. We understand everyone has their own unique talents and experiences. We encourage you to apply to this role even if you don't think you meet 100% of the qualifications outlined above. We can find out together if it's the right match for your skillset.
We've partnered with KeyValues to help demonstrate the amazing culture we've built here at LaunchDarkly, find more info at https://www.keyvalues.com/launchdarkly.
LaunchDarkly is also committed to giving back to our community and is a part of Pledge 1%, an organization that helps companies make this a priority. Through this initiative and its charitable arm, the LaunchDarkly Foundation, the company is committed to such causes as supporting education for the underserved, homelessness relief and moving towards having a net-zero carbon footprint. You can find more about the LaunchDarkly Foundation and the organizations we serve at https://launchdarkly.com/foundation/.
Do you need a disability accommodation?
Fill out this accommodations request form and someone from our People Operations team will contact you for assistance.
Show Less
Report",4.1,501 to 1000 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD)
Data Platform Engineer (REMOTE),$82K - $185K (Employer est.),GEICO3.0 ★,"Chevy Chase, MD","GEICO is leading the way in adopting the latest Data Engineering concepts. We have a culture of experimentation, lean/agile delivery, and adoption of modern technologies, complemented with strong partnership and collaboration with architecture and product teams.
The Data Movement team is seeking a highly motivated Software Engineer with Data Engineering background to start or continue an IT career in GEICO Data Movement team. Teaming up with architects, product owners, software engineers, and customers, you will work in an Agile environment. You will be part of the software engineering team building the data platform, enabling the use of data across GEICO. You will be establishing standards and implementing guard rails to ensure they are followed. You will be working in an environment that will expose you to cloud architecture and open-source software in data ingestion, data integration and transformations. You should be intellectually curious, have a solutions-oriented attitude and enjoy learning new tools and techniques.

Responsibilities:
Focus on key data capability areas such as data ingestion, transformations and provide technical and thought leadership to the data team.
With a product mindset, own complete solution across its entire life cycle and be accountable for the quality, usability, supportability, and performance of the solutions.
Design, test and implement self service capabilities for data producers to create data for analytics and consumers to use the data effectively to solve business problems
Collaborate with product managers, data consumers, data governance, and other engineering teams to pilot innovative solutions
Lead design sessions and code reviews to elevate the quality of engineering across the organization.
Utilize programming languages and frameworks such as Java, Scala, Python, Spark, SQL, dbt, .NET, Container Orchestration services including Docker and Kubernetes, and a variety of Azure tools and services.
Consistently share best practices and improve processes within and across teams.
Mentor junior team members and provide feedback to peers to help them grow.

Experience & Skills:
5+ years of experience in data software development building data platforms for batch or streaming ingestion, processing the data for analytics, and storing the data in various stages
3+ years of experience in system design and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and experience with cloud data architecture in Azure, AWS or GCP
1+ years of experience in open-source technologies and frameworks such as Spark, Kafka, Delta Lake, Iceberg
Experience in software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Java, Scala, dbt and/or other frameworks,
Experience with building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data.
Experience with designing and implementing micro-services architecture, deploying applications using cloud native services or Kubernetes with resiliency and availability baked into the solution
Bachelor's degree in computer science, Information Systems, or equivalent education or work experience
Benefits:
At GEICO, we make sure you have the support and resources to leverage and develop your skills, secure your financial future, and take care of your health and well-being. GEICO continually seeks to provide a workplace where everyone can be their authentic self. To help achieve this goal, we support associate-led Employee Resource Groups that foster a true sense of community. Through GEICO’s competitive benefits offerings and various training and development opportunities, we have you covered with our
Total Rewards Program
that includes:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Reimbursement
Paid Training and Licensures
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
#LI-SS3
Annual Salary
$82,000.00 - $185,000.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
Start your job application: click Apply Now
Show Less
Report",3.0,10000+ Employees,1936,Subsidiary or Business Segment,Insurance Carriers,Insurance,$10+ billion (USD)
Data Engineer - Remote,$95K - $115K (Employer est.),Aramark Uniform & Career Apparel LLC3.3 ★,"Burbank, CA","The Data Engineer will work within Aramark Uniform Services IT department and is responsible for supporting the data integration needs of Enterprise Data Warehouse, Data Lakes, and other integration solutions. The role of the Data Engineer is responsible for building and maintaining optimized and highly available data and analytics layer that facilitates deeper analysis and reporting to business consumers. Data Engineer will design robust and scalable data integration solutions based on industry best practices. The candidate must have a strong hands-on experience working in big data, and data warehouse environment.

Responsibilities/Essential Functions:
-Design, implement, and continuously improve data analytics platform
-Implement optimized and simplify data query and analysis capabilities of the data platform
-Develop and improve the current architecture, emphasizing data security, data quality and timeliness, scalability, and extensibility
-Deploy and apply big data solution and run pilots to design highly performing and low latency data architectures to scale
-Collaborate with cross functional team and develop, implement, and validate KPIs, statistical analyses, data profiling, prediction, forecasting, clustering, and machine learning algorithms
-Leverage best practices, disciplined approaches, and standards to solving technical problems.
-Perform ongoing monitoring, optimization, and refinement of reports and BI solutions
-The role entails 100% hands-on development using MS SQL / Python on MS Azure Synapse Platform
-Build data solutions with efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
-Develop big data analytics solutions sourcing data from distributed systems and applications
-Ensuring accurate and efficient governance policy development and adherence
-Report on statuses when requested
-Submit all time and expense reporting procedures accurately and timely
-Maintain good standing and completion on all compliance related matters (i.e., assigned mandatory trainings, actions required from audits, corporate policies, etc.)
-Perform all additional duties and responsibilities based on the direction and guidance of supervisor

Knowledge/Skills/Abilities:
-Expertise in ELT optimization, designing, coding, and tuning big data processes using Microsoft Azure Synapse or similar technologies.
-Experience with building data pipelines and analytics solution to stream and process datasets at low latencies.
-Knowledge of Data Engineering and Data Operational Excellence using standard methodologies.
-Hands on level skills coding and optimizing complex SQL and Python
-Strong Knowledge of data warehousing frameworks and methodologies.
-Proven ability to foresee opportunities to innovate and leads the way
-Excellent verbal and written communication
-Proven interpersonal skills and ability to convey key insights from complex analyses in summarized business terms
-Ability to effectively communicate with technical and non-technical teams
-Ability to work with shifting deadlines in a fast-paced environment Skilled and proficient in MS Office O365 suite (i.e. Word, PowerPoint, Excel, SharePoint, Teams, Communications Tools, etc.)
-Ability to operate with a customer-centric service approach

Working Environment/Safety Requirements:
-Ensure necessary working environment and capabilities to effectively carry out responsibilities if working from a non-AUS location (remote work)
-Ability and willingness to handle work related issues during all hours of the day, every day of the week, understanding the responsibility of our organization’s requirement for 24/7 production support
-Ability, willingness, and flexibility to travel as needed for approved work purposes in accordance with project and management schedules

Experience/Qualifications:
-Bachelor’s degree in Computer Science, Mathematics, Statistics or related field or 6+ years relevant experience
-Experience in data mining, profiling, and analysis
-Experience in Azure Data Factory
-Experience with complex data modelling, ELT design, and using large databases in a business environment
-5+ years experience with languages like SQL, Python, Java, or similar language
-Must have at least three years of work experience in Big Data projects such as Microsoft Azure Synapse or similar Big Data platform a must

License Requirements/Certifications:
-Valid U.S. driver license (for rental cars when applicable)
-Be legally able to work in the United States: U.S. Citizen or Legal Resident
Benefits: Aramark offers a wide array of comprehensive benefit programs and services including medical, dental, vision, short and long-term disability, basic life insurance, and paid parental leave. Employees are able to enroll in the company’s 401k plan. Employees are eligible for 120 hours of vacation, 16 hours of floating holidays, and paid sick time every year. Employees will also receive 9 paid holidays throughout the calendar year.
Compensation: The salary rate for this position ranges from $95K-115K, depending on circumstances including an applicant’s skills and qualifications, certain degrees and certifications, prior job experience, market data, and other relevant factors.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)
Apply Now: click Easy Apply
Show Less
Report",3.3,10000+ Employees,1959,Company - Public,Catering & Food Service Contractors,Restaurants & Food Service,$10+ billion (USD)
Senior Data Engineer,$153K - $210K (Employer est.),Snackpass3.3 ★,"San Francisco, CA","Who we are ✨
Snackpass's mission is to unify the physical and digital world for local commerce.
We power mobile order pickup and social commerce for restaurants, modernizing the customer experience while making restaurant operators successful.
Opportunity ✨
Snackpass is one of the fastest growing marketplaces (a16z top marketplaces), and a top 100 YC company. We are backed by Andreeson Horowitz, Y Combinator, General Catalyst, First Round Capital, Craft Ventures and many others. We are hiring people who are humble and hungry to join us in any of our hubs (NYC, SF, LA) or remotely.
Our vision is to be the dominant platform for pickup, a $750B market globally.
About the Role
We are seeking a talented and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in building and maintaining data pipelines, developing and managing dashboards, and contributing to tool development. Your expertise will enable us to efficiently process and analyze large volumes of data, providing valuable insights to drive business decisions.
What we're looking for
4+ years of proven experience as a Data Engineer, with a focus on building robust data pipelines and maintaining data infrastructure.
Strong programming skills in languages like Python, Scala or Javascript, with a solid understanding of software engineering principles.
Hands-on Experience building or maintaining data pipelines with a modern orchestration tools like Airflorw/Prefect/Dagster
Strong expertise with SQL and non-SQL DBs, cloud-based data platforms (e.g., AWS, GCP) and their related services (S3, BigQuery, etc.).
Expert level writing of SQL for data manipulation, transformation and for analytics.
Experience building and maintaining interactive and intuitive dashboards using tools like Looker.
Comfortable with tool development, including building and enhancing internal data tools and frameworks.
Excellent problem-solving and analytical skills, with a strong attention to detail.
Effective communication skills, with the ability to collaborate with cross-functional teams and present complex ideas to both technical and non-technical stakeholders.
What you'll be working on
Build scalable and efficient data pipelines to collect, process, and transform large volumes of data from diverse sources.
Develop and maintain data models, ensuring data integrity and optimizing query performance.
Collaborate with stakeholders to understand their data requirements and provide the necessary infrastructure and support.
Build and maintain interactive dashboards and visualizations to enable data-driven decision-making across the organization.
Contribute to the development of internal data tools and frameworks, automating repetitive tasks and improving data accessibility and usability.
Stay up to date with the latest advancements in data engineering technologies and best practices, and proactively recommend improvements to our data infrastructure.
Mentor and provide guidance to junior data engineers, fostering a culture of learning and growth.
Cash Compensation for this role: $153,000 - $210,000
Please note: Final offer amounts are determined by multiple factors, including prior experience, expertise, and leveling. The final offer amount may vary from the amount above. Please note that this range does not represent additional compensation benefits (such as equity, 401K or medical, dental, or vision insurance).

What You Will Get From Us:
You will receive competitive compensation, a generous equity grant in a high-growth start-up, and benefits like healthcare, medical & dental coverage, unlimited PTO, a home office budget, wellness budget and more.
Importantly, you will also receive an unparalleled amount of ownership over the work you do here. We are a small team, so the opportunity to make a large impact, work on a broad spectrum of challenges and grow your personal skill-set awaits you here.
Finally, you will get a diverse and inclusive work environment where you will be surrounded by hungry and humble colleagues.
Snackpass is an equal opportunity employer and we value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. In fact, we are confident that the most inclusive and diverse teams accomplish the most extraordinary results.
Show Less
Report",3.3,1 to 50 Employees,-1,Unknown,-1,-1,$5 to $25 million (USD)
Azure Data Engineer - All Levels (REMOTE),$72K - $237K (Employer est.),GEICO3.0 ★,"Chevy Chase, MD","GEICO’s AI & Machine Learning Engineering team is seeking Azure Data Engineers to support Machine Learning and Data Science development, deployment, and maintenance efforts. As part of our team, you will be responsible for designing, implementing, automating, and monitoring a variety of data ingest pipelines using a variety of tools such as Azure Data Factory (ADF). The role requires proficiency with or ability to quickly learn about a variety of technologies related to software design and development, as well as Azure services. Experience with Machine Learning techniques is nice to have. You will also participate in the planning and implementation of new projects, including work on both Machine Learning deliverables and the deployment frameworks that launch these deliverables. In this role, the ability to build and maintain mission critical production software in the Azure cloud is a must. As a Senior/Principal Azure Data Engineer, you would also contribute to areas such as Model Deployment Frameworks, Automated Machine Learning, Data Visualization and Machine Learning Interpretability, as well as Data Engineering.


Required Qualifications:
4+ years of development experience
Hands-on experience with cloud platforms (MS Azure, AWS or Google Cloud Services)
Hands-on experience with distributed computing (preferably using Apache Spark or Databricks)
Proficiency with at least one or more programming language such as Scala, Java, C# or Python
Knowledge of software design patterns
Familiarity with source control tools and concepts (e.g. Git)
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience

Desired Qualifications:
Microsoft Certified: Azure Data Engineer or
Microsoft Certified: Azure Developer Associate
Experience with Apache Spark
Experience with Azure Data Factory (ADF)
Experience with Machine Learning algorithms
Experience with Cloud-based deployments
Experience with Microservices Architecture
Familiarity with Azure DevOps (ADO) and Shell scripting
Familiarity with unit testing and integration testing (e.g. JUnit and JMeter)
Benefits:
At GEICO, we make sure you have the support and resources to leverage and develop your skills, secure your financial future, and take care of your health and well-being. GEICO continually seeks to provide a workplace where everyone can be their authentic self. To help achieve this goal, we support associate-led Employee Resource Groups that foster a true sense of community. Through GEICO’s competitive benefits offerings and various training and development opportunities, we have you covered with our
Total Rewards Program
that includes:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Assistance including Direct Billing and Reimbursement payment plan options
Paid Training, Licensures, and Certificates
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins with the pay period after hire date. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
#LI-AP1
Annual Salary
$72,000.00 - $236,500.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
To apply to this job, click Apply Now
Show Less
Report",3.0,10000+ Employees,1936,Subsidiary or Business Segment,Insurance Carriers,Insurance,$10+ billion (USD)
Sr Data Engineer (10+ Years experience),$70.00 - $80.00 Per Hour (Employer est.),Violet ink4.0 ★,"Dallas, TX","Role - Sr. Data Engineer
Location: Dallas, TX (Hybrid)
C2H
SQL, Spark, Analytics, Hadoop, Scala, data pipelines, tableau/PBI, Airflow, Kafka,
Job Description:
Designs, develops, and implements Hadoop eco-system based applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation.
Required Qualifications –
Option 1: Bachelor’s degree in Computer Science and 4 years' experience in software engineering or related field.
Option 2: 6 years’ experience in software engineering or related field.
Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field. 3 years' experience in data engineering, database engineering, business intelligence, or business analytics.
Nice to have soft skills –
7+ years of experience with 3+ years of Big data development experience
Experience in HDFS, Hive, Hive UDF’s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix.
Demonstrates expertise in writing complex, highly optimized queries across large data sets
Retail experience and knowledge of commercial data is a huge plus
Experience with BI Tool Tableau or Looker is a plus
Main Skills:
strong experience in Scala, Spark, SQL, Python , Tableau , Power BI and more.
Design, develop and build database to power Big Data analytical systems.
Design pipelines from a wide variety of data sources using Spark, SQL, HQL and other
technologies.
Build robust and scalable applications using SQL, Scala/Python and Spark.
Create real time data streaming and processing using Kafka and/or Spark streaming.
Build dashboards using Tableau, Power BI and other reporting tools.
Hybrid role - will need to go into the office 1 - 2 days a week.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75204: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Dallas, TX 75204
Show Less
Report",4.0,1 to 50 Employees,2007,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Sr. Data Engineer,$95K - $115K (Employer est.),Newbold Advisors4.1 ★,"Omaha, NE","Permanent job opportunity for a Sr. Data Engineer to lead data projects and mentor technical team members.
Job Qualifications:
Bachelor’s degree (Computer Science preferred)
5 years of experience with data warehousing, relational database management systems, and multi-dimensional database management systems
Advanced level proficiency in SQL
Strong programming skills using Python
Strong programming skills using PySpark
Experience working with Data Stage, Informatica or other ETL tools
Demonstrated knowledge of data warehouse design and data warehousing data population techniques for target structures (Star Schemas, Snowflake Schemas)
Strong relational database knowledge, with a broad understanding of current and prospective data architecture and database performance tuning
Experience developing, testing and maintaining scripts / jobs that are required to extract, transform, clean and move data and meta data to be loaded into a data warehouse or data mart
Preferred:
Experience working within DB2 environment
Experience mentoring less experienced designers /developers
Job Type: Full-time
Pay: $95,000.00 - $115,000.00 per year
Benefits:
Dental insurance
Health insurance
Vision insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Omaha, NE 68131: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 5 years (Required)
Work Location: In person
Show Less
Report",4.1,501 to 1000 Employees,-1,Contract,Accounting & Tax,Financial Services,Unknown / Non-Applicable
Data Engineer,$120K - $140K (Employer est.),Zelis3.8 ★,"Saint Petersburg, FL","Summary:
· Design, develop and implement scalable batch/real time data pipelines (ETLs) to integrate data from a variety of sources into Data Warehouse and Data Lake
· Design and implement data model changes that align with warehouse dimensional modeling standards.
· Proficient in Data Lake, Data Warehouse Concepts and Dimensional Data Model.
· Responsible for maintenance and support of all database environments, design and develop data pipelines, workflow, ETL solutions on both on-prem and cloud-based environments.
· Design and develop SQL stored procedures, functions, views, and triggers
· Design, code, test, document and troubleshoot deliverables
· Collaborate with others to test and resolve issues with deliverables
· Maintain awareness of and ensure adherence to Zelis standards regarding privacy.
· Create and maintain Design documents, Source to Target mappings, unit test cases, data seeding.
· Ability to perform Data Analysis and Data Quality tests and create audit for the ETLs.
· Perform Continuous Integration and deployment using Azure DevOps and Git
· Experience with Banking and Finance Processes specifically NACHA, BAI2 and Bank 822 Files required.
· This position will support the Payments Advanced Record Keeping Project
Requirements:
· Minimum of 5+ years experience in the following:
o 5+ years data engineering experience to include data analysis
o 2+ years working with an ETL tool (DBT preferred)
o 5+ years programming SQL objects (procedures, triggers, views, functions) in SQL Server. Plus experience optimizing SQL queries a plus
o 2+ years designing and developing Azure/AWS Data Factory Pipelines.
o 2+ years Columnar MPP Cloud data warehouse using Snowflake
· Advanced understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
· Experience designing and implementing Data Warehouse.
· Working Knowledge of Azure/AWS Architecture, Data Lake
Preferred Skills:
· Microsoft BI stack (SSIS/SSRS/SSAS)
· Working knowledge managing data in the Data Lake.
· Business analysis experience to analyze data to write code and drive solutions
· Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence.
· Healthcare and/or Payments experience
Job Type: Full-time
Pay: $120,000.00 - $140,000.00 per year
Schedule:
Monday to Friday
Application Question(s):
Do you have experience designing and developing with Data Factory Pipelines (Azure/AWS)?
How many years of work experience do you have with Snowflake Cloud?
Do you have experience analyzing data to write code and drive solutions?
Experience:
ETL: 2 years (Required)
SQL: 4 years (Required)
Work Location: In person
Show Less
Report",3.8,1001 to 5000 Employees,2016,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
Azure Data Engineer,$84K - $191K (Employer est.),PSRTEK4.5 ★,"Berwick, ME","Job Title : Azure Data Engineer
Job Type: Contract
Job Description
Atleast 2 years of experience working at onsite closely with client.
Strong exp with Azure Cloud Technologies, including ADF, Databricks, SQL DB, ADLS
Experience with data transformation and manipulation using Azure Databricks
Working knowledge of Azure DevOps CI/CD tools and concepts Azure pipelines, GitHub
Scripting experience with Python and Rest API
Experience in implementing CDC (Change Data Capture) Data Virtualization
Experience in creating base view, Derived View and Data source connections using Denodo
Experience in Denodo caching in snowflake
Experience in creating data models by extracting from various data sources by connecting through JDBC, ODBC and Web API in Denodo Skills Required: Python ADF ADB SQL Denodo Snowflake
Job Type: Contract
Salary: $84,181.34 - $190,840.44 per year
Ability to commute/relocate:
Berwick, ME 03901: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 5 years (Preferred)
SQL: 6 years (Preferred)
Data warehouse: 5 years (Preferred)
Work Location: In person
Speak with the employer
+91 609-934-3291
Show Less
Report",4.5,Unknown,-1,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Sr. Data Engineer,$84K - $191K (Employer est.),GG tech global,Remote,-1,4.5,-1,-1,-1,-1,-1,-1
"Lead Engineer, Data Platform",$199K - $200K (Employer est.),Sephora3.6 ★,"San Francisco, CA","Job ID: 226189
Location Name: FSC REMOTE SF/NY/DC -173(USA_0173)
Address: FSC, Remote, CA 94105, United States (US)
Job Type: Full Time
Position Type: Regular
Job Function: Information Technology
Remote Eligible:

Company Overview:
At Sephora we inspire our customers, empower our teams, and help them become the best versions of themselves. We create an environment where people are valued, and differences are celebrated. Every day, our teams across the world bring to life our purpose: to expand the way the world sees beauty by empowering the Extra Ordinary in each of us. We are united by a common goal - to reimagine the future of beauty.

The Opportunity:
Your role at Sephora:
As a Lead Engineer, Data Platforms at Sephora, you will: Streamline the intake of the raw data into the Azure Data Lake. Perform production support and deployment activities. Proactively drive the execution of core data engineering, business intelligence, and data warehouse frameworks. Build data pipelines from systems including CRM and Ecommerce, with the emphasis on scalability and reliability. Leverage central data warehouse with other data sources to create enriched customer information in the CRM system. Analyze and translate business needs into data models to support long-term, scalable, and reliable solutions. Create logical and physical data models using best practices to ensure high data quality and reduced redundancy. Drive data quality across the organization. Develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking. Define and manage SLA’s for data sets and processes running in production. Continuously improve our data infrastructure and stay ahead of technology. Design a system for data backup in case of system failure. Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs. (Position allows some work-from- home flexibility, with schedule to be approved by manager. Must be able to work on site as required.)

We are excited about you if you have:
Bachelor’s or foreign equivalent degree in Computer Science, Engineering, or Information Technology.
Six (6) years of progressively responsible post-baccalaureate experience in the design of data warehouse architecture.

Experience must include:
Scala
Spark
Python
Business intelligence and data warehouse frameworks
Data engineering
Data modeling and pipelines
Data warehouse production support
Data warehouse SLAs and best practices
Programming in SQL
Databricks
IBM Datastage

Salary: $199,098 to $200,000 per year depending on experience
Working at Sephora’s Field Support Center (FSC)
Our North American operations are based in the heart of San Francisco’s Financial District, but you won’t hear us call it a headquarters – it’s the Field Support Center (FSC). At the FSC, we support our stores in providing the best possible experience for every client. Dedicated teams cater to our client’s every need by creating covetable assortments, curated content, compelling storytelling, smart strategy, skillful analysis, expert training, and more. It takes a lot of curious and confident individuals, disrupting the status quo and taking chances. The pace is fast, the fun is furious, and the passion is real. We never rest on our laurels. Our motto? If it’s not broken, fix it.

As a condition of employment, Sephora requires all newly hired employees to be fully vaccinated against COVID-19 by their start date unless they have requested and received an exemption due to a qualifying medical condition, a sincerely held religious belief or practice, or a requirement by law.

Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities.

Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.

The actual base salary offered depends on a variety of factors, which may include, as applicable, the applicant’s qualifications for the position; years of relevant experience; specific and unique skills; level of education attained; certifications or other professional licenses held; other legitimate, non-discriminatory business factors specific to the position; and the geographic location in which the applicant lives and/or from which they will perform the job. Individuals employed in this position may also be eligible to earn bonuses. Sephora offers a generous benefits package to full-time employees, which includes comprehensive health, dental and vision plans; a superior 401(k) plan, various paid time off programs; employee discount/perks; life insurance; disability insurance; flexible spending accounts; and an employee referral bonus program.

While at Sephora, you’ll enjoy…

The people. You will be surrounded by some of the most talented leaders and teams – people you can be proud to work with.
The learning. We invest in training and developing our teams, and you will continue evolving and building your skills through personalized career plans.
The culture. As a leading beauty retailer within the LVMH family, our reach is broad, and our impact is global. It is in our DNA to innovate and, at Sephora, all 40,000 passionate team members across 35 markets and 3,000+ stores, are united by a common goal - to reimagine the future of beauty.

You can unleash your creativity, because we’ve got disruptive spirit. You can learn and evolve, because we empower you to be your best. You can be yourself, because you are what sets us apart. This, is the future of beauty. Reimagine your future, at Sephora.

Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities.

Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.

As a condition of employment, Sephora requires all newly hired employees to be fully vaccinated against COVID-19 by their start date unless they have requested and received an exemption due to a qualifying medical condition, a sincerely held religious belief or practice, or a requirement by law.
To apply to this job, click Apply Now
Show Less
Report",3.6,10000+ Employees,1969,Company - Private,Beauty & Personal Accessories Stores,Retail & Wholesale,$1 to $5 billion (USD)
Senior Data Engineer,$164K - $170K (Employer est.),Anywhere Real Estate3.4 ★,"Madison, NJ","Senior Data Engineer wanted by Anywhere Real Estate Operations, LLC, Madison, NJ.

Responsibilities :
Design and build high performance, scalable data solutions that meet the needs of millions of agents, brokers, home buyers, and sellers.
Work with other Data Engineers for build out of Next Generation Data Ingestion Platform.
Design and develop data ingestion pipelines for batch and real-time streaming of data from in-house OLTP systems and third-party data.
Work with team to design and develop Data Lake to store and process 10s of terabyte of data.
Design Data Lake CLI to manage Data Lake Storage and Access.
Design and develop ETL pipelines to process data in data lake for descriptive and prescriptive reporting.
Develop ETL data pipelines to build Enterprise Data Models for Property, Agent, Broker, office and other master entities.
Design and develop CI/CD process for continuous delivery in AWS Cloud.
Design, develop, and test robust, scalable data platform components.
Work with a variety of teams and individuals, including product engineers to understand their data pipeline needs and come up with innovative solutions.

Qualifications :

Must have a Bachelor’s degree in Software Engineering or related field, plus five (5) years of experience in the job offered or in any occupation that includes the required experience and skills. Experience must include:
2 years of experience utilizing Scala to build data ingestion pipelines, build data aggregation pipelines, and canonicalize and normalize data;
2 years of experience utilizing Spark to perform distributed data transformations and perform data aggregations;
2 years of experience utilizing AWS to build serverless data pipelines and applications;
2 years of experience utilizing Kafka/Messaging Systems to process real time and batch data events; and
2 years of experience utilizing Development Tools including Git and Jira to develop highly available and reliable software.

JOB LOCATION: Candidate can work from home office located anywhere within the US.

#LI-AD1
#LI-Remote


Exciting News:

EEO Statement: EOE AA M/F/Vet/Disability

Compensation Range:
$164,000 - $170,000; At Anywhere, actual compensation within that range will be dependent upon the individual’s skills, experience, and qualifications.
Show Less
Report",3.4,Unknown,-1,Company - Public,Real Estate,Real Estate,Unknown / Non-Applicable
Senior Data Engineer,$139K - $231K (Employer est.),Rec Room4.5 ★,"Seattle, WA","Rec Room is the best place to build and play games together. Chat, hang out, explore MILLIONS of rooms, or build something new to share with us all! As a Senior Analytics Engineer, you'll drive the vision for our data warehouse and analytics layer, setting us up to scale in both the number of players and internal users. You'll partner closely with teams such as Growth, Economy, User-generated Content, Social, among others, empowering them to understand player behavior deeply.
WHAT YOU'LL DO:
Partner with PMs, designers, engineers, and analysts/data scientists to build out the data layer to empower analysis, experimentation, and new product features.
Architect data warehouse modeling layer and establish the patterns and processes which will serve as the foundation for scaling our data.
Apply software engineering practices (e.g., code review, testing, version control) when creating new datasets to ensure data quality across our pipelines and BI tools
Identify gaps in our data collection processes and collaborate with product and engineering to iterate and improve current methodologies.
Design and implement top-level company dashboards to serve as the source of truth for company performance.
Build out experimentation datasets and pipelines to allow for granular analysis of A/B tests.
Partner with ML teams to build out data models to be used in model deployment
Work with partner teams to build datasets that will impact production, such as creator and player statistics.
Build out systems to evaluate ML model performance.
WE ARE LOOKING FOR INDIVIDUALS WITH:
5+ years of analytics experience, with substantial experience as an analytics engineer
Have a high level of proficiency in SQL (aggregate functions, window functions, complex joins, CTE), including query optimization skills
Experience coding with Python
Substantial previous experience with pipeline orchestration. DBT and airflow preferred
Experience with dashboarding tools
Familiar with processes for handling event-based data
The base pay range for this position is listed below; please note the base pay may vary depending on location, job-related knowledge, skills, and experience. Stock options and, in some cases, a sign-on bonus may be offered as part of the compensation package. We also offer a full slate of benefits, including flexible vacation, medical, dental vision, life and disability coverage, long-term care insurance, FSA, commuter benefits, a 401(k) plan with company match, and a parental leave program. We also offer some not-so-standard benefits, including equipment, family, and pet care stipends.
Base Pay Range
$138,750—$231,250 USD
COMPANY INFO TO KNOW:
Rec Room offers generous medical, dental, and vision plans that cover you, your spouse/domestic partner, and children. We also support your retirement benefits with a company match. Rec Room values work-life balance by providing unlimited paid time off. Our company values are real and drive our culture. We work hard to be a safe and friendly place for people from all walks of life.
Rec Room provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Applicants who are in need of a reasonable accommodation for any part of the application process may contact, in confidence, accessibilityrequest.hr@recroom.com. Rec Room will work with each individual to define their application-related needs and to try to accommodate those needs.
Applicants can find our CCPA disclosure notice here.
Show Less
Report",4.5,51 to 200 Employees,2016,Company - Private,Video Game Publishing,Media & Communication,Unknown / Non-Applicable
Sr. Data Visualization Engineer,$124K - $184K (Employer est.),DocuSign3.7 ★,"San Francisco, CA","Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

The Senior Data Visualization Engineer will work with business partners from across DocuSign to develop and improve analytics dashboards that drive DocuSign business. This work involves executing all steps in the dashboard development lifecycle in a fast paced and quickly growing environment. Additionally, this position will also be responsible for developing dashboards for executive staff.

Your dashboard development expertise will ensure a best-in-class implementation of Tableau at DocuSign and help drive strategy and vision for the business intelligence practice.

This position is an individual contributor role reporting to the Senior Manager, Data & Analytics.

Responsibility
Deliver complex Tableau dashboards by executing the end-to-end development lifecycle including requirements gathering, specification documentation, model architecture and development, dashboard prototyping and development, data catalog integration, user acceptance testing, production deployment, and optimizations and enhancements
Understand and translate business requirements into dashboard design
Apply best practices when it comes to UI as well as performance of the dashboard
Contribute to Tableau Center of Excellence program
Champion corporate policies and laws regarding data privacy and security
Keep the Tableau platform running optimally
Participate as part of an agile team
Contribute to the strategy and vision for centralized dashboards


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)

Positions at DocuSign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within DocuSign. DocuSign reserves the right to change a position's job designation depending on business needs and as permitted by local law.


What you bring

Basic
8+ years of developing Tableau dashboards against high volume data sources
BS degree in Computer Science or related field
Experience developing data models to efficiently support analytic dashboards
Experience with SQL
Experience working with Figma/Miro for dashboard prototyping

Preferred
Master’s degree in Computer Science or related field


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $130,500 - $208,050 base salary

Washington and New York (including NYC metro area): $123,700 - 184,275 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.
Apply Now: click Apply Now
Show Less
Report",3.7,5001 to 10000 Employees,2003,Company - Public,Information Technology Support Services,Information Technology,$1 to $5 billion (USD)
Senior Data Engineer,$130K - $150K (Employer est.),Reorg Research3.6 ★,Remote,"A market leader in credit intelligence, Reorg brings together journalists, financial analysts, legal analysts, technologists, and data scientists to collect and synthesize highly complex information into actionable intelligence. Since 2013, tens of thousands of professionals across hedge funds, investment banks, management consulting, and law firm verticals have come to rely on Reorg to make better, faster, and more confident decisions in pace with the fast-moving credit markets. For more information, visit: www.reorg.com
Working at Reorg
Consistent with our growth, Reorg hires innovators and trailblazers across the globe to drive our business and our incredible corporate culture alike. Our core values – Action Oriented, Customer First Mindset, Effective Team Players, and Driven to Excel – define an organizational ethos that’s as high-performing as it is human. Among other perks, Reorg employees enjoy competitive health benefits, matched 401k and pension plans, Paid time off, generous parental leave, gym subsidies, educational reimbursements for career development, recognition programs, pet-friendly offices, and much more.

The Role

We are seeking a highly skilled and experienced Senior Data Engineer with a strong background in building and managing data pipelines, data warehouses, and data lakes. As a Senior Data Engineer, you will play a pivotal role in our organization's data infrastructure, enabling efficient and reliable data processing, storage, and analysis.
Responsibilities
Design and develop robust, scalable, and efficient data pipelines to support the extraction, transformation, and loading (ETL) processes from various data sources into data warehouses and data lakes.
Collaborate closely with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and design optimal solutions.
Build and manage data warehouses and data lakes to store and organize large volumes of structured and unstructured data efficiently.
Implement data governance processes and best practices to ensure data quality, integrity, and security throughout the data lifecycle.
Identify and address performance bottlenecks, data inconsistencies, and data quality issues in data pipelines, warehouses, and lakes.
Develop and maintain monitoring and alerting systems to proactively identify and resolve data-related issues.
Continuously evaluate and explore emerging technologies and tools in the data engineering space to improve data processing efficiency and scalability.
Mentor and guide junior data engineers, providing technical leadership and fostering a collaborative and innovative environment.
Requirements
Bachelor's degree in Computer Science or a related field.
Proven experience (minimum 5 years) in building and managing data pipelines, data warehouses, and data lakes in a production environment.
Proficiency in programming languages such as Python, SQL and experience with data processing frameworks like Apache Spark or Apache Beam.
Experience ETL/ELT frameworks and tools like AWS Glue, dbt, Airflow, Airbyte, etc.
In-depth knowledge of relational databases (e.g., MySQL, PostgreSQL) and experience with columnar storage technologies (e.g., Redshift, Snowflake).
Strong understanding of distributed systems, data modeling, and database design principles.
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and experience in deploying data infrastructure on the cloud.
Experience with containerization technologies like Docker and container orchestration systems like Kubernetes.
Excellent problem-solving and troubleshooting skills, with a strong attention to detail.
Effective communication and collaboration skills to work effectively with cross-functional teams.
Ability to adapt to a fast-paced and rapidly changing environment.
Preferences
Experience with real-time data processing frameworks like Kafka, Spark.
Knowledge of data warehousing concepts and technologies, such as data modeling, star schemas, and OLAP.
Familiarity with big data technologies and frameworks, such as Hadoop, Hive, and Presto.
$130,000 - $150,000 a year

At Reorg, we consider a range of factors in connection with compensation decisions, including experience, skills, location, and our business needs and limitations. As a result, compensation may vary within and across similar roles and positions. Please note that the salary range information below is a good faith estimate for this position and actual compensation for any individual may fall outside this range if warranted by the circumstances applicable to that individual. If we identify a role that would be suitable for a broader range of skills and experience such that we would consider hiring at multiple levels then the range listed below may reflect that breadth.

The above-shared salary range is an estimate for this position. The actual compensation will be at Reorg’s sole discretion and will be determined by the aforementioned and other relevant factors. This position is eligible for an annual discretionary bonus.
Reorg provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Reorg complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
Show Less
Report",3.6,201 to 500 Employees,2012,Company - Private,Research & Development,Management & Consulting,Unknown / Non-Applicable
"Software Engineer, Data Platform",$99K - $160K (Glassdoor est.),Inclusively5.0 ★,"Seattle, WA","Inclusively is partnering with a one of the largest transportation networks to hire a Software Engineer, Data Platform.
ABOUT INCLUSIVELY:
Inclusively is a digital tech platform that connects candidates with disabilities, who may benefit from workplace accommodations, to inclusive employers. This includes all disabilities under the ADA, including mental health conditions (e.g. anxiety, depression, PTSD), chronic illnesses (e.g. diabetes, Long COVID), and neurodivergence (e.g. autism, ADHD). Applicants with one or more of these conditions are encouraged to apply; Inclusively does not require applicants to disclose their specific disability.
Responsibilities:
Design, develop, deploy, monitor, operate and maintain existing or new elements of our platform
Help establish roadmap and architecture based on technology and our needs
Write well-crafted, well-tested, readable, maintainable code
Analyze our internal systems and processes and locate areas for improvement/automation
Collaborate with product org stakeholders to address and prioritize custom edge cases
Help lead large projects from inception to positive execution
Unblock, support and communicate with internal partners to achieve results
Experience:
3+ years of software engineering industry experience and with data structures/algorithms
2+ years of experience building and developing large-scale infrastructure, distributed systems or networks, and/or experience with data infrastructure
Experience working with kubernetes and container technologies (e.g. Docker, cri-o, etc)
Familiar with a cloud-based environments such as AWS/GCP/Azure
Benefits:
Great medical, dental, and vision insurance options
Mental health benefits
Family building benefits
In addition to 12 observed holidays, salaried team members have unlimited paid time off, hourly team members have 15 days paid time off
401(k) plan to help save for your future
18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible
Pre-tax commuter benefits
Out team members get an exclusive opportunity to test new benefits of our Ridership Program
Job Type: Full-time
Benefits:
401(k)
Health insurance
Paid time off
Parental leave
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",5.0,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Data Engineer,$130K - $165K (Employer est.),Fabletics4.4 ★,California,"Job Description
How Do You Fit In?
We are currently seeking a highly skilled and motivated Data Engineer to join our dynamic team. If you have a passion for data analysis, possess strong technical skills, and are excited about leveraging data to drive business insights, this position is for you. As a Data Engineer, you will join a tight knit group of key contributors who are actively working together to achieve aggressive goals and meet timelines to drive the business forward.

This position will report to the Manager, Data Engineering (EDW), Data Platforms.

What You Will Do:
Work closely with stakeholders to understand their data needs and provide data-driven solutions.
Design, develop, and maintain scalable data pipelines and ETL processes to collect, process, and analyze large volumes of structured and unstructured data.
Solid understanding and experience in data modeling, including designing and implementing efficient data models.
Build and optimize data models, data warehouses, and data marts to support reporting and analytics needs.
Perform exploratory data analysis to identify trends, patterns, and insights that contribute to business decision-making.
Stay up-to-date with the latest trends, tools, and technologies in data analytics and apply them to enhance data-driven decision-making processes.
Perform code reviews to ensure data engineering best practices, code quality, and maintainability.
Provide production support on a rotating basis to ensure the availability, reliability, and performance of data analytics systems and processes.

What You Can Bring:
Bachelor’s degree in Computer science, Engineering, Mathematics, Statistics, or a related field. A master’s degree is a plus.
4 years of experience in creating and managing data pipelines.
1 year of experience with Python or another scripting language.
Experience in performing code reviews and ensuring adherence to best practices.
Excellent communication and presentation skills, with the ability to effectively convey complex data findings to non-technical stakeholders.
Proven experience in data analytics, data engineering, or a related role.
Strong expertise in SQL, particularly in an analytics/reporting capacity, with significant experience in creating and maintaining reporting processes.
Nice to Have:
Familiarity with traditional data warehousing concepts (e.g., Kimball methodology).
Experience in data engineering on cloud platforms such as BigQuery, Redshift, Snowflake, Teradata, Vertica, etc.
Knowledge of Dbt and/or Airflow for data pipeline management.
Previous experience in e-commerce, retail, or internet industries.
Compensation & Total Rewards:
At TechStyleOS, we believe work and life should fit together! We continue to build a culture of flexibility, to empower you to do your best and put yourself first. Our Total Rewards program rewards employees for their hard work, supporting their health, well-being, families, and ultimately their life journey. Total Rewards at TechStyleOS includes:

Hybrid Work Schedule*
Unlimited Paid Time Off*
Summer Fridays*
Healthcare Plans
Employee Discounts
401k
Annual Bonus Program
Equity Program*
And More

Varied for retail and fulfillment roles

The annual base salary range for this position is from $130,000-$165,000. The range provided includes the base salary that TechStyleOS expects to pay for the role. Offered base salary will be dependent on factors including the scope and complexity of the role, candidate’s related work experience, subject matter expertise and work location.
#LI-GR1
#LI-TechStyleOS
About TechStyleOS
TechStyleOS is the globally integrated Operations and Services provider behind some of the fastest growing online fashion brands in history, including Fabletics, Savage X Fenty, JustFab, ShoeDazzle, and FabKids. With capabilities spanning technology, data science, supply chain management, fulfillment, customer service, and more, we help brands launch, scale and grow—across product categories and geographically. From predictive analytics to data-driven marketing and attribution, our unique approach is powered by our proprietary, end-to-end tech platform that enables the brands we serve to deliver a level of personalization, value, and satisfaction that are unrivaled in the fashion industry.
Fabletics, Inc. is an equal opportunity employer. We recruit, employ, compensate, develop, and promote regardless of race, national origin, religion, sex, sexual orientation, gender identity, age, disability, genetic information, veteran status, and other protected status as required by applicable. At Fabletics, Inc., we champion a vibrant workplace culture that thrives on diversity law and do not tolerate discrimination or harassment. We are one team from many backgrounds, innovating through diversity of individuals, who are driven by passion for creating an inclusive space for all. Fabletics, Inc. will continue to champion a workplace culture that prizes diversity and inclusivity.
We encourage you to apply regardless of meeting all qualifications and/or requirements.
Apply Now: click Apply Now
Show Less
Report",4.4,Unknown,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Data Engineer,$110K (Employer est.),Capitol Federal2.9 ★,"Topeka, KS","Job Description:
Pay: up to $110,000 Annually
Job Type: Full Time
The Data Engineer assists in setting overall development roadmap and standards for the Bank and helps evaluate and architect the use of data solutions, using industry best practices. This position works as part of a collaborative team to design, code, and implement data solutions to support internal business requirements or external customers and vendors. An innovative mindset and an ability to translate complex business scenarios into a technical solution is required. This position performs a variety of tasks under general supervision. The position reports directly to an IT manager and requires regular, predictable and timely attendance at work to meet department workload demands.
Paid time off and holiday available on your first day! Benefits available to anyone working 20 hours or more per week!
CapFed® is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Job Type: Full-time
Work Location: In person
Show Less
Report",2.9,501 to 1000 Employees,1893,Company - Public,Banking & Lending,Financial Services,$100 to $500 million (USD)
"Software Engineer, Data Platform",$115K - $175K (Glassdoor est.),Notion4.8 ★,"San Francisco, CA","About Us:
We're on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us can't build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft.
We've been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, we're growing fast and excited for new teammates to join us who are the best at what they do. We're passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide.
About The Role:
You'll join a team of talented engineers who will design and own foundational data products that are key to the company's business and product. Notion's data platform and infrastructure are vital to both empowering every team at Notion to make decisions using data, and are increasingly used in our product features, like search, user notifications, workspace analytics and Notion AI.
What You'll Achieve:
You'll work cross-functionally with partners from the Data Science, Data Engineering, AI, Product, Go-to-Market, Legal and Finance organizations to deliver short- and long-term impact
You'll help in executing the roadmap for data infrastructure and systems to power high volume product features using Notion's data.
You'll play a pivotal role in the development of tools and infrastructure that democratize data access and enable analytics capabilities across the organization
You'll determine the best ways to handle Notion's unique data model and usage patterns to derive insights and bring intelligence to product features like search and discovery.
Skills You'll Need to Bring:
You have worked cross-functionally to establish the right overarching data architecture for a company's needs, to build data ingestion (real-time & batch), and to provide guidance on best data practices for the business.
You have worked on data or infrastructure-focused engineering teams, particularly ones that own a wide swath of software platforms (hosted or built in-house).
You've experienced the challenges of scaling and re-architecting data platforms and infrastructure through orders of magnitude of growth and scaling data volume.
You have a deep background with big data compute, storage, and best practices that you can ask the right questions of your team, balance technology and people concerns, and make hard tradeoffs.
Nice to Haves:
You've built out data infrastructure from, or nearly from, scratch at a fast-growing startup.
You've led or managed a Data Engineering / Platform / Infrastructure Team.
You have experience building MLOps and ML serving infrastructure.
Our customers come from all walks of life and so do we. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. If you share our values and our enthusiasm for small businesses, you will find a home at Notion.
Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation made due to a disability, please let your recruiter know.
#LI-Onsite
Show Less
Report",4.8,201 to 500 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
Azure Data Engineer,$84K - $191K (Employer est.),PSRTEK4.5 ★,"Berwick, ME","Job Title : Azure Data Engineer
Job Type: Contract
Job Description
Atleast 2 years of experience working at onsite closely with client.
Strong exp with Azure Cloud Technologies, including ADF, Databricks, SQL DB, ADLS
Experience with data transformation and manipulation using Azure Databricks
Working knowledge of Azure DevOps CI/CD tools and concepts Azure pipelines, GitHub
Scripting experience with Python and Rest API
Experience in implementing CDC (Change Data Capture) Data Virtualization
Experience in creating base view, Derived View and Data source connections using Denodo
Experience in Denodo caching in snowflake
Experience in creating data models by extracting from various data sources by connecting through JDBC, ODBC and Web API in Denodo Skills Required: Python ADF ADB SQL Denodo Snowflake
Job Type: Contract
Salary: $84,181.34 - $190,840.44 per year
Ability to commute/relocate:
Berwick, ME 03901: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 5 years (Preferred)
SQL: 6 years (Preferred)
Data warehouse: 5 years (Preferred)
Work Location: In person
Speak with the employer
+91 609-934-3291
Show Less
Report",4.5,Unknown,-1,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Sr. Data Engineer - Remote,-1,Chamberlain Group3.7 ★,Illinois,"Chamberlain Group is a global leader in access solutions with top brands, such as LiftMaster and Chamberlain, found in millions of homes, businesses, and communities worldwide.
As a leader in the Smart Home industry, we boast one of the largest IoT install bases, with innovative products consisting of cameras, locks, card readers, garage door openers, gates and more, all powered by our myQ digital ecosystem.
This role is responsible for providing technical expertise and leadership to design and deliver end-to-end data engineering solutions to support advanced analytics capabilities and drive innovation and decision-making
across Chamberlain.
Essential Duties and Responsibilities
Build and maintain real-time and batch data pipelines across the advanced analytics platform.
Design, develop and orchestrate highly robust and scalable ETL pipelines.
Design and implement Dimensional and NoSQL data modelling as per the business requirements.
Develop highly optimal codebase and perform Spark optimizations for Big Data use cases.
Design, develop and deploy optimal monitoring and testing strategy for the data products.
Collaborate with stakeholders and advanced analytics business partners to understand business needs and translate requirements into scalable data engineering solutions.
Collaborate with data scientists to prepare data for model development and production.
Collaborate with data visualization and reporting application developers to ensure the sustainability of production applications and reports.
Collaborate with data architects on the enhancement of Chamberlain’s enterprise data architecture and platforms.
Provide leadership to third-party contractors.
Comply with health and safety guidelines and rules.
Protect CGI’s reputation by keeping information confidential.
Maintain professional and technical knowledge by attending educational workshops, professional publications, establishing personal networks, and participating in professional societies.
Minimum Qualifications
Education/Certifications:
Bachelor’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
Knowledge, Skills, and Abilities:
Natural sense of urgency, teamwork, and collaboration reflected in daily work ethic.
Proficient in Spark or Databricks, Cloud Data Engineering Services preferably Azure, Streaming frameworks like Event Hubs or Kafka.
Proficient in Microsoft Office.
Familiarity with modern Machine Learning Operationalization techniques.
Agile methodologies.
Familiarity with Data visualization tools, such as Qlik or Power BI.
Preferred Qualifications
Education/Certifications:
Master’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
2+ years of professional experience delivering engineering for advanced analytics or data science solutions
Knowledge, Skills, and Abilities:
Agile methodologies
Experience with IoT Data Architecture.
Machine Learning Operationalization (MLOps) proficiency.
REST API design and development.
Proficiency with streaming design patterns.

The pay range for this position is $103,300.00 to $177.475.00; base pay offered may vary depending on a number of factors including, but not limited to, the position offered, location, education, training, and/or experience. In addition to base pay, also offered is a comprehensive benefits package and 401k contribution (all benefits are subject to eligibility requirements).
This position is eligible for participation in a short-term incentive plan subject to the terms of the applicable plans and policies.
#LI-Remote
We're an organization who values its human capital and provides support to assist its employees succeed.

Chamberlain Group is proud to be an Equal Opportunity Employer. You will be considered for this position based upon your experience and education, without regard to race, color, religion, sex, national origin, age, sexual orientation, ancestry; marital, disabled or veteran status. We are committed to creating and maintaining a workforce environment that is free from any form of discriminations or harassment.

Persons with disabilities who anticipate needing accommodations for any part of the application process may contact, in confidence
Recruiting@Chamberlain.com
.

NOTE: Staffing agencies, headhunters, recruiters, and/or placement agencies, please do not contact our hiring managers via email or phone or other methods.
Start your job application: click Apply Now
Show Less
Report",3.7,1001 to 5000 Employees,1900,Company - Private,Consumer Product Manufacturing,Manufacturing,$500 million to $1 billion (USD)
"Lead Engineer, Data Platform",$199K - $200K (Employer est.),Sephora3.6 ★,"San Francisco, CA","Job ID: 226189
Location Name: FSC REMOTE SF/NY/DC -173(USA_0173)
Address: FSC, Remote, CA 94105, United States (US)
Job Type: Full Time
Position Type: Regular
Job Function: Information Technology
Remote Eligible:

Company Overview:
At Sephora we inspire our customers, empower our teams, and help them become the best versions of themselves. We create an environment where people are valued, and differences are celebrated. Every day, our teams across the world bring to life our purpose: to expand the way the world sees beauty by empowering the Extra Ordinary in each of us. We are united by a common goal - to reimagine the future of beauty.

The Opportunity:
Your role at Sephora:
As a Lead Engineer, Data Platforms at Sephora, you will: Streamline the intake of the raw data into the Azure Data Lake. Perform production support and deployment activities. Proactively drive the execution of core data engineering, business intelligence, and data warehouse frameworks. Build data pipelines from systems including CRM and Ecommerce, with the emphasis on scalability and reliability. Leverage central data warehouse with other data sources to create enriched customer information in the CRM system. Analyze and translate business needs into data models to support long-term, scalable, and reliable solutions. Create logical and physical data models using best practices to ensure high data quality and reduced redundancy. Drive data quality across the organization. Develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking. Define and manage SLA’s for data sets and processes running in production. Continuously improve our data infrastructure and stay ahead of technology. Design a system for data backup in case of system failure. Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs. (Position allows some work-from- home flexibility, with schedule to be approved by manager. Must be able to work on site as required.)

We are excited about you if you have:
Bachelor’s or foreign equivalent degree in Computer Science, Engineering, or Information Technology.
Six (6) years of progressively responsible post-baccalaureate experience in the design of data warehouse architecture.

Experience must include:
Scala
Spark
Python
Business intelligence and data warehouse frameworks
Data engineering
Data modeling and pipelines
Data warehouse production support
Data warehouse SLAs and best practices
Programming in SQL
Databricks
IBM Datastage

Salary: $199,098 to $200,000 per year depending on experience
Working at Sephora’s Field Support Center (FSC)
Our North American operations are based in the heart of San Francisco’s Financial District, but you won’t hear us call it a headquarters – it’s the Field Support Center (FSC). At the FSC, we support our stores in providing the best possible experience for every client. Dedicated teams cater to our client’s every need by creating covetable assortments, curated content, compelling storytelling, smart strategy, skillful analysis, expert training, and more. It takes a lot of curious and confident individuals, disrupting the status quo and taking chances. The pace is fast, the fun is furious, and the passion is real. We never rest on our laurels. Our motto? If it’s not broken, fix it.

As a condition of employment, Sephora requires all newly hired employees to be fully vaccinated against COVID-19 by their start date unless they have requested and received an exemption due to a qualifying medical condition, a sincerely held religious belief or practice, or a requirement by law.

Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities.

Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.

The actual base salary offered depends on a variety of factors, which may include, as applicable, the applicant’s qualifications for the position; years of relevant experience; specific and unique skills; level of education attained; certifications or other professional licenses held; other legitimate, non-discriminatory business factors specific to the position; and the geographic location in which the applicant lives and/or from which they will perform the job. Individuals employed in this position may also be eligible to earn bonuses. Sephora offers a generous benefits package to full-time employees, which includes comprehensive health, dental and vision plans; a superior 401(k) plan, various paid time off programs; employee discount/perks; life insurance; disability insurance; flexible spending accounts; and an employee referral bonus program.

While at Sephora, you’ll enjoy…

The people. You will be surrounded by some of the most talented leaders and teams – people you can be proud to work with.
The learning. We invest in training and developing our teams, and you will continue evolving and building your skills through personalized career plans.
The culture. As a leading beauty retailer within the LVMH family, our reach is broad, and our impact is global. It is in our DNA to innovate and, at Sephora, all 40,000 passionate team members across 35 markets and 3,000+ stores, are united by a common goal - to reimagine the future of beauty.

You can unleash your creativity, because we’ve got disruptive spirit. You can learn and evolve, because we empower you to be your best. You can be yourself, because you are what sets us apart. This, is the future of beauty. Reimagine your future, at Sephora.

Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities.

Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.

As a condition of employment, Sephora requires all newly hired employees to be fully vaccinated against COVID-19 by their start date unless they have requested and received an exemption due to a qualifying medical condition, a sincerely held religious belief or practice, or a requirement by law.
Apply Now: click Apply Now
Show Less
Report",3.6,10000+ Employees,1969,Company - Private,Beauty & Personal Accessories Stores,Retail & Wholesale,$1 to $5 billion (USD)
Data Engineer Cloud Analytics,-1,SMK Soft Inc,Remote,-1,3.6,-1,-1,-1,-1,-1,-1
Expert Data Engineer (Remote),$82K - $119K (Glassdoor est.),Experian4.2 ★,"Costa Mesa, CA","Company Description

Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been named in the 100 “World’s Most Innovative Companies” by Forbes Magazine. Experian Consumer Information Services is redefining the way our clients do business within all aspects of the customer credit lifecycle. Fueled by best-in-class data and innovative technology we help businesses make smarter decisions, identify consumers, make decisions on loans, market to prospects and collect.

Job Description

We are looking for an Expert ETL Data Engineer who will be responsible for building data pipelines, data warehouse solutions, and analytics processing tools to democratize data.
About us, but we’ll be brief
Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. We are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to work for. In addition, for the last five years we’ve been named in the 100 “World’s Most Innovative Companies” by Forbes Magazine.
This position will be supporting the Experian Consumer Services - a passionate and innovative team with a mission to provide Financial Power to All™. Our portfolio offers credit education and identity protection solutions to consumers and helps businesses manage the impact of a data breach.
What you’ll be doing
Design and develop foundational components of the BI platform which includes Data Engineering, Data Quality, and Data Catalog framework.
Partnering with Program Managers, Subject Matter Experts, Architects, Engineers, and Data Scientists across the organization where appropriate to understand customer requirements, design prototypes, and optimize existing data services/products.
Design and maintain data warehouse solutions that allow for large-scale analytics processing.
Build scalable ETL data pipelines, ingesting high volume of data from internal and external sources.
Build and maintain Data Engineering solutions that support self-service BI Platform, partnering with BI Engineers to deliver end to end solutions.
Work closely with product teams to understand business success criteria, translate business needs into technical requirements, worked with other Data Engineers and enable project success.
Ability to develop high-performing data engineering team with an inspiring leadership style.
Curious, detail oriented, and highly motivated self-starter. Able to work independently and with the team to formulate innovative solutions
Troubleshoot and resolve data, system, and performance issues.
Participant in production support on-call rotation
#LI-REMOTE

Qualifications

What your background looks like
Minimum 10 years of experience in Data Engineering development and support
5 years of experience within big data domain
5 years of experience in Python scripting
5 years of experience with AWS ecosystem (Redshift, EMR, MWAA, S3, etc.)
2 years of experience with BI tools such as Tableau, Alteryx
5 years of experience in Agile development methodology
Excellent communication skills
Ability to understand complex metrics and translate business requirements to technical requirements
Ability to multitask and prioritize an evolving workload in a fast pace environment.
Proven track record of leading large-scale project
Education: BS degree or higher in computer science or related fields

Additional Information

All your information will be kept confidential according to EEO guidelines.
Our compensation reflects the cost of labor across several U.S. geographic markets. The base pay range for this position is listed above. Within this range, individual pay is determined by work location and additional factors such as job-related skills, experience and education. This position is also eligible for a variable pay opportunity and a comprehensive benefits package which includes health, life and disability insurance, generous paid time off including paid parental and family care leave, an employee stock purchase plan and a 401(k) plan with a company match.
Experian is proud to be an Equal Opportunity and Affirmative Action employer. We’re passionate about unlocking the power of data to transform lives and create opportunities for consumers, businesses, and society. For more than 125 years, we’ve helped people and economies flourish – and we’re not done.
We take our people’s agenda very seriously. We focus on what truly matters; diversity and inclusion, work/life balance, flexible working, development, collaboration, wellness, reward & recognition, volunteering, making an impact... the list goes on. See our DEI work in action!
The power of YOU. We are building a culture where everyone is comfortable bringing their whole self to work. A place where we not only respect our differences and values but celebrate them in a positive and supportive environment.
Find out what is like to work for Experian and discover the Unexpected!
To apply to this job, click Easy Apply
Show Less
Report",4.2,10000+ Employees,1980,Company - Public,Business Consulting,Management & Consulting,$5 to $10 billion (USD)
Data Engineer I,$80K (Employer est.),VirginPulse3.4 ★,Remote,"Overview:
Now is the time to join us!
At Virgin Pulse we value and celebrate diversity and we are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Why? Because diversity inspires innovation, collaboration, and challenges us to produce better solutions. But more than this, diversity is our strength, and a catalyst in our ability to #changelivesforgood.
Responsibilities:
Who are you?
Data Engineer I perform development activities with the guidance of another member of the data engineering team. You will work closely with account management, ETL, data warehouse, business intelligence, and reporting teams as you develop data pipelines and enhancements and investigate and troubleshoot issues.

In this role you will wear many hats, but your knowledge will be essential in the following:
Extracting, cleansing, and loading data.
Building data pipelines using SQL, Python, and other technologies.
Triage incoming bugs and incidents.
Perform technical operation tasks.
Investigate and troubleshoot issues with data and data pipelines.
Participation in sprint refinement, planning, and kick-off to help estimate stories, raise awareness and additional implementation details.
Help monitor areas of the data pipeline and raise awareness to team when issues arise.
Performing quality assurance work to verify the accuracy of code and data results
You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
Qualifications:
What you bring to the Virgin Pulse team
In order to represent the best of what we have to offer you come to us with a multitude of positive attributes including:
1– 2 years or less experience in data engineering
SW certification or degree in IT related field
You also take pride in offering the following Core Skills, Competencies, and Characteristics:
Solid grasp of modern relational and non-relational models and differences between them.
Proficiency in writing SQL, the use of Excel, and some analytical tools.
Understanding of REST API.
Understanding of JSON.
Detail oriented and able to examine data and code for quality and accuracy.
Knowledge of Agile environments, including Scrum and Kanban methodologies
Python / R / programming language experience preferred
ETL experience preferred
AWS Lambda / Console experience preferred
Git experience preferred
No candidate will meet every single desired qualification. If your experience looks a little different from what we’ve identified and you think you can bring value to the role, we’d love to learn more about you!

#LI-REMOTE

Why work at Virgin Pulse?
We believe a career should provide competitive pay and benefits, a collaborative and supportive culture and cutting-edge technology and services. Virgin Pulse is an equal opportunity organization and is committed to diversity, inclusion, equity and social justice. To that end, we make a particular effort to recruit candidates from minoritized backgrounds to apply for open positions.

In compliance with all states and cities that require transparency of pay, the base compensation for this position ranges up to $80,000 annually. Note that salary may vary based on location, skills, and experience. This position is eligible for [10%] target bonus/variable compensation as well as health, dental, vision, mental health and other benefits.
Show Less
Report",3.4,501 to 1000 Employees,2004,Company - Private,Computer Hardware Development,Information Technology,$100 to $500 million (USD)
Software Engineer | Data Platform,$153K - $180K (Employer est.),Ramp Financial4.5 ★,"New York, NY","Location
New York, Miami, Remote
Type
Full time
Department
Engineering

About Ramp
Ramp is building the next generation of finance tools—from corporate cards and expense management, to bill payments and accounting integrations—designed to save businesses time and money with every click. Over 12,000 customers cut their expenses by 3.5% per year and close their books 8x faster by switching to the Ramp platform.
Founded in 2019, Ramp powers the fastest-growing corporate card and bill payment software in America and enables billions of dollars of purchases each year. Ramp continues to grow quickly, more than doubling its revenue run rate in the first half of 2022.
Valued at $8.1 billion, Ramp's investors include Founders Fund, Stripe, Citi, Goldman Sachs, Coatue Management, D1 Capital Partners, Redpoint Ventures, General Catalyst, and Thrive Capital, as well as over 100 angel investors who were founders or executives of leading companies. The Ramp team comprises talented leaders from leading financial services and fintech companies—Stripe, Affirm, Goldman Sachs, American Express, Mastercard, Visa, Capital One—as well as technology companies such as Meta, Uber, Netflix, Twitter, Dropbox, and Instacart. Ramp was named Fast Company’s #1 Most Innovative Company in North America in 2023 and #5 on LinkedIn Top Startups 2022.
About the Role
The Data Platform team develops and owns the systems that enable Ramp's reporting and strategic decision-making, as well as integrating machine learning models into our Risk systems and the product. As a member of the Data Platform team, you’ll build and maintain the infrastructure that enables Ramp to realize value from data. You’ll also partner with Ramp’s analytics engineers, data scientists, and other data professionals to build internally and externally facing data products.
Our ideal candidate is excited about building systems for data collection, processing, storage, and retrieval, and is also passionate about making these systems observable, reliable, scalable, and highly automated.
What You’ll Do
Build and integrate the components of Ramp's Analytics Platform and Machine Learning Platform
Build tools that improve the agility and data experience of Ramp's Data Scientists, Analytics Engineers, Engineers, and Operations teams
Build the batch and streaming data pipelines critical to Ramp’s daily operations using Airflow, Snowflake, Materialize, and other data processing technologies
Collaborate with stakeholder teams on building and productionizing analytical products and machine learning models
Build reliable, scalable, maintainable, and cost-efficient systems across the stack
What You Need
Minimum 2 years of experience with workflow orchestrators like Airflow, Dagster, or Prefect
Minimum 2 years of experience building infrastructure on AWS, GCP, or Azure
Knowledge of SQL and experience with Snowflake, Redshift, BigQuery, or similar databases
Intuition around analytics and machine learning
Strong Python programming skills
Track record of building highly-reliable infrastructure for data storage and processing
Nice to Haves
Expertise with AWS
Expertise with the Modern Data Stack - dbt, Looker, Snowflake, and Fivetran
Expertise with building and deploying machine learning systems.
Experience with Terraform and Datadog
Compensation
The annual salary/OTE range for the target level for this role is $153,000-$180,000 + target equity + benefits (including medical, dental, vision, and 401(k)
Ramp Benefits (for U.S. based employees)
100% medical, dental & vision insurance coverage for you
Partially covered for your dependents
One Medical annual membership
401k (including employer match)
Please note only 401k contributions made while employed by Ramp are eligible for an employer match
Flexible PTO
Fertility HRA (up to $5,000 per year)
WFH stipend to support your home office needs
Wellness stipend
Parental Leave
Relocation support
Pet insurance
Show Less
Report",4.5,201 to 500 Employees,2019,Company - Private,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
Data Engineer,$76K - $106K (Glassdoor est.),Greystar Real Estate Partners LLC3.8 ★,"Irving, TX",-1,4.5,-1,-1,-1,-1,-1,-1
Data Engineer,$150K - $180K (Employer est.),"Amino, Inc.4.4 ★","San Francisco, CA","About Amino - What We Do
At Amino, we are a leading innovator in healthcare, empowering individuals to take charge of their healthcare journey. Our powerful digital tools help navigate and unlock the full potential of health plan options. With Amino's cutting-edge solutions, health plan members can easily understand and make informed decisions about their benefits, considering factors such as available programs, provider network quality, and cost.
We are excited to announce that we have recently secured additional funding to fuel our growth and continue developing market-leading navigation tools. As part of this journey, we are looking for a talented and driven Data Engineer to join us. You will play a vital role in helping millions of people understand and optimize their health plan options.
The Role
As a Data Engineer at Amino, you will be at the heart of our organization. Your primary responsibilities will involve processing, transforming, and organizing large and complex datasets that power our products. We are expanding our data sources and enhancing our current offerings, and we need a Data Engineer to help us build competitive and compliant healthcare guidance products.
Key Priorities and Projects:
Collaborate with Product Managers, Architects, and senior leadership to translate high-level requirements into detailed plans.
Integrate new data sources from vendors and internal systems into our ETL (Extract, Transform, Load) pipelines.
Enhance our editorial tools for creating and maintaining accurate data about health entities.
Modernize our orchestration infrastructure using open-source tools like dbt and Dagster.
Impact you will have:
Revamp and expand critical data pipelines that drive our existing and future products.
Mentor other junior data engineers and share your expertise.
Collaborate closely with data scientists and software engineers in related teams.
Influence the foundational aspects of data modeling and warehousing, benefiting various teams across the company.
Utilize modern data stack technologies to build the foundation for Amino's next-generation data assets.
Technologies you will work with:
Snowflake, Python 3, dbt, Docker, AWS, Looker, Databricks, Spark, Jenkins CI/CD, Airflow, Dagster, Terraform, PostgreSQL, RDS, Elasticsearch, and Kinesis.
Skills and Experience you should possess:
Familiarity with various relational and non-relational databases.
Proficiency in writing clean and well-tested code using Python.
Experience handling and working with large datasets and the associated tools.
Previous exposure to ETL pipelines and familiarity with data modeling and data warehousing best practices (experience with dbt is preferred).
Strong collaboration and feedback skills, with a willingness to seek and incorporate input from others.
Excellent documentation and verbal communication skills, capable of conveying technical concepts to peers and non-technical stakeholders.
Bonus Skills:
Experience handling PHI or PII
Deep understanding of the healthcare domain, particularly with healthcare claims data.
We offer
We're committed to helping you achieve your best work in a supportive, growth-oriented environment. We have seriously big goals, and expectations are high and we'll equip you with the tools and resources you need to be successful.
Expected base salary: $150k to $180k plus standard company benefits and a generous option grant. Amino values transparency and has included the reasonable estimate of the base salary range for this full-time role at any approved US location. Individual pay is determined by a range of factors, including job-related skills, experience, relevant education or training, licensure or certifications, and other business and organizational needs. Amino does not typically hire at or near the top of a salary range.
We offer full-time employees 100% paid employee healthcare premiums; dependent premium coverage depends on the plan.
401(k) and FSA programs
This position, like all roles at the firm, will have a good deal of autonomy. We're a remote-first team and have designed our culture for a balance of synchronous and asynchronous work with people operating from all over the country. To support your remote office, we provide every new Amino with a generous office set-up allowance plus a monthly stipend for internet/phone.
PTO is non accrual and we expect Amino's to take a minimum of 15 days a year.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We know the reputation and track record that the tech industry has, and work hard to be exceptional in this regard.
Our Culture
We are a small team who believes that success is a group activity. You should expect to learn from everyone at Amino, and be excited to share your knowledge. You will play a big part in influencing the shape of the product and be empowered to provide your thoughts and ideas.
We believe in collaboration, respect, and curiosity. We believe in having a growth mindset, and have a passion for solving problems that have never been faced before. Everyone's input is valued, be it about code, data models, business models, or product ideas.
Show Less
Report",4.4,1 to 50 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Data Engineer,$109K (Employer est.),The Wendy's Company3.4 ★,"Dublin, OH","Overview:
When our square shaped burgers made their first sizzle on the scene more than 50 years ago, people knew our approach wasn’t like any other. Same goes for the way we support our employees. Our culture of openness, flexibility, and inclusiveness allows everybody to flourish in their own way. If you’re looking for a career where you can be part of the action as we continue to grow our iconic brand – We got you!

As a Data Engineer, you’ll be responsible for leading projects by defining business requirements, executing the changes needed, manage small to medium projects which will require coordinating efforts between business partners and vendors to ensure the projects are delivered on time and of high quality. Support various data tools such as Looker, Snaplogic, CRM applications (Salesforce). This includes day to day support, user management / security and permissions, configuration changes, existing integrations, data management, testing and QA, and deployments. This role would also help in the creation and management of reports and dashboards to support organizational needs.
Responsibilities:
Complete sprint assignments, provide level-of effort estimates to develop end-to-end integrations of Wendy’s data platform with various source and target systems
Manage existing implementations and execute system configuration changes
Participate in requirement gathering, story scoping and project timeline efforts to deliver solutions in a timely manner
Partner with other Developers and Product Owners to understand requirements and provide scalable solutions in alignment with business technology strategy and architecture
Support the key stakeholder for change management and training shepherding new features to production
Keep up to date on industry trends and recommend solutions to meet business needs

Minimum Wage: USD $64,000.00/Yr. Maximum Wage: USD $109,000.00/Yr. Qualifications:
Education: Bachelors Degree, required
Minimum 2 years experience with software implementation and data management
Working knowledge of cloud technology tools (Google Cloud preferred) e.g. Looker, Python, Snaplogic, preferred
Exhibits well-developed spoken and written message skills, ability to effectively communicate complicated technical topics in a manner that is understood by non-technical team members
Demonstrated organizational and multi-tasking skills, within a rapidly changing environment
Self-motivated, with the ability to manage own workload with minimal supervision
Ability to question directions / roadmaps / plans in a manner that is insightful & respectful

Wendy’s was built on the premise, ""Quality is our Recipe®,"" which remains the guidepost of the Wendy's system. Today, Wendy's and its franchisees employ hundreds of thousands of people across more than 7,000 restaurants worldwide with a vision of becoming the world's most thriving and beloved restaurant brand.

#LI-Remote
Show Less
Report",3.4,10000+ Employees,1969,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$1 to $5 billion (USD)
Senior Data Engineer,-1,Stuzo4.4 ★,Remote,"Who We Are
Stuzo, with its Open Commerce® product suite and patent pending Wallet Steering® System, empowers Convenience & Fuel Retailers to gain more share of wallet and customer lifetime value than possible with any other solution provider. Stuzo’s unified Open Commerce product suite consists of: Activate for Intelligent 1:1 Loyalty, Transact for Contactless Commerce, and Experience for Cross-Channel Customer Experiences. Stuzo’s solutions are supported by a set of subscription-based program management services and are contractually backed by its 1.5X Performance Guarantee.
What You’ll Do:
We are seeking a Senior Data Engineer to join our team and help us build the next generation of data tools for our retail and commerce loyalty platform. The ideal candidate will have a passion for tackling complex challenges related to extracting insights from transactional data. In this role, you will work closely with our Application Development teams, Customer Support, Business SMEs, and stakeholders to design and implement data pipelines that transform our data into valuable insights. As we strive to scale and deploy new technologies that enhance the user experience and provide deeper insights, your expertise will be critical to our success.
Responsibilities + Activities:
Explores and documents new sources of data.
Implement schema parsers for standard event protocols.
Own Data Quality Management for specific data lake models.
Handle performance improvements and bottleneck removal.
Adds and maintains schemas for elements of the Open Commerce data lake.
Develops and tests data pipelines using PySpark.
Creates notebooks and configures jobs for running data pipelines on Databricks.
Collaborates with the team to ensure quality, consistency, and performance of all pipelines across the entire data platform.
Prepare and support internal documentation for the data lake.
Experience & Skills You’ll Need:
Excellent interpersonal, communication and organizational skills are essential.
Self motivated, ability to work as collaborative member of a team
5-8 years experience working with data in databases such as PostgreSQL, MySQL, etc.
Knowledge of ETL/ELT data pipelines and best practices
3-5 years experience coding and testing with Python
2-3 years experience working with Spark and Structured Streaming
2-3 years experience working with Spark orchestration platforms such as Databricks
Experience with project and bug-tracking systems like Jira
Experience with Scrum processes.
Reports To:
This position reports to the Principal Data Engineer.
Why You’ll Love Working at Stuzo
Stuzo has a unique way of working that is grounded in what we call Team Chemistry. Team Chemistry happens when people with complementary aptitudes and skills collaborate to leverage each other’s strengths while supporting each other in the areas that are harder for one team member than they are for another. Team Chemistry empowers us to combine our strengths together in a manner where we achieve multiples more together than we could individually.
We are looking to add amazing folks to our team who will bring diversity across many lines, including race, ethnicity, religion, sexual orientation, age, marital status, disability, gender identity, sex, and country of origin. And while we’re headquartered in Philadelphia, we have a “Remote First” approach to work and do not see being in the office or living in the Philadelphia area as a requirement. Several of Stuzo’s Senior Leaders (your future peers, we hope) are fully remote.
Show Less
Report",4.4,51 to 200 Employees,-1,Company - Private,Enterprise Software & Network Solutions,Information Technology,Less than $1 million (USD)
Sr. Data Visualization Engineer,$124K - $184K (Employer est.),DocuSign3.7 ★,"San Francisco, CA","Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

The Senior Data Visualization Engineer will work with business partners from across DocuSign to develop and improve analytics dashboards that drive DocuSign business. This work involves executing all steps in the dashboard development lifecycle in a fast paced and quickly growing environment. Additionally, this position will also be responsible for developing dashboards for executive staff.

Your dashboard development expertise will ensure a best-in-class implementation of Tableau at DocuSign and help drive strategy and vision for the business intelligence practice.

This position is an individual contributor role reporting to the Senior Manager, Data & Analytics.

Responsibility
Deliver complex Tableau dashboards by executing the end-to-end development lifecycle including requirements gathering, specification documentation, model architecture and development, dashboard prototyping and development, data catalog integration, user acceptance testing, production deployment, and optimizations and enhancements
Understand and translate business requirements into dashboard design
Apply best practices when it comes to UI as well as performance of the dashboard
Contribute to Tableau Center of Excellence program
Champion corporate policies and laws regarding data privacy and security
Keep the Tableau platform running optimally
Participate as part of an agile team
Contribute to the strategy and vision for centralized dashboards


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)

Positions at DocuSign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within DocuSign. DocuSign reserves the right to change a position's job designation depending on business needs and as permitted by local law.


What you bring

Basic
8+ years of developing Tableau dashboards against high volume data sources
BS degree in Computer Science or related field
Experience developing data models to efficiently support analytic dashboards
Experience with SQL
Experience working with Figma/Miro for dashboard prototyping

Preferred
Master’s degree in Computer Science or related field


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $130,500 - $208,050 base salary

Washington and New York (including NYC metro area): $123,700 - 184,275 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.
Start your job application: click Apply Now
Show Less
Report",3.7,5001 to 10000 Employees,2003,Company - Public,Information Technology Support Services,Information Technology,$1 to $5 billion (USD)
Data Engineer,$130K - $170K (Employer est.),ASSURANCE3.2 ★,"Seattle, WA","About Assurance
Assurance IQ is a technology company headquartered in Seattle. We were acquired by Prudential (NYSE: PRU) to further the joint mission of improving financial wellness across the world.

Our team of world class software engineers, data scientists, and business professionals work every day to expand our product offerings and the reach of our platform. We simplify the complex world of insurance and financial services into straightforward, valuable solutions to improve people's lives. We start by asking customers a few questions, so our system can learn about their needs; from there, our ground-breaking, proprietary platform takes over and analyzes the thousands of data points that make customers unique. This is how we create custom-tailored plans for each customer; plans built precisely for their needs and budget. Our platform serves as the intersection between customer and seller, technology, and the human touch.

At Assurance, we are innovative, persevering, collaborative, calculated, and authentic, and we're working together to improve the lives of millions!

About the Position
As we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.
To be successful in this role, you must possess the following:
Experience with Python and SQL
Experience in data modelling
Business Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.
Comfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct
Comfort with learning new technologies to help the team explore new solutions to existing problems
Excellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.
Enthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.
Bachelors degree in mathematics, statistics, data science or related field of study.
The following additional experience is desired:
You have a proven ability to drive business results by building the right infrastructure that enables data-based insights.
You are comfortable working with a wide range of stakeholders and functional teams.
The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes.
We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail.
You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity.
You can work independently, with little oversight or guidance.

Note: Assurance is required by multiple state and city laws to include the salary range on position postings when hiring in those specific locals. The salary range for this position will be between $130,000-$170,000 and may be eligible for additional bonus or commission plans + benefits. Eligibility to participate in the bonus or commission plans is subject to the rules governing those programs, whereby an award, if any, depends on various factors including, without limitation, individual and/or organizational performance. In addition, employees are eligible for standard benefits package including paid time off, medical, dental and retirement.
Start your job application: click Apply Now
Show Less
Report",3.2,1001 to 5000 Employees,2016,Company - Public,Insurance Agencies & Brokerages,Insurance,$100 to $500 million (USD)
Senior Data Engineer (Remote),$170K (Employer est.),Home Depot / THD3.8 ★,"Atlanta, GA","Position Purpose:
The Senior Data Engineer will lead the approach and execution of integrating new datasets that support analytics and data science initiatives for Home Depot's Customer Experience organization. They will combine, organize, and automate data sets from the contact center, e-commerce, transactional, supply chain, and operational sources. Furthermore, the role entails stitching and migrating various customer-related data sources into Google BigQuery, maintaining the data warehouse, designing and performing QA, and providing ad-hoc reporting.

Key Responsibilities:
50% Design and develop robust, user friendly applications, reports and dashboards using BI tools like Microstrategy and SAS
25% Partner with leaders to identify needs and gather requirements. Provide solutions by building tools, reports and predictive models. Automate reporting as needed
25% Research and document best practices and standards for using our BI tools. Provide insight on industry trends

Direct Manager/Direct Reports:
Position reports to Sr Manager or Manager, Online Business Intelligence.
Position has no direct reports.

Travel Requirements:
Typically requires overnight travel less than 10% of the time.

Physical Requirements:
Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.

Working Conditions:
Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.

Minimum Qualifications:
Must be eighteen years of age or older.
Must be legally permitted to work in the United States.

Preferred Qualifications:
Expert in SQL (preferably BigQuery)
Strong knowledge of Python
Strong knowledge of Airflow
Prior experience with data stitching process (the process of combining data sets from different sources and devices to get deep insights into customer behavior and journey - such as stitching data from call centers to customer orders)
Demonstrates strong ability in translating business needs into technical specifications towards building BI solutions
Prior experience in migrating data on a regular cadence from one platform to another
Prior experience in researching and analyzing company data to support statistical analysis by the business
Prior retail industry experience is preferred


Minimum Education:
The knowledge, skills and abilities typically acquired through the completion of a bachelor's degree program or equivalent degree in a field of study related to the job.

Preferred Education:
No additional education

Minimum Years of Work Experience:
3

Preferred Years of Work Experience:
No additional years of experience

Minimum Leadership Experience:
None

Preferred Leadership Experience:
None

Certifications:
None

Competencies:
Strong decision making and problem solving skills
Proficiency in Microsoft Excel and Access
Ability to lead and manage cross functionally
Strong organizational, analytical and customer service skills
Positive, upbeat, can-do, professional and responsible attitude, independent and self-directed yet also team oriented
Influential; practiced in negotiating with others in ways that result in win-win outcomes.
Start your job application: click Apply Now
Show Less
Report",3.8,10000+ Employees,1978,Company - Public,Home Furniture & Housewares Stores,Retail & Wholesale,$10+ billion (USD)
Sr. Data Engineer,$84K - $191K (Employer est.),GG tech global,Remote,-1,3.8,-1,-1,-1,-1,-1,-1
"Software Engineer, Data Products",$119K - $163K (Glassdoor est.),LaunchDarkly4.1 ★,"Oakland, CA","Please note, before progressing to our application, this position is based in the San Francisco Bay Area and not suitable for remote candidates.
About the Job:
We are looking for exceptional Software Engineers to make a profound impact on how data products will be integrated into companies' software in the future. We are integrating data into everything LaunchDarkly offers on top of our unrivaled feature management platform.
As a Data Products - Software Engineer, you will help us architect and write fast, reliable, and scalable data processing tools to process data from our thousands of customers and their hundreds of millions of users around the world. We're looking for someone who knows what it takes to deliver value to customers and takes pride in the quality of their work.
The primary technologies we use daily include Golang, Scala, Kinesis, and Flink. If working as a part of such a poly-functional team to bring to change how experimentation is done forever appeals to you then come join the Experimentation team at LaunchDarkly.
Responsibilities:
Build and expand our data platform and services
Help us identify the best technologies for our evolving data needs
Collaborate with product team to spec and deliver user-facing features
Monitor and improve data pipeline performance
Actively participate in code reviews
Improve engineering standards, tooling, and processes
Qualifications:
Proven experience and fluency in a JVM or functional language
Experience building data platforms (e.g. using Flink, Kafka, DataFlow, Hadoop, Spark)
Strong communication skills, a positive attitude, and empathy
You write code that can be easily understood by others, with an eye towards maintainability
You hold yourself and others to a high bar when working with production systems
You value high code quality, automated testing, and other engineering best practices
Pay:
Target pay range for a Level P3 in San Francisco/Bay Area: $144,000 - $169,000*
Restricted Stock Units (RSUs), health, vision, and dental insurance, and mental health benefits in addition to salary.
LaunchDarkly operates from a place of high trust and transparency; we are happy to state the pay range for our open roles to best align with your needs. Exact compensation may vary based on skills, experience, degree level, and location.
About LaunchDarkly:
LaunchDarkly is a Feature Management Platform that serves trillions of feature flags daily to help software teams build better software, faster. Feature flagging is an industry standard methodology of wrapping a new or risky section of code or infrastructure change with a flag. Each flag can easily be turned off independent of code deployment (aka ""dark launching""). LaunchDarkly has SDKs for all major web and mobile platforms. We are building a diverse team so that we can offer robust products and services. Our team culture is dynamic, friendly, and supportive. Our headquarters are in Oakland.
At LaunchDarkly, we believe in the power of teams. We're building a team that is humble, open, collaborative, respectful and kind. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status.
One of our company values is 'Widen the Circle'. Which means we seek out diversity of perspectives to get better results. We understand everyone has their own unique talents and experiences. We encourage you to apply to this role even if you don't think you meet 100% of the qualifications outlined above. We can find out together if it's the right match for your skillset.
We've partnered with KeyValues to help demonstrate the amazing culture we've built here at LaunchDarkly, find more info at https://www.keyvalues.com/launchdarkly.
LaunchDarkly is also committed to giving back to our community and is a part of Pledge 1%, an organization that helps companies make this a priority. Through this initiative and its charitable arm, the LaunchDarkly Foundation, the company is committed to such causes as supporting education for the underserved, homelessness relief and moving towards having a net-zero carbon footprint. You can find more about the LaunchDarkly Foundation and the organizations we serve at https://launchdarkly.com/foundation/.
Do you need a disability accommodation?
Fill out this accommodations request form and someone from our People Operations team will contact you for assistance.
Show Less
Report",4.1,501 to 1000 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD)
Senior Data Engineer,$159K - $199K (Employer est.),Alto Pharmacy3.2 ★,Remote,"Alto is America’s leading digital pharmacy, transforming a $500 billion industry. Founded in 2015, Alto’s better pharmacy model is centered on the critical role of pharmacists as the final link in a person’s health journey. Alto combines expert pharmacist care with purpose-built technology to deliver a more convenient and affordable experience for those who need medication. To date, Alto has fulfilled more than three million prescriptions, expanded to twelve markets, and built a mobile app experience that makes it easier than ever to manage medications and chat with a pharmacist. As Alto continues its rapid growth, it remains customer obsessed, with an industry-leading NPS score of 86.
About the Role
The Alto Data Engineering team is responsible for several key business areas:
ETL, data visualization, and metadata platforms (fivetran, snowflake, dbt, looker, datahub) that power business analytics and decision making
ML platform (kubeflow) powering the models that automate and optimize pharmacy operations
Data integrations with healthcare networks, drug manufacturers, and other partnerships
Our goal is to make high quality data accessible to everyone at Alto to accelerate decision making and improve our products. We’re looking for an experienced, customer minded engineer with a strong sense of ownership to join our team.
Accelerate Your Career as You
Build a world class self service data platform that makes it easy to quickly answer business questions, trace lineage, and monitor data accuracy and latency
Scale out our machine learning platform and collaborate with our Data Science team to integrate ML/AI applications with our pharmacy operations platform
Identify core problems we can solve for our customer teams and build products that can support and scale data at Alto
A Bit About You
Minimum Qualifications:
7+ years of production data engineering experience
Strong technical skills in python, SQL, and data modeling
Experience with data warehouses, E(LT/TL) tools, and cloud services
Strong sense of ownership over your work
Comfortable working at startup pace and focus
Preferred Qualifications:
Demonstrated experience improving data governance, accuracy, and latency
Familiarity defining and implementing security and data access policies
Experience and opinions on how to best leverage our core technologies
AWS expertise
Additional Physical Job Requirements
Read English, comprehend, and follow simple oral and written instructions. The worker is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading. Assessing the accuracy, neatness and thoroughness of the work assigned.
Communicating with others to exchange information. Expressing or exchanging ideas by means of the spoken word; those activities where detailed or important spoken instructions must be conveyed to other workers accurately, loudly, or quickly.
Perceiving the nature of sounds at normal speaking levels with or without correction, and having the ability to receive detailed information through oral communication, and making fine discriminations in sound.
Frequent repeating motions required to operate a computer that may include the wrists, hands and/or fingers.
Sedentary work: Sitting most of the time, exerting up to 10 pounds of force occasionally and/or a negligible amount of force frequently or constantly to lift, carry, push, pull or otherwise move objects, including the human body. Walking & standing are required occasionally.
Salary and Benefits
Salary Range: $159,000 - $199,000
Commission Eligible: No
Equity Eligible: Yes
Travel: No
Benefits: Full-time: Medical, Dental, Vision, 401(k), Group Life, AD&D, Employer paid STD/LTD, generous PTO and parental leave.
#LI-Remote
Alto Pharmacy is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company.
Show Less
Report",3.2,1001 to 5000 Employees,2015,Company - Private,Health Care Services & Hospitals,Healthcare,$500 million to $1 billion (USD)
"Software Engineer, Data Platform",$120K - $185K (Employer est.),SentiLink5.0 ★,Remote,"SentiLink is building the future of identity verification in the United States. The existing ways to determine if somebody is who they claim to be are too clunky, ineffective, and expensive, but we believe strongly that the future will be 10x better.
We’ve had tremendous traction and are growing extremely quickly. Already our real-time APIs have helped verify hundreds of millions of identities, beginning with financial services. In 2021, we raised a $70M Series B round, led by Craft Ventures to rapidly scale our best in class products. We’ve earned coverage and awards from TechCrunch, CNBC, Bloomberg, Forbes, Business Insider, PYMNTS, American Banker, LendIt, and have recently landed on the Forbes Fintech 50, 2023. Last but not least, we’ve even been a part of history-we were the first company to go live with the eCBSV and testified before the United States House of Representatives.
Role:
As a software engineer on the Data Platform team at SentiLink, you will own the data infrastructure components to support the SentiLink suite of products. You will work with product, engineering, and data science teams across the company to build, enhance and modify the data platform that powers our fraud detection products. You have outstanding programming skills and are proficient in our technology stack and pick up new technologies quickly as we evolve.
Responsibilities:
Build, expand, and optimize data architecture in order to create the most accurate dataset of identities and their relationships
Develop and operate scalable and resilient data stores and distributed data processing infrastructures to meet business requirements
Design, develop, test and support a suite of API-based data products
Enable the data science team by ensuring data availability at scale and efficiency
Partner with product management to drive agile delivery of both existing and new offerings
Ensure data platform and services meet SLA and quality requirements; on call rotation for production issues, along with the rest of engineering
Develop functional subject matter expertise within various areas of identity fraud domain
Requirements:
5+ years of software product development experience
3+ years of development experience in python or golang and related technologies and frameworks
Strong expertise in big data technologies and distributed data processing frameworks such as Spark, Kafka, Hadoop etc
Experience with public cloud platforms such as AWS, Microsoft Azure or GCP
Deep understanding of different database technologies including but not limited to RDBMS (e.g. postgres), NoSQL (Elasticsearch, vector DB), Columnar data stores etc. and experience with writing efficient queries and optimization techniques.
Proven track record of building and delivering enterprise grade, scalable, data-intensive backend services on Kubernetes or similar platforms.
Excellent analytical and problem solving skills, interpersonal skills and a sense of humor (enjoy the journey)
Experience working in a scrum / Agile development environment
Bonus points if you have..
experience working with Spark/EMR
built real time streaming applications
experience with AWS technologies such as EKS, sqs/sns, emr, redshift, s3 etc
Infrastructure-as-code (IAC) such as terraform, CloudFormation etc
prior experience working in a fintech startup
Candidates must be legally authorized to work in the United States and must live in the United States
Salary Range:
$120,000/year - $185,000/year
Perks:
Remote (or hybrid, if specified) work with home office stipend, and regular company-wide in-person events
Lunch, coffee and snacks during the work day are fully reimbursable
Employer paid group health insurance for you and your dependents
. . . and other typical start-up benefits (e.g. 401(k) plan with employer match, flexible paid time off, etc.)
Corporate Values:
Follow Through
Deep Understanding
Whatever It Takes
Do Something Smart
Show Less
Report",5.0,51 to 200 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
Sr. Data Engineer,$120K - $153K (Employer est.),Nava4.5 ★,Remote,"About Nava
Nava is a consultancy and public benefit corporation working to make government services simple, effective, and accessible to all. Since 2013, federal, state, and local government agencies have trusted Nava to build transformative digital services to help people access public benefits. Meeting our mission is an opportunity to restore trust between people and public institutions. We focus on populations that are the least protected because the stakes are higher.

As a client services company, we work with government agencies to improve how people apply for benefits, navigate their health care, and more. We bill for our time, selling our expertise and problem-solving methodology to government clients. Our clients hire us to help improve their products and services so that their users and beneficiaries have a better customer experience.

These end-users—the humans who benefit from our work—are at the core of everything we do. We research beneficiaries’ needs to help our government clients better deliver on their missions, providing everyone at Nava opportunities to do meaningful, impactful work.

Position Summary:
Nava is at the forefront of reimagining how our government serves its people, and we are looking for a Data Engineer who will help us to drive our vision forward. Working in a highly-collaborative environment, you’ll create experiences that improve the lives of millions of Americans. We're looking for a Data Engineer who cares deeply about end users and is passionate about crafting simple and usable solutions.

In this role, you'll work with government stakeholders, alongside Nava's engineering, design and product teams, to modernize data architectures and data pipelines in critical government programs. You'll design and implement data models and databases, improve data pipelines, enhance data security, and write code to process data more efficiently and support storage, processing, and analysis of terabytes of critical data. You will provide subject matter expertise and actively contribute in data and architecture review meetings, while supporting the implementation of data integration requirements and developing the pipeline from raw data through curation layers (including data acquisition, cleaning, transformation, derivation, and aggregation) to consumable data for Data Analysts and Data Scientists.

You will deploy and operate mission-critical, highly-available, and scalable systems by adopting and defining standards and best practices in data engineering. You will support our software developers and Infrastructure Engineers on data initiatives and will help ensure data delivery is optimal and consistent throughout ongoing projects. You will support the data needs of multiple teams, systems, and products in addition to creating and optimizing our government partners' data architecture to support their data initiatives and programs.

You will have a significant strategic impact and will be uniquely positioned and empowered to make a huge impact on Nava's mission to reimagine how our government serves its people. The ideal candidate will be comfortable working directly with clients in both a consulting and delivery capacity to tackle complex, enterprise cloud, or on-premises software and technology projects. You'll have an innate desire to learn new technologies and languages as well as be a problem solver who likes and creative thinker. You're also highly organized and works well independently, as part of a team, and with stakeholders. Our ideal candidate will also be an effective oral and written communicator with a strong ability to relay technical concepts clearly and concisely to a wide audience with various levels of technical knowledge and awareness.
What You’ll Do
Document, improve, and maintain data strategies and artifacts, including logical and physical data models, data dictionary, data architecture roadmap, and data security policies, using industry best practices and adhering to federal standards, along with operational runbook procedures
Collect data access patterns and review current data models to optimize designs for customer use cases
Standardize data ingestion and processing pipelines to scale with increasing utilization
Audit and reverse-engineer business rules in legacy systems, and build data connectors for integrating them into a data and analytics platform
Implement large-scale data ecosystems within cloud-based platforms that include data management and data governance of structured and unstructured data
Leverage and enhance automation to speed development and improve reliability and performance
Work with cross-functional project teams to gather business requirements and translate to detailed technical specifications
Work with Government partners to assist and develop data engineering applications and pipelines that will enable data services and processing capabilities
Design, develop, test, automate, and deploy data engineering solutions in a cloud platforms, such as Azure
Participate in software design and code reviews.
Develop automated testing, monitoring and alerting, and CI/CD for production systems
Maintain security and privacy standards in all aspects of the data pipeline
Required Technical Skills:
7+ years data engineering experience
3+ years of experience in cloud data architecture (AWS preferred) and big data technologies
Experience with professional software engineering practices using such tools and methodologies as Agile Software Development, Test Driven Development, CI/CD, and Source Code Management
Experience with building ETL pipelines in Python
Proficient with relational databases and advanced SQL queries
Prior experience with Java, Python, Scala
Experience with data cleaning and data modeling while protecting sensitive data
Proficient with building data integrations using both API and file-based protocols
Proficient in refining high-level goals into high-impact, low-effort tasks and milestones based on human-centered design practices to prioritize options for stakeholders

Other requirements
Must have lived in the United States for at least three (3) of the past five (5) years.
An active green card or citizenship is required.
Candidates who are offered a job with Nava must possess work authorization that does not require sponsorship by the employer for a visa now or in the future.
Applicants selected for this position may be subject to a government security investigation and must meet eligibility requirements for access to classified information or applicants who are eligible for security clearances.

Perks working with Nava
Your well-being is important to us, so we offer highly competitive benefits for medical, dental, and vision
20 days of PTO accrued, 12 days paid federal holidays, 5 floating holidays, unlimited sick leave
16 weeks of fully paid parental leave and weekly meal deliveries during leave
Sabbatical Leave
401k contributions match at 4% of your salary
Flexible work arrangements
$1,000 new home office set up budget, monthly phone allowance
Monthly partial reimbursement for utilities (where applicable)
$2,000 annual tuition and professional development budget, on top of a LinkedIn Learning license
Equity stock options
Employee referral program
Commuter Benefits
Short and Long-Term Disability Insurance
Life and Accidental Death Insurance
Diverse, inclusive, highly collaborative, and vibrant culture, fostering remote work!

Location
You can work in a hybrid work arrangement from one of Nava’s offices in NYC, DC, or San Francisco. We also have fully remote options if you reside in one of the following states:

Alabama, Arizona, California, Colorado, DC, Florida, Georgia, Illinois, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, North Carolina, New Jersey, New York, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, Texas, Tennessee, Virginia, Washington, Wisconsin.

If you are not living in one of the states listed above, unfortunately, you will not be considered for a position at this time.

Stay in touch
Sign up for our newsletter to find out about career opportunities, new partnerships, and news from the broader civic tech community.

Nava PBC equal opportunity employer that is deeply committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, age, gender, religious or political beliefs, national origin or heritage, marital status, disability, sex, sexual orientation or gender identity, genetic information, pregnancy, status as a protected veteran or any characteristic protected by federal, state, or local laws. Our commitment to diversity, equity, and inclusion not only reflects our values as a public benefit corporation but also enriches our ability to do our work. Learn more about where we are today and hope to be by 2025.

Please contact the recruiting team at recruiting@navapbc.com if you would like to request reasonable accommodation during the application or interviewing process.

We participate in E-Verify. Upon hire, we will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. This role is required to work from the contiguous United States.
Show Less
Report",4.5,1001 to 5000 Employees,1996,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
Senior Data Engineer,$174K - $202K (Employer est.),Salesforce4.1 ★,"Bellevue, WA","To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Data
Job Details
Job Duties : Support ongoing maintenance and iterative design and implementation of Employee Success Strategy & Analytics projects. Support data science and consulting team by designing, developing and maintaining data pipelines for reporting, machine learning or other computational requirements. Working with data science team to deploy/productionize machine learning models on the cloud. Proactively identify and remediate performance & data quality problems, drive architectural and code enhancements to improve execution speed and reliability. Quickly create functioning ETL prototypes to address changing business needs. Collaborate with business partners and recruiters to conduct ad hoc queries and analysis. Design and develop dashboards and visualizations for business partners to help drive smarter business decisions. Design and develop sustainable automations for employee success projects that replace legacy manual tasks. Telecommuting is an option. Some travel to Salesforce offices is required.
Minimum Requirements : Master’s degree (or its foreign degree equivalent) in Computer Science, Applied Statistics, Engineering (any field), or a related quantitative discipline, and two (2) years of experience in the field of software engineering/machine learning or two (2) years of experience in the job offered or Bachelor’s degree (or its foreign degree equivalent) in Computer Science, Applied Statistics, Engineering (any field), or a related quantitative discipline, and five (5) years of progressively responsible experience in the field of software engineering/machine learning or five (5) years of experience in the job offered.
A related technical degree required (Computer Science, Applied Statistics, Engineering (any field)).
Special Skill Requirements : (1) Data warehousing concepts; (2) Relational star-schema database designs; (3) Python (4) Tableau, Power BI, or similar reporting tools; (5) Oracle or similar databases; (6) SQL; (7) Unix shell scripting; (8) Designing and implementing data warehouse applications; (9) Working in an agile environment (10) Data Modeling. Any suitable combination of education, training and/or experience is acceptable. Education, experience and criminal background checks will be conducted. Telecommuting is an option. Some travel to Salesforce offices is required.
Salary: $173,909.00 - $201,700.00 per annum.
Certain roles may be eligible for incentive compensation, equity, and benefits. More details about company benefits can be found at the following link: https://www.salesforcebenefits.com.
Submit a resume using the apply button on this posting or by email at: onlinejobpostings@salesforce.com at Job # 18-6939. Salesforce is an Equal Opportunity & Affirmative Action Employer. Education, experience and criminal background checks will be conducted.
#LI-DNI
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org .
Salesforce welcomes all.
More details about our company benefits can be found at the following link: https://www.getsalesforcebenefits.com/
For Washington-based roles, the base salary hiring range for this position is $173,909 to $201,700.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Apply Now: click Apply Now
Show Less
Report",4.1,10000+ Employees,1999,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
"Senior / Principal Software Engineer, Data Platform",$140K - $190K (Employer est.),NewLimit,"San Francisco, CA",-1,4.1,-1,-1,-1,-1,-1,-1
"Data Center Facilities Engineering, Electrical Engineer",$129K - $196K (Employer est.),Meta3.9 ★,Remote,"Meta is seeking an Electrical Engineer to become part of our Data Center Facilities Engineering team. Our data centers are the foundation upon which our rapidly growing infrastructure efficiently operates and our innovative services are delivered. The Facilities Engineering team delivers proactive and responsive full-life-cycle engineering expertise to the teams who operate and support our expanding global fleet of data centers.Electrical Engineers are involved in all aspects of data center operations, essential to ensuring effective power distribution, sequenced system operations, monitoring feedback and analysis. They provide direct support to our field teams for new construction, site specific projects and global level programmatic efforts.The establishment of new regions in the US, Europe, and Asia are important next steps in the evolution of Meta's infrastructure expansion.


Data Center Facilities Engineering, Electrical Engineer Responsibilities:
Lead technically complex Electrical engineering projects for the facilities infrastructure operations team. This includes working cross functionally with other technical organizations including design teams, construction, field operations, vendors and R&D.
Engineering point of escalation for Electrical system issues that occur in our data centers.
Troubleshoot and engineer enhancements and corrective actions for designs and implementation processes/methods.
Conduct detailed analyses and audits of Electrical system utilization.
Develop effective measurements and reports from Building Management System (BMS) and other monitoring systems to optimize performance and reliability.
Evaluate systems to perform effective and efficient load management and capacity planning. Monitor, trend, and develop engineering metrics and statistical data for strategic planning.
Review new and retrofit Electrical designs, commissioning plans, and critical operational procedures.
Review Electrical ""as-built"" technical documentation and verify accuracy. Investigate actual site conditions and update records.
Develop and generate specifications, RFPs, ROMs and SOWs for projects.
Evaluate facility processes and industry best practices to drive efficiencies and effectiveness.
Routinely report and present to peers, partners and leadership.
10 to 30% travel to data center and vendor sites for engineering studies, on-site technical support, Electrical systems audits, startup testing, and full commissioning.



Minimum Qualifications:
Experience to research, diagnose, troubleshoot and identify solutions to resolve system issues within discipline.
Experience with presentations, project management, and reporting skills.
Experience working independently and as a member of a cross functional team
4+ years professional experience in data center engineering, mission critical design, construction, operations, maintenance or related industry.
BS degree in Electrical Engineering, or related field.
Experience clearly communicating technical issues.
Experience with computer based mathematical analysis and data analysis.
Experience driving complex projects under pressure.
Knowledge of mission critical power distribution
Knowledge of industry standards, building codes and safety standards applicable to the region where the position will be based
Knowledge of short circuit coordination and arc flash studies



Preferred Qualifications:
Organizational skills and experience beyond the field technician level in activities such as project management, project engineering, and operations management.
Electrical Professional Engineer (PE) registration or local equivalent.
Cross-discipline knowledge of multiple critical facility systems.
Experience associated with high voltage (115kV, 12kV) substations, mission critical power distribution and generator power systems
Experience in load management at the operations level within a critical facility
Experience in the conduct and interpretation of short circuit coordination studies
Familiarity with SKM or ETAP or equivalent software





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice here. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. We may use your information to maintain the safety and security of Meta, its employees, and others as required or permitted by law. You may view Meta's Pay Transparency Policy, Equal Employment Opportunity is the Law notice, and Notice to Applicants for Employment and Employees by clicking on their corresponding links. Additionally, Meta participates in the E-Verify program in certain locations, as required by law
Start your job application: click Apply Now
Show Less
Report",3.9,10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
Senior Data Engineer (Remote),$88K - $123K (Glassdoor est.),OPENLANE3.5 ★,"Carmel, IN","Location: Remote-North America (EST preferred)

Who We Are:
At OPENLANE we make wholesale easy so our customers can be more successful.
We’re a technology company building the world’s most advanced—and uncomplicated—digital marketplace for used vehicles.
We’re a data company helping customers buy and sell smarter with clear, actionable insights they can understand and use.
And we’re an innovation company accelerating the future of wholesale remarketing through curiosity, collaboration, and an entrepreneurial spirit.
Our Values:
Driven Waybuilders. We pursue challenges that inspire us to build, create and innovate.
Relentless Curiosity. We seek to understand and improve our customers’ experience.
Smart Risk-Taking. We transform risk into progress through data, experience, and intuition.
Fearless Ownership. We deliver what we promise and learn along the way.
What We Offer:
Competitive pay
Medical, dental, and vision benefits with employer HSA contributions (US) and FSA options (US)
Immediately vested 401K (US) or RRSP (Canada) with company match
Paid Vacation, Personal, and Sick Time
Paid maternity and paternity leave (US)
Employer-paid short-term disability, long-term disability, life insurance, and AD&D
Robust Employee Assistance Program
Employer paid Leap into Service Day to volunteer
Tuition Reimbursement for eligible programs
Opportunities to expand your skill set and share your knowledge across a publicly traded, global organization
Company culture of internal promotions, diverse career paths, and rapid advancement
We’re Looking For:
We are seeking a Senior Data Engineer (Remote) with experience in Python and SQL. You will be part of a Data and BI team responsible for implementing/maintaining ADF ETL jobs to ingest external data sources into the Snowflake Data Warehouse and working closely with the Product and Data Science teams to deliver data in useable formats and to the appropriate data sources. In this role, you will have the opportunity to use your experience in AWS and/or Azure Cloud. The ideal candidate will have 7+ years of experience with Python and SQL.
What You Will Be Doing:
You will participate daily in Agile ceremonies to help design, plan, build, test, develop, and support Openlane data products and platforms consisting of ADF ETL pipelines and Postgres, DynamoDB, Elastic search, and Snowflake databases. Our team works in a shared services delivery model supporting seven lines of business, including front-end customer facing products, B2B portals, mobile applications, business analytics, and data science initiatives.
Responsibilities include:
Work with product, data science, analytics, and engineering teams to learn project data needs and define project scope
Design and planning of data services solutions on the Data Platform
Building and delivery of Python/Docker feed framework data pipeline jobs and services
Contribute to the Data Engineering team delivery framework including building of re-usable code, implementing industry best practices, and maintain a common delivery framework
Monitoring, maintenance, documentation, and incident resolution of scheduled production data jobs supporting internal and external customers data needs
What You Need to Be Successful:
7+ years of experience in SQL and Python development including working with data science packages (required)
Experience planning and designing maintainable data schemas (required)
Experience with Azure Cloud or AWS (required)
Experience using Github / Azure DevOps (CI/CD) (required)
Experience with ADF, Docker, Kubernetes and data warehouse environments (preferred)
Experience with Kinesis/Kafka (preferred)
Experience working with large enterprise data lakes / Snowflake (preferred)
Sound like a match? Apply Now - We can't wait to hear from you!
Show Less
Report",3.5,1001 to 5000 Employees,2006,Company - Public,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
Data Engineer,$70K - $140K (Employer est.),CVS Health3.1 ★,"Irving, TX","Assists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs

Applies understanding of key business drivers to accomplish own work

Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems

Leads portions of initiatives of limited scope, with guidance and direction

Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing

Collaborates with client team to transform data and integrate algorithms and models into automated processes

Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines

Uses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems

Builds data marts and data models to support clients and other internal customers

Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards

Pay Range
The typical pay range for this role is:
Minimum: 70,000
Maximum: 140,000

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company's 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (PTO) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit
jobs.CVSHealth.com/benefits

Required Qualifications
1+ years of progressively complex related experience

Experience with bash shell scripts, UNIX utilities & UNIX Commands

Preferred Qualifications
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources

Ability to understand complex systems and solve challenging analytical problems

Strong problem-solving skills and critical thinking ability

Strong collaboration and communication skills within and across teams

Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar

Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment

Experience building data transformation and processing solutions

Has strong knowledge of large-scale search applications and building high volume data pipelines

Education
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline

Master’s degree or PhD preferred

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.
Start your job application: click Apply Now
Show Less
Report",3.1,10000+ Employees,1963,Company - Public,Health Care Services & Hospitals,Healthcare,$10+ billion (USD)
Azure Data Engineer - All Levels (REMOTE),$72K - $237K (Employer est.),GEICO3.0 ★,"Chevy Chase, MD","GEICO’s AI & Machine Learning Engineering team is seeking Azure Data Engineers to support Machine Learning and Data Science development, deployment, and maintenance efforts. As part of our team, you will be responsible for designing, implementing, automating, and monitoring a variety of data ingest pipelines using a variety of tools such as Azure Data Factory (ADF). The role requires proficiency with or ability to quickly learn about a variety of technologies related to software design and development, as well as Azure services. Experience with Machine Learning techniques is nice to have. You will also participate in the planning and implementation of new projects, including work on both Machine Learning deliverables and the deployment frameworks that launch these deliverables. In this role, the ability to build and maintain mission critical production software in the Azure cloud is a must. As a Senior/Principal Azure Data Engineer, you would also contribute to areas such as Model Deployment Frameworks, Automated Machine Learning, Data Visualization and Machine Learning Interpretability, as well as Data Engineering.


Required Qualifications:
4+ years of development experience
Hands-on experience with cloud platforms (MS Azure, AWS or Google Cloud Services)
Hands-on experience with distributed computing (preferably using Apache Spark or Databricks)
Proficiency with at least one or more programming language such as Scala, Java, C# or Python
Knowledge of software design patterns
Familiarity with source control tools and concepts (e.g. Git)
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience

Desired Qualifications:
Microsoft Certified: Azure Data Engineer or
Microsoft Certified: Azure Developer Associate
Experience with Apache Spark
Experience with Azure Data Factory (ADF)
Experience with Machine Learning algorithms
Experience with Cloud-based deployments
Experience with Microservices Architecture
Familiarity with Azure DevOps (ADO) and Shell scripting
Familiarity with unit testing and integration testing (e.g. JUnit and JMeter)
Benefits:
At GEICO, we make sure you have the support and resources to leverage and develop your skills, secure your financial future, and take care of your health and well-being. GEICO continually seeks to provide a workplace where everyone can be their authentic self. To help achieve this goal, we support associate-led Employee Resource Groups that foster a true sense of community. Through GEICO’s competitive benefits offerings and various training and development opportunities, we have you covered with our
Total Rewards Program
that includes:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Assistance including Direct Billing and Reimbursement payment plan options
Paid Training, Licensures, and Certificates
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins with the pay period after hire date. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
#LI-AP1
Annual Salary
$72,000.00 - $236,500.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
Start your job application: click Apply Now
Show Less
Report",3.0,10000+ Employees,1936,Subsidiary or Business Segment,Insurance Carriers,Insurance,$10+ billion (USD)
Senior Clinical Data Engineer,$138K - $213K (Employer est.),RedSail Technologies4.6 ★,Remote,"Senior Clinical Data Engineer
Job Summary
The RedSail Technologies Network Services Business Unit has the primary mission to create incremental value streams for RedSail through the development and activation of Clinical, Financial, & Operational Programs that leverage our uniquely integrated technology platforms as well as our associated reach within the targeted market segments. The Senior Clinical Data Engineer, will play a critical role as part of the team in building and maintaining the systems and structures that store, extract, and organize data to enable data scientists and team members to access and use data effectively..
Key Duties
Collecting and storing data though manual and automated processes including but not limited to establishing connections to static and dynamic data stores across the enterprise.
Improving efficiency and quality of data infrastructure through a cycle of continuous quality improvement.
Developing and updating technical documentation including data dictionaries.
Organizing and exporting data required by external business associates and participating in development of programs thereof.
Automating data exports for reporting required by contract or as needed for descriptive analytics.
Extracting, organizing, aggregating data for business intelligence reporting systems such as individual pharmacy dashboards.
Organizing and aggregating data for efficient clinical insight of patient cohorts such as creation of patient profiles that support medication adherence assessments.
Organizing and aggregating data in format supportive of machine learning in pursuit of prescriptive and predictive analytics.
Building and maintaining the systems and structures that store, extract, and organize data as other business needs require.
Education/Training
Bachelors Degree in Data Engineering, Data Science, or similar data relevant computer science / software development degree.

Required Work Experience/Skills
Pharma/Pharmacy/Healthcare experience.
Ability to transform complex data across multiple platforms into concise datasets.
Ability to visualize data in the most effective way possible for a given project or study.
Strong analytical and problem-solving skills; inquisitive.
Familiarity with data management tools.
Ability to work independently and with team members from different backgrounds and collaborative styles.
Excellent attention to detail with critical thinking skills.
Preferred Work Experience/Skills
Masters Degree in related field, or similar data related computer science / software development work.
5 years of experience as a Data Engineer
Experience with Pharma/Pharmacy transactions.
Experience as a database administrator.
Experience in database / data warehouse / data lake tools: programming with SQL, Python; data storage management with Apache tools; and analytics with Dremio, PowerBI, etc.
Discretionary Judgment
Uses independent judgment and discretion based upon the employee’s experience in the position and knowledge of the products, equipment, and services.
Uses good judgment and possesses ethical work values.
Physical Demands, Working Conditions, and General Employment Guidelines
Moderate or high stress levels may be experienced in the job performance.
Position is performed in a general office environment, home office, or approved remote workspace where physical work includes, but is not limited to, sitting, standing, reaching, kneeling, bending, and lifting to 25 lbs.
Equipment
Daily use of Microsoft Teams (phone), computer, printer, and other routine office equipment.
Must have reliable and consistent internet access.
Safety to Self and Others
Little responsibility for the safety of others. Job is performed in an office setting where there are no hazardous materials or equipment.
Working Conditions/Hazards
Position is performed in an open office environment or approved remote work location.
Work Location
Remote
Show Less
Report",4.6,501 to 1000 Employees,2020,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
Sr. Azure Data Engineer,$75.00 - $80.00 Per Hour (Employer est.),Kairos Technologies4.6 ★,"Dallas, TX","Hello,
Hope you are doing great!
If you are comfortable with the requirement, kindly respond with below information and your updated resume( word formatted ) ASAP
Full Name:
Current Location:
Contact No Primary:
Work Authorization:
S.S.N No( Last 4 digits ):
D.O.B( MM/DD ):
Availability( Any notice period required to join in project ):
Currently working(Yes/No):
LinkedIn:
Employer details( If on C2C ):
Please check the below requirement
Direct Client Requirement
Position: Sr. Data Engineer with Azure Cloud and Strong SQL( Hybrid/ Onsite for 2 days a week )
Location: Dallas TX or VA
Duration: 12+ Months Long Term Contract
Work Authorization: US Citizen/GC/GC-EAD/TN Visa( on C2C is fine )
Need Locals Candidates..
Job Summary:
We are seeking an experienced and highly skilled Senior Database Engineer with strong hands-on experience in modernizing and automating on-premises SQL databases, as well as a solid track record of successful migration projects from on-premises to Azure cloud. As a Senior Database Engineer, you will play a critical role in driving our database modernization efforts, ensuring scalability, performance, and security in the Azure cloud environment.
Responsibilities:
Lead the modernization and automation of on-premises SQL databases, implementing best practices and efficient processes to enhance performance, scalability, and reliability.
Design and execute successful migration strategies from on-premises databases to Azure cloud, ensuring seamless data transfer and minimal downtime.
Develop and implement automation solutions using tools such as Python, PowerShell, and Azure DevOps (ADO) to streamline database management tasks, including provisioning, backup and recovery, monitoring, and deployment processes.
Collaborate closely with cross-functional teams to gather requirements, understand business needs, and propose effective database solutions that align with the overall technical architecture in the Azure cloud environment.
Utilize tools like Liquibase or similar to manage database schema changes and version control.
Implement effective monitoring and logging solutions using tools like Splunk and Datadog to ensure database performance, availability, and security.
Perform thorough assessments and evaluations of existing on-premises databases, identifying areas for improvement, optimization, and consolidation prior to migration to Azure cloud.
Ensure data integrity, security, and compliance with industry standards and regulations throughout the database modernization and migration process in the Azure cloud environment.
Provide guidance and mentorship to junior database engineers, fostering their technical growth and promoting best practices in database management, automation, and migration in the Azure cloud.
Stay up to date with the latest trends, tools, and technologies in database management, Azure cloud services, and DevOps practices, evaluating their potential impact and relevance to our organization.
Collaborate with vendors and third-party providers to assess and implement new technologies, tools, and services that can enhance our Azure cloud database environment and support our modernization goals.
Qualifications:
Bachelor's degree in computer science, information technology, or a related field.
Minimum of 10 years of hands-on experience in database engineering, with a strong focus on SQL databases and Azure cloud.
Extensive experience in modernizing and automating on-premises SQL databases, including performance optimization, scalability improvements, and process automation.
Proven expertise in successfully migrating on-premises databases to Azure cloud, ensuring data integrity and minimal disruption to operations.
Strong proficiency in SQL scripting, database performance tuning, backup and recovery, and database security practices in the Azure cloud environment.
Experience with database migration tools and technologies, such as Azure Database Migration Service, Azure Data Factory, or similar tools.
Solid understanding of Azure cloud database services, including Azure SQL Database, Azure SQL Managed Instance, and Azure Cosmos DB.
Familiarity with automation and configuration management tools like Python, PowerShell, and Azure DevOps (ADO) for streamlining database management and deployment processes in Azure.
Experience in using tools like Liquibase or similar for managing database schema changes and version control.
Knowledge of monitoring and logging solutions like Splunk and Datadog for database performance and security monitoring in Azure cloud.
Strong analytical and problem-solving skills, with the ability to troubleshoot complex database issues and propose effective solutions in the Azure cloud environment.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.
Tech Stack:
Databases: On-premises SQL databases (e.g., Oracle, MySQL, PostgreSQL)
Cloud Platform: Microsoft Azure (Azure SQL Database, Azure SQL Managed Instance, Azure Cosmos DB)
Scripting and Automation: Python, PowerShell
Version Control: Git
CI/CD: Azure DevOps (ADO)
Database Migration: Azure Database Migration Service, Azure Data Factory
Database Schema Management: Liquibase or similar tools
Monitoring and Logging: Splunk, Datadog
Thanks and Regards
Prasad Mamidela | Kairos Technologies Inc.
Direct Number: 972.777.9484 | Mobile: 201.613.3664
433 E Las Colinas Blvd, # 1240, Irving, TX 75039 USA
http://www.kairostech.com
LinkedIn: https://www.linkedin.com/in/prasadmamidela
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 4 years (Required)
SQL: 6 years (Required)
On-premises: 2 years (Required)
Total IT: 10 years (Required)
Azure DevOps: 2 years (Required)
Work Location: In person
Show Less
Report",4.6,51 to 200 Employees,2003,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
Data Engineer,$110K (Employer est.),Capitol Federal2.9 ★,"Topeka, KS","Job Description:
Pay: up to $110,000 Annually
Job Type: Full Time
The Data Engineer assists in setting overall development roadmap and standards for the Bank and helps evaluate and architect the use of data solutions, using industry best practices. This position works as part of a collaborative team to design, code, and implement data solutions to support internal business requirements or external customers and vendors. An innovative mindset and an ability to translate complex business scenarios into a technical solution is required. This position performs a variety of tasks under general supervision. The position reports directly to an IT manager and requires regular, predictable and timely attendance at work to meet department workload demands.
Paid time off and holiday available on your first day! Benefits available to anyone working 20 hours or more per week!
CapFed® is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Job Type: Full-time
Work Location: In person
Show Less
Report",2.9,501 to 1000 Employees,1893,Company - Public,Banking & Lending,Financial Services,$100 to $500 million (USD)
Azure Data Engineer - All Levels (REMOTE),$72K - $237K (Employer est.),GEICO3.0 ★,"Chevy Chase, MD","GEICO’s AI & Machine Learning Engineering team is seeking Azure Data Engineers to support Machine Learning and Data Science development, deployment, and maintenance efforts. As part of our team, you will be responsible for designing, implementing, automating, and monitoring a variety of data ingest pipelines using a variety of tools such as Azure Data Factory (ADF). The role requires proficiency with or ability to quickly learn about a variety of technologies related to software design and development, as well as Azure services. Experience with Machine Learning techniques is nice to have. You will also participate in the planning and implementation of new projects, including work on both Machine Learning deliverables and the deployment frameworks that launch these deliverables. In this role, the ability to build and maintain mission critical production software in the Azure cloud is a must. As a Senior/Principal Azure Data Engineer, you would also contribute to areas such as Model Deployment Frameworks, Automated Machine Learning, Data Visualization and Machine Learning Interpretability, as well as Data Engineering.


Required Qualifications:
4+ years of development experience
Hands-on experience with cloud platforms (MS Azure, AWS or Google Cloud Services)
Hands-on experience with distributed computing (preferably using Apache Spark or Databricks)
Proficiency with at least one or more programming language such as Scala, Java, C# or Python
Knowledge of software design patterns
Familiarity with source control tools and concepts (e.g. Git)
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience

Desired Qualifications:
Microsoft Certified: Azure Data Engineer or
Microsoft Certified: Azure Developer Associate
Experience with Apache Spark
Experience with Azure Data Factory (ADF)
Experience with Machine Learning algorithms
Experience with Cloud-based deployments
Experience with Microservices Architecture
Familiarity with Azure DevOps (ADO) and Shell scripting
Familiarity with unit testing and integration testing (e.g. JUnit and JMeter)
Benefits:
At GEICO, we make sure you have the support and resources to leverage and develop your skills, secure your financial future, and take care of your health and well-being. GEICO continually seeks to provide a workplace where everyone can be their authentic self. To help achieve this goal, we support associate-led Employee Resource Groups that foster a true sense of community. Through GEICO’s competitive benefits offerings and various training and development opportunities, we have you covered with our
Total Rewards Program
that includes:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Assistance including Direct Billing and Reimbursement payment plan options
Paid Training, Licensures, and Certificates
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins with the pay period after hire date. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop and retain the most talented individuals to join our team.
GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
#LI-AP1
Annual Salary
$72,000.00 - $236,500.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
Start your job application: click Apply Now
Show Less
Report",3.0,10000+ Employees,1936,Subsidiary or Business Segment,Insurance Carriers,Insurance,$10+ billion (USD)
Data Engineer,$109K (Employer est.),The Wendy's Company3.4 ★,"Dublin, OH","Overview:
When our square shaped burgers made their first sizzle on the scene more than 50 years ago, people knew our approach wasn’t like any other. Same goes for the way we support our employees. Our culture of openness, flexibility, and inclusiveness allows everybody to flourish in their own way. If you’re looking for a career where you can be part of the action as we continue to grow our iconic brand – We got you!

As a Data Engineer, you’ll be responsible for leading projects by defining business requirements, executing the changes needed, manage small to medium projects which will require coordinating efforts between business partners and vendors to ensure the projects are delivered on time and of high quality. Support various data tools such as Looker, Snaplogic, CRM applications (Salesforce). This includes day to day support, user management / security and permissions, configuration changes, existing integrations, data management, testing and QA, and deployments. This role would also help in the creation and management of reports and dashboards to support organizational needs.
Responsibilities:
Complete sprint assignments, provide level-of effort estimates to develop end-to-end integrations of Wendy’s data platform with various source and target systems
Manage existing implementations and execute system configuration changes
Participate in requirement gathering, story scoping and project timeline efforts to deliver solutions in a timely manner
Partner with other Developers and Product Owners to understand requirements and provide scalable solutions in alignment with business technology strategy and architecture
Support the key stakeholder for change management and training shepherding new features to production
Keep up to date on industry trends and recommend solutions to meet business needs

Minimum Wage: USD $64,000.00/Yr. Maximum Wage: USD $109,000.00/Yr. Qualifications:
Education: Bachelors Degree, required
Minimum 2 years experience with software implementation and data management
Working knowledge of cloud technology tools (Google Cloud preferred) e.g. Looker, Python, Snaplogic, preferred
Exhibits well-developed spoken and written message skills, ability to effectively communicate complicated technical topics in a manner that is understood by non-technical team members
Demonstrated organizational and multi-tasking skills, within a rapidly changing environment
Self-motivated, with the ability to manage own workload with minimal supervision
Ability to question directions / roadmaps / plans in a manner that is insightful & respectful

Wendy’s was built on the premise, ""Quality is our Recipe®,"" which remains the guidepost of the Wendy's system. Today, Wendy's and its franchisees employ hundreds of thousands of people across more than 7,000 restaurants worldwide with a vision of becoming the world's most thriving and beloved restaurant brand.

#LI-Remote
Show Less
Report",3.4,10000+ Employees,1969,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$1 to $5 billion (USD)
Data Engineer,$120K - $140K (Employer est.),Zelis3.8 ★,"Saint Petersburg, FL","Summary:
· Design, develop and implement scalable batch/real time data pipelines (ETLs) to integrate data from a variety of sources into Data Warehouse and Data Lake
· Design and implement data model changes that align with warehouse dimensional modeling standards.
· Proficient in Data Lake, Data Warehouse Concepts and Dimensional Data Model.
· Responsible for maintenance and support of all database environments, design and develop data pipelines, workflow, ETL solutions on both on-prem and cloud-based environments.
· Design and develop SQL stored procedures, functions, views, and triggers
· Design, code, test, document and troubleshoot deliverables
· Collaborate with others to test and resolve issues with deliverables
· Maintain awareness of and ensure adherence to Zelis standards regarding privacy.
· Create and maintain Design documents, Source to Target mappings, unit test cases, data seeding.
· Ability to perform Data Analysis and Data Quality tests and create audit for the ETLs.
· Perform Continuous Integration and deployment using Azure DevOps and Git
· Experience with Banking and Finance Processes specifically NACHA, BAI2 and Bank 822 Files required.
· This position will support the Payments Advanced Record Keeping Project
Requirements:
· Minimum of 5+ years experience in the following:
o 5+ years data engineering experience to include data analysis
o 2+ years working with an ETL tool (DBT preferred)
o 5+ years programming SQL objects (procedures, triggers, views, functions) in SQL Server. Plus experience optimizing SQL queries a plus
o 2+ years designing and developing Azure/AWS Data Factory Pipelines.
o 2+ years Columnar MPP Cloud data warehouse using Snowflake
· Advanced understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
· Experience designing and implementing Data Warehouse.
· Working Knowledge of Azure/AWS Architecture, Data Lake
Preferred Skills:
· Microsoft BI stack (SSIS/SSRS/SSAS)
· Working knowledge managing data in the Data Lake.
· Business analysis experience to analyze data to write code and drive solutions
· Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence.
· Healthcare and/or Payments experience
Job Type: Full-time
Pay: $120,000.00 - $140,000.00 per year
Schedule:
Monday to Friday
Application Question(s):
Do you have experience designing and developing with Data Factory Pipelines (Azure/AWS)?
How many years of work experience do you have with Snowflake Cloud?
Do you have experience analyzing data to write code and drive solutions?
Experience:
ETL: 2 years (Required)
SQL: 4 years (Required)
Work Location: In person
Show Less
Report",3.8,1001 to 5000 Employees,2016,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
Software Engineer | Data Platform,$153K - $180K (Employer est.),Ramp Financial4.5 ★,"New York, NY","Location
New York, Miami, Remote
Type
Full time
Department
Engineering

About Ramp
Ramp is building the next generation of finance tools—from corporate cards and expense management, to bill payments and accounting integrations—designed to save businesses time and money with every click. Over 12,000 customers cut their expenses by 3.5% per year and close their books 8x faster by switching to the Ramp platform.
Founded in 2019, Ramp powers the fastest-growing corporate card and bill payment software in America and enables billions of dollars of purchases each year. Ramp continues to grow quickly, more than doubling its revenue run rate in the first half of 2022.
Valued at $8.1 billion, Ramp's investors include Founders Fund, Stripe, Citi, Goldman Sachs, Coatue Management, D1 Capital Partners, Redpoint Ventures, General Catalyst, and Thrive Capital, as well as over 100 angel investors who were founders or executives of leading companies. The Ramp team comprises talented leaders from leading financial services and fintech companies—Stripe, Affirm, Goldman Sachs, American Express, Mastercard, Visa, Capital One—as well as technology companies such as Meta, Uber, Netflix, Twitter, Dropbox, and Instacart. Ramp was named Fast Company’s #1 Most Innovative Company in North America in 2023 and #5 on LinkedIn Top Startups 2022.
About the Role
The Data Platform team develops and owns the systems that enable Ramp's reporting and strategic decision-making, as well as integrating machine learning models into our Risk systems and the product. As a member of the Data Platform team, you’ll build and maintain the infrastructure that enables Ramp to realize value from data. You’ll also partner with Ramp’s analytics engineers, data scientists, and other data professionals to build internally and externally facing data products.
Our ideal candidate is excited about building systems for data collection, processing, storage, and retrieval, and is also passionate about making these systems observable, reliable, scalable, and highly automated.
What You’ll Do
Build and integrate the components of Ramp's Analytics Platform and Machine Learning Platform
Build tools that improve the agility and data experience of Ramp's Data Scientists, Analytics Engineers, Engineers, and Operations teams
Build the batch and streaming data pipelines critical to Ramp’s daily operations using Airflow, Snowflake, Materialize, and other data processing technologies
Collaborate with stakeholder teams on building and productionizing analytical products and machine learning models
Build reliable, scalable, maintainable, and cost-efficient systems across the stack
What You Need
Minimum 2 years of experience with workflow orchestrators like Airflow, Dagster, or Prefect
Minimum 2 years of experience building infrastructure on AWS, GCP, or Azure
Knowledge of SQL and experience with Snowflake, Redshift, BigQuery, or similar databases
Intuition around analytics and machine learning
Strong Python programming skills
Track record of building highly-reliable infrastructure for data storage and processing
Nice to Haves
Expertise with AWS
Expertise with the Modern Data Stack - dbt, Looker, Snowflake, and Fivetran
Expertise with building and deploying machine learning systems.
Experience with Terraform and Datadog
Compensation
The annual salary/OTE range for the target level for this role is $153,000-$180,000 + target equity + benefits (including medical, dental, vision, and 401(k)
Ramp Benefits (for U.S. based employees)
100% medical, dental & vision insurance coverage for you
Partially covered for your dependents
One Medical annual membership
401k (including employer match)
Please note only 401k contributions made while employed by Ramp are eligible for an employer match
Flexible PTO
Fertility HRA (up to $5,000 per year)
WFH stipend to support your home office needs
Wellness stipend
Parental Leave
Relocation support
Pet insurance
Show Less
Report",4.5,201 to 500 Employees,2019,Company - Private,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
Data Engineer,$87K - $129K (Glassdoor est.),N J Malin & Associates3.7 ★,"Addison, TX","Data Engineer Duties and Responsibilities
Assemble large, complex sets of data that meet non-functional and functional business requirements
Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues
Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
Skills and Qualifications
Ability to build and optimize data sets, ‘big data' data pipelines and architectures
Ability to understand and build complex data models for Power BI reporting.
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Must Have experience in (in order of importance)
10 + years experience as a Data Engineer or similar role.
SQL Server
T-SQL code
Administrator functions
Data Factory
Python
Power BI
DAX code
M code
Azure
Malin is an Equal Opportunity Employer - M/F/Veteran/Disability/Sexual Orientation/Gender Identity
Show Less
Report",3.7,501 to 1000 Employees,1971,Company - Private,Shipping & Trucking,Transportation & Logistics,$100 to $500 million (USD)
"Lead Engineer, Data Platform",$199K - $200K (Employer est.),Sephora3.6 ★,"San Francisco, CA","Job ID: 226189
Location Name: FSC REMOTE SF/NY/DC -173(USA_0173)
Address: FSC, Remote, CA 94105, United States (US)
Job Type: Full Time
Position Type: Regular
Job Function: Information Technology
Remote Eligible:

Company Overview:
At Sephora we inspire our customers, empower our teams, and help them become the best versions of themselves. We create an environment where people are valued, and differences are celebrated. Every day, our teams across the world bring to life our purpose: to expand the way the world sees beauty by empowering the Extra Ordinary in each of us. We are united by a common goal - to reimagine the future of beauty.

The Opportunity:
Your role at Sephora:
As a Lead Engineer, Data Platforms at Sephora, you will: Streamline the intake of the raw data into the Azure Data Lake. Perform production support and deployment activities. Proactively drive the execution of core data engineering, business intelligence, and data warehouse frameworks. Build data pipelines from systems including CRM and Ecommerce, with the emphasis on scalability and reliability. Leverage central data warehouse with other data sources to create enriched customer information in the CRM system. Analyze and translate business needs into data models to support long-term, scalable, and reliable solutions. Create logical and physical data models using best practices to ensure high data quality and reduced redundancy. Drive data quality across the organization. Develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking. Define and manage SLA’s for data sets and processes running in production. Continuously improve our data infrastructure and stay ahead of technology. Design a system for data backup in case of system failure. Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs. (Position allows some work-from- home flexibility, with schedule to be approved by manager. Must be able to work on site as required.)

We are excited about you if you have:
Bachelor’s or foreign equivalent degree in Computer Science, Engineering, or Information Technology.
Six (6) years of progressively responsible post-baccalaureate experience in the design of data warehouse architecture.

Experience must include:
Scala
Spark
Python
Business intelligence and data warehouse frameworks
Data engineering
Data modeling and pipelines
Data warehouse production support
Data warehouse SLAs and best practices
Programming in SQL
Databricks
IBM Datastage

Salary: $199,098 to $200,000 per year depending on experience
Working at Sephora’s Field Support Center (FSC)
Our North American operations are based in the heart of San Francisco’s Financial District, but you won’t hear us call it a headquarters – it’s the Field Support Center (FSC). At the FSC, we support our stores in providing the best possible experience for every client. Dedicated teams cater to our client’s every need by creating covetable assortments, curated content, compelling storytelling, smart strategy, skillful analysis, expert training, and more. It takes a lot of curious and confident individuals, disrupting the status quo and taking chances. The pace is fast, the fun is furious, and the passion is real. We never rest on our laurels. Our motto? If it’s not broken, fix it.

As a condition of employment, Sephora requires all newly hired employees to be fully vaccinated against COVID-19 by their start date unless they have requested and received an exemption due to a qualifying medical condition, a sincerely held religious belief or practice, or a requirement by law.

Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities.

Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.

The actual base salary offered depends on a variety of factors, which may include, as applicable, the applicant’s qualifications for the position; years of relevant experience; specific and unique skills; level of education attained; certifications or other professional licenses held; other legitimate, non-discriminatory business factors specific to the position; and the geographic location in which the applicant lives and/or from which they will perform the job. Individuals employed in this position may also be eligible to earn bonuses. Sephora offers a generous benefits package to full-time employees, which includes comprehensive health, dental and vision plans; a superior 401(k) plan, various paid time off programs; employee discount/perks; life insurance; disability insurance; flexible spending accounts; and an employee referral bonus program.

While at Sephora, you’ll enjoy…

The people. You will be surrounded by some of the most talented leaders and teams – people you can be proud to work with.
The learning. We invest in training and developing our teams, and you will continue evolving and building your skills through personalized career plans.
The culture. As a leading beauty retailer within the LVMH family, our reach is broad, and our impact is global. It is in our DNA to innovate and, at Sephora, all 40,000 passionate team members across 35 markets and 3,000+ stores, are united by a common goal - to reimagine the future of beauty.

You can unleash your creativity, because we’ve got disruptive spirit. You can learn and evolve, because we empower you to be your best. You can be yourself, because you are what sets us apart. This, is the future of beauty. Reimagine your future, at Sephora.

Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities.

Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.

As a condition of employment, Sephora requires all newly hired employees to be fully vaccinated against COVID-19 by their start date unless they have requested and received an exemption due to a qualifying medical condition, a sincerely held religious belief or practice, or a requirement by law.
Apply Now: click Apply Now
Show Less
Report",3.6,10000+ Employees,1969,Company - Private,Beauty & Personal Accessories Stores,Retail & Wholesale,$1 to $5 billion (USD)
Data Engineer,$150K - $180K (Employer est.),"Amino, Inc.4.4 ★","San Francisco, CA","About Amino - What We Do
At Amino, we are a leading innovator in healthcare, empowering individuals to take charge of their healthcare journey. Our powerful digital tools help navigate and unlock the full potential of health plan options. With Amino's cutting-edge solutions, health plan members can easily understand and make informed decisions about their benefits, considering factors such as available programs, provider network quality, and cost.
We are excited to announce that we have recently secured additional funding to fuel our growth and continue developing market-leading navigation tools. As part of this journey, we are looking for a talented and driven Data Engineer to join us. You will play a vital role in helping millions of people understand and optimize their health plan options.
The Role
As a Data Engineer at Amino, you will be at the heart of our organization. Your primary responsibilities will involve processing, transforming, and organizing large and complex datasets that power our products. We are expanding our data sources and enhancing our current offerings, and we need a Data Engineer to help us build competitive and compliant healthcare guidance products.
Key Priorities and Projects:
Collaborate with Product Managers, Architects, and senior leadership to translate high-level requirements into detailed plans.
Integrate new data sources from vendors and internal systems into our ETL (Extract, Transform, Load) pipelines.
Enhance our editorial tools for creating and maintaining accurate data about health entities.
Modernize our orchestration infrastructure using open-source tools like dbt and Dagster.
Impact you will have:
Revamp and expand critical data pipelines that drive our existing and future products.
Mentor other junior data engineers and share your expertise.
Collaborate closely with data scientists and software engineers in related teams.
Influence the foundational aspects of data modeling and warehousing, benefiting various teams across the company.
Utilize modern data stack technologies to build the foundation for Amino's next-generation data assets.
Technologies you will work with:
Snowflake, Python 3, dbt, Docker, AWS, Looker, Databricks, Spark, Jenkins CI/CD, Airflow, Dagster, Terraform, PostgreSQL, RDS, Elasticsearch, and Kinesis.
Skills and Experience you should possess:
Familiarity with various relational and non-relational databases.
Proficiency in writing clean and well-tested code using Python.
Experience handling and working with large datasets and the associated tools.
Previous exposure to ETL pipelines and familiarity with data modeling and data warehousing best practices (experience with dbt is preferred).
Strong collaboration and feedback skills, with a willingness to seek and incorporate input from others.
Excellent documentation and verbal communication skills, capable of conveying technical concepts to peers and non-technical stakeholders.
Bonus Skills:
Experience handling PHI or PII
Deep understanding of the healthcare domain, particularly with healthcare claims data.
We offer
We're committed to helping you achieve your best work in a supportive, growth-oriented environment. We have seriously big goals, and expectations are high and we'll equip you with the tools and resources you need to be successful.
Expected base salary: $150k to $180k plus standard company benefits and a generous option grant. Amino values transparency and has included the reasonable estimate of the base salary range for this full-time role at any approved US location. Individual pay is determined by a range of factors, including job-related skills, experience, relevant education or training, licensure or certifications, and other business and organizational needs. Amino does not typically hire at or near the top of a salary range.
We offer full-time employees 100% paid employee healthcare premiums; dependent premium coverage depends on the plan.
401(k) and FSA programs
This position, like all roles at the firm, will have a good deal of autonomy. We're a remote-first team and have designed our culture for a balance of synchronous and asynchronous work with people operating from all over the country. To support your remote office, we provide every new Amino with a generous office set-up allowance plus a monthly stipend for internet/phone.
PTO is non accrual and we expect Amino's to take a minimum of 15 days a year.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We know the reputation and track record that the tech industry has, and work hard to be exceptional in this regard.
Our Culture
We are a small team who believes that success is a group activity. You should expect to learn from everyone at Amino, and be excited to share your knowledge. You will play a big part in influencing the shape of the product and be empowered to provide your thoughts and ideas.
We believe in collaboration, respect, and curiosity. We believe in having a growth mindset, and have a passion for solving problems that have never been faced before. Everyone's input is valued, be it about code, data models, business models, or product ideas.
Show Less
Report",4.4,1 to 50 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
Data Engineer Cloud Analytics,-1,SMK Soft Inc,Remote,-1,4.4,-1,-1,-1,-1,-1,-1
Senior Data Engineer,$159K - $199K (Employer est.),Alto Pharmacy3.2 ★,Remote,"Alto is America’s leading digital pharmacy, transforming a $500 billion industry. Founded in 2015, Alto’s better pharmacy model is centered on the critical role of pharmacists as the final link in a person’s health journey. Alto combines expert pharmacist care with purpose-built technology to deliver a more convenient and affordable experience for those who need medication. To date, Alto has fulfilled more than three million prescriptions, expanded to twelve markets, and built a mobile app experience that makes it easier than ever to manage medications and chat with a pharmacist. As Alto continues its rapid growth, it remains customer obsessed, with an industry-leading NPS score of 86.
About the Role
The Alto Data Engineering team is responsible for several key business areas:
ETL, data visualization, and metadata platforms (fivetran, snowflake, dbt, looker, datahub) that power business analytics and decision making
ML platform (kubeflow) powering the models that automate and optimize pharmacy operations
Data integrations with healthcare networks, drug manufacturers, and other partnerships
Our goal is to make high quality data accessible to everyone at Alto to accelerate decision making and improve our products. We’re looking for an experienced, customer minded engineer with a strong sense of ownership to join our team.
Accelerate Your Career as You
Build a world class self service data platform that makes it easy to quickly answer business questions, trace lineage, and monitor data accuracy and latency
Scale out our machine learning platform and collaborate with our Data Science team to integrate ML/AI applications with our pharmacy operations platform
Identify core problems we can solve for our customer teams and build products that can support and scale data at Alto
A Bit About You
Minimum Qualifications:
7+ years of production data engineering experience
Strong technical skills in python, SQL, and data modeling
Experience with data warehouses, E(LT/TL) tools, and cloud services
Strong sense of ownership over your work
Comfortable working at startup pace and focus
Preferred Qualifications:
Demonstrated experience improving data governance, accuracy, and latency
Familiarity defining and implementing security and data access policies
Experience and opinions on how to best leverage our core technologies
AWS expertise
Additional Physical Job Requirements
Read English, comprehend, and follow simple oral and written instructions. The worker is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading. Assessing the accuracy, neatness and thoroughness of the work assigned.
Communicating with others to exchange information. Expressing or exchanging ideas by means of the spoken word; those activities where detailed or important spoken instructions must be conveyed to other workers accurately, loudly, or quickly.
Perceiving the nature of sounds at normal speaking levels with or without correction, and having the ability to receive detailed information through oral communication, and making fine discriminations in sound.
Frequent repeating motions required to operate a computer that may include the wrists, hands and/or fingers.
Sedentary work: Sitting most of the time, exerting up to 10 pounds of force occasionally and/or a negligible amount of force frequently or constantly to lift, carry, push, pull or otherwise move objects, including the human body. Walking & standing are required occasionally.
Salary and Benefits
Salary Range: $159,000 - $199,000
Commission Eligible: No
Equity Eligible: Yes
Travel: No
Benefits: Full-time: Medical, Dental, Vision, 401(k), Group Life, AD&D, Employer paid STD/LTD, generous PTO and parental leave.
#LI-Remote
Alto Pharmacy is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company.
Show Less
Report",3.2,1001 to 5000 Employees,2015,Company - Private,Health Care Services & Hospitals,Healthcare,$500 million to $1 billion (USD)
Data Engineer,$130K - $170K (Employer est.),ASSURANCE3.2 ★,"Seattle, WA","About Assurance
Assurance IQ is a technology company headquartered in Seattle. We were acquired by Prudential (NYSE: PRU) to further the joint mission of improving financial wellness across the world.

Our team of world class software engineers, data scientists, and business professionals work every day to expand our product offerings and the reach of our platform. We simplify the complex world of insurance and financial services into straightforward, valuable solutions to improve people's lives. We start by asking customers a few questions, so our system can learn about their needs; from there, our ground-breaking, proprietary platform takes over and analyzes the thousands of data points that make customers unique. This is how we create custom-tailored plans for each customer; plans built precisely for their needs and budget. Our platform serves as the intersection between customer and seller, technology, and the human touch.

At Assurance, we are innovative, persevering, collaborative, calculated, and authentic, and we're working together to improve the lives of millions!

About the Position
As we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.
To be successful in this role, you must possess the following:
Experience with Python and SQL
Experience in data modelling
Business Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.
Comfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct
Comfort with learning new technologies to help the team explore new solutions to existing problems
Excellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.
Enthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.
Bachelors degree in mathematics, statistics, data science or related field of study.
The following additional experience is desired:
You have a proven ability to drive business results by building the right infrastructure that enables data-based insights.
You are comfortable working with a wide range of stakeholders and functional teams.
The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes.
We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail.
You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity.
You can work independently, with little oversight or guidance.

Note: Assurance is required by multiple state and city laws to include the salary range on position postings when hiring in those specific locals. The salary range for this position will be between $130,000-$170,000 and may be eligible for additional bonus or commission plans + benefits. Eligibility to participate in the bonus or commission plans is subject to the rules governing those programs, whereby an award, if any, depends on various factors including, without limitation, individual and/or organizational performance. In addition, employees are eligible for standard benefits package including paid time off, medical, dental and retirement.
To apply to this job, click Apply Now
Show Less
Report",3.2,1001 to 5000 Employees,2016,Company - Public,Insurance Agencies & Brokerages,Insurance,$100 to $500 million (USD)
Data Analytics Engineer,$57K - $113K (Employer est.),Abbott Laboratories3.8 ★,"Orlando, FL",-1,3.2,-1,-1,-1,-1,-1,-1
Senior Data Engineer,$174K - $202K (Employer est.),Salesforce4.1 ★,"Bellevue, WA","To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Data
Job Details
Job Duties : Support ongoing maintenance and iterative design and implementation of Employee Success Strategy & Analytics projects. Support data science and consulting team by designing, developing and maintaining data pipelines for reporting, machine learning or other computational requirements. Working with data science team to deploy/productionize machine learning models on the cloud. Proactively identify and remediate performance & data quality problems, drive architectural and code enhancements to improve execution speed and reliability. Quickly create functioning ETL prototypes to address changing business needs. Collaborate with business partners and recruiters to conduct ad hoc queries and analysis. Design and develop dashboards and visualizations for business partners to help drive smarter business decisions. Design and develop sustainable automations for employee success projects that replace legacy manual tasks. Telecommuting is an option. Some travel to Salesforce offices is required.
Minimum Requirements : Master’s degree (or its foreign degree equivalent) in Computer Science, Applied Statistics, Engineering (any field), or a related quantitative discipline, and two (2) years of experience in the field of software engineering/machine learning or two (2) years of experience in the job offered or Bachelor’s degree (or its foreign degree equivalent) in Computer Science, Applied Statistics, Engineering (any field), or a related quantitative discipline, and five (5) years of progressively responsible experience in the field of software engineering/machine learning or five (5) years of experience in the job offered.
A related technical degree required (Computer Science, Applied Statistics, Engineering (any field)).
Special Skill Requirements : (1) Data warehousing concepts; (2) Relational star-schema database designs; (3) Python (4) Tableau, Power BI, or similar reporting tools; (5) Oracle or similar databases; (6) SQL; (7) Unix shell scripting; (8) Designing and implementing data warehouse applications; (9) Working in an agile environment (10) Data Modeling. Any suitable combination of education, training and/or experience is acceptable. Education, experience and criminal background checks will be conducted. Telecommuting is an option. Some travel to Salesforce offices is required.
Salary: $173,909.00 - $201,700.00 per annum.
Certain roles may be eligible for incentive compensation, equity, and benefits. More details about company benefits can be found at the following link: https://www.salesforcebenefits.com.
Submit a resume using the apply button on this posting or by email at: onlinejobpostings@salesforce.com at Job # 18-6939. Salesforce is an Equal Opportunity & Affirmative Action Employer. Education, experience and criminal background checks will be conducted.
#LI-DNI
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org .
Salesforce welcomes all.
More details about our company benefits can be found at the following link: https://www.getsalesforcebenefits.com/
For Washington-based roles, the base salary hiring range for this position is $173,909 to $201,700.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Apply Now: click Apply Now
Show Less
Report",4.1,10000+ Employees,1999,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
Principal Data Engineer,$76K - $237K (Employer est.),GEICO3.0 ★,"Chevy Chase, MD","Customer data will be the fuel for GEICO’s growth and transformation through the years ahead. Here’s your chance to get involved and help shape one of our company’s most critical initiatives. The Single View of the Customer (SVOC) Team is seeking talented senior to principal-level data engineers to join our team. You will team up with business analysts, product owners, ADLs, and managers to deliver agile projects and design, develop, deploy, and maintain our codebase. An ideal candidate is intellectually curious, has a solution-oriented mindset, and enjoys learning new tools and techniques. You will have the opportunity to design and execute vital efforts such as re-platforming our data services, delivering real-time streaming capabilities to our business applications, and enhancing GEICO ID - the source of record for customer data across the GEICO journey.
Position Responsibilities
As a Principal Data Engineer, you will:
Focus on a few key areas and provide leadership to the engineering teams.
Own complete solution across its entire life cycle.
Design, build, and optimize scalable data pipelines and ETL processes to support data ingestion, transformation, and storage.
Ensure data quality and integrity by implementing robust data validation and cleansing processes.
Identify and evaluate new technologies and tools in the data engineering space to improve efficiency and drive innovation.
Stay up to date with industry trends and advancements in data engineering and recommend relevant strategies and technologies for adoption.
Influence and build vision with product managers, team members, customers, and other engineering teams to solve complex problems for building enterprise-class business applications.
Lead in design sessions and code reviews to elevate the quality of engineering across the organization.
Mentor more junior team members professionally to help them realize their full potential.
Consistently share best practices and improve processes within and across teams.
Qualifications
Experience in data software development, using data technologies such as Relational and NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, loading and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce
Experience building the architecture and design (architecture, design patterns, reliability, and scaling) of new and current systems
Experience with Azure data stack such as ADF, Eventhub, Databricks in an Azure Kubernetes environment (AKS) is a plus
Experience with NoSQL databases like Cassandra
Strong background in SQL
Experience with Graph data is a plus (Neo4j, Datastax, etc)
Experience working with data warehousing stacks such as Snowflake, Delta Lake, or ADLS
Experience implementing Customer Data Platform or MDM initiatives
Demonstrated history of defining standards and best practices as well as mentoring developers
Ability to work independently as well as function as an integral part of a team, take initiative and contribute in a fast-paced environment
Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication)
Able to balance multiple projects concurrently and manage changes in scope along the way
Strong communication and interpersonal skills to collaborate with vendors, business users, and executives, and the ability to communicate technical solutions in business terms
Azure certification, at the associate level (Solutions Architect or Developer), or specialty (Big Data) is a strong advantage
Experience
6+ years of professional software development experience
3+ years of experience with architecture and design
3+ years of experience with AWS, GCP, Azure, or another cloud service
4+ years of experience in open source frameworks
1+ years of people management experience
Education
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
Annual Salary
$76,000.00 - $236,500.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
Start your job application: click Apply Now
Show Less
Report",3.0,10000+ Employees,1936,Subsidiary or Business Segment,Insurance Carriers,Insurance,$10+ billion (USD)
Data Engineer,$114K - $140K (Employer est.),Crane Aerospace & Electronics3.6 ★,"Lynnwood, WA","Crane Aerospace & Electronics seeks mid-level Data Engineer to join our Lynnwood, WA team. This is a hybrid/remote position.
The Data Engineer will be responsible for supporting and updating existing data solutions as well as creating new data solution using Microsoft Azure BI stack. The Data Engineer will partner with Senior Data Architect and our business teams to identify data analytics opportunities, define functional and non-functional requirements for technology or process solutions, and develop data workflows and processes to achieve a business solution.
The candidate should be knowledgeable of a variety of techniques to provide actionable insights while familiar with manipulating, cleaning, and analyzing large datasets. The candidate must be comfortable with ambiguity and able to converse with Data Architect/Business analyst to better understand data and business problems. They must have familiarity with software development tools and processes. Knowledge of ERP and other business systems and data models is preferred.
Essential Functions:
Maintain/Update existing data platform solutions and data models created using Azure platform/Power BI.
Understand user requirements for how data is used to make decisions. Includes process mapping, optimization, and technology enablement.
Extract data from various data sources; perform exploratory data analysis, cleanse, massage, and aggregate data to develop end to end data and analytics solutions.
Efficiently use relevant Azure services related to data and analytics to build Enterprise Data warehouse/Data Lake
Help support day to day activities of the Modern Analytics team; this may include creating knowledge base articles, updates to platform manuals and documentation, best practices and guidance, and diagnostics of issues that are raised as well as promoting and advocating for the value of advanced modeling techniques.
Participate in demos and user acceptance reviews while working with the technical team on the creation of design documents and artifacts.
Collaborate with others on the team and business teams to deliver new, innovative solutions to Crane Aerospace & Electronics users.
Desire to learn new areas and tools required to enable BI solutions.
Interact with infrastructure teams to coordinate technology deployment.
Must Have Qualifications :
Sound knowledge of Data Warehousing with dimensional modelling
Hands on Knowledge of Azure Data Factory V2/Azure SQL server/ Azure Data Lake Gen2
6-7 years of demonstrated experience delivering multiple data solutions with ETL development - both on-premises and in the cloud (2+ years in Azure preferred)
Hands on Knowledge of Power BI data models, reports, and dashboards
3-4 years of demonstrated experience with report development using tools such as Power BI, QLIK
Data analysis training or experience working with structured and/or unstructured datasets
Knowledge about Azure Services, Computation using Azure Databricks/ Synapse/Python
Strong problem-solving skills with an emphasis on analysis and resolution of complex functional areas and technical issues across teams.
Preferred Qualifications :
· Knowledge of ERP and other business systems and data models
REST APIs, PowerShell, Azure Functions, Logic Apps
Experience with analyzing telemetry data
Experience with machine learning platforms and Tools
Experience with Azure DevOps
Familiarity with custom development, open-source products implementation, systems integration
Min Education: Bachelor or higher degree in computer science, Engineering, Information Systems, or other quantitative fields
Eligibility Requirement: This position may require access to Controlled Data or Information. Where the position requires such access only US persons will be considered. As a US Department of Defense contractor, we are bound by International Traffic in Arms Regulations (ITAR).
Working Conditions :
Standard office environment.
Work requires substantial visual concentration on detail.
Working conditions are normal for a manufacturing environment.
Manufacturing operations may require the use of safety equipment to include but not limited to: eye safety glasses, gowns, masks, hearing protectors, heel/wrist straps and any other required PPE.
May be exposed to unusual environmental conditions such as loud noises, cold temperatures, confined spaces, dust, or fumes.
Ability to carry laptops and other equipment weighing up to 25 lbs.
Ability to travel to various Crane Co locations, if and as required for projects.
Standing: 10% *percentage is approximate and may vary depending on work task
Sitting: 90% *percentage is approximate and may vary depending on work task
Lifting (in pounds): up to 25 pounds
Pushing (in pounds): up to 10 pounds
Mental/Visual: use of computer, calculator, filing cabinets
Workspace (line, cube, etc.,): cubicle/desk
About Crane Aerospace & Electronics:
Crane Aerospace & Electronics supplies critical systems and components to the aerospace and defense markets. You will find Crane Aerospace & Electronics in some of the toughest environments: from engines to landing gear; from satellites to medical implants and from missiles to unmanned aerial systems (UAS).
Crane Aerospace offers a full line of products for sensing both position (proximity) and pressure of mechanical, flight control and air data systems. All incorporate flight proven, highly reliable technology and are in use on many commercial and military aircraft.
We are committed to operational excellence and world-class processes. We employ Lean manufacturing techniques to optimize manufacturing efficiency and accuracy on all product lines. Our products are known for their technical strength, proven reliability and overall value.
Salary range: $114,212 to $140,000. Several factors contribute to actual salary, including experience in a similar role or performing comparable job responsibilities, skills, training, and other qualifications. Compensation packages also include comprehensive benefits, 401K contribution and match, Paid Time Off, paid holidays, tuition reimbursement and more. Some roles may be eligible for participation in performance-based bonus programs. You can see a list of our benefits at https://www.craneae.com/company/careers or visit our website at www.CraneAE.com for more information on our company and great opportunities
In our efforts to maintain a safe and drug-free workplace, Crane Aerospace & Electronics requires that candidates complete a satisfactory background check. FAA sensitive positions require employees to participate in a random drug test pool.
This description has been designed to indicate the general nature and level of work being performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job.
Crane Company is an Equal Opportunity Employer and does not discriminate on the basis of race, color, creed, religion, sex, national origin, marital status, age, sexual orientation, gender identity, disability, pregnancy, medical condition, genetic information, protected veteran status or any other characteristic protected under federal, state, or applicable local law.
To apply to this job, click Apply Now
Show Less
Report",3.6,1001 to 5000 Employees,1855,Subsidiary or Business Segment,Aerospace & Defense,Aerospace & Defense,$500 million to $1 billion (USD)
Principal- Big Data Engineer,$127K - $212K (Employer est.),AT&T3.7 ★,"Dallas, TX","DUTIES: Responsible for interpreting the requirements of various Big Data analytics use cases and scenarios. Drive the design and implementation of specific data models to assist drive better business decision. Develop the necessary enables and data platform in the Big Data Lake Environment. Define data requirements, gather and mine large scale of structured and unstructured data, and validate data by running various data tools in a cloud environment. Support the standardization, customization and ad-hoc data analysis. Develop the mechanisms to ingest, analyze, validate, normalize and clean data. Implement statistical data quality procedures on new data sources, by applying rigorous iterative data analytics. Support Data Scientists in data sourcing and preparation to visualize data and synthesize insights of commercial value. Work with Big Data Policy and Security teams and Legal to create data policy and develop interfaces and retention models which requires synthesizing and anonymizing data. Develop and maintain data engineering best practices and contribute to insights on data analytics and visualization concepts, methods and techniques. Utilize Spark, Hadoop, MapReduce, Airflow, Snowflake, Hive, SQL, Scala, Python, and Linux/Unix.

REQUIREMENTS: Requires a Master’s degree, or foreign equivalent degree, in Computer Science, Computer Engineering, Computer and Information Science or Information Technology and 3 years of experience in the job offered or 3 years of experience in a related occupation utilizing Spark, Hadoop, MapReduce, Airflow, Snowflake, Hive, SQL, Scala, Python, and Linux/Unix.
Our Principal- Big Data Engineers earn between $127,100 - $211,900 yearly. Not to mention all the other amazing rewards that working at AT&T offers.
Joining our team comes with amazing perks and benefits:
Medical/Dental/Vision coverage
401(k) plan
Tuition reimbursement program
Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year
and 9 company-designated holidays)
Paid Parental Leave
Paid Caregiver Leave
Additional sick leave beyond what state and local law require may be available but is
unprotected
Adoption Reimbursement
Disability Benefits (short term and long term)
Life and Accidental Death Insurance
Supplemental benefit programs: critical illness/accident hospital indemnity/group legal
Employee Assistance Programs (EAP)
Extensive employee wellness programs
Employee discounts up to 50% off on eligible AT&T mobility plans and accessories,
AT&T internet (and fiber where available) and AT&T phone
AT&T is an Affirmative Action/Equal Opportunity Employer, and we are committed to hiring a diverse and talented workforce. EOE/AA/M/F/D/V
*np*
To apply to this job, click Apply Now
Show Less
Report",3.7,10000+ Employees,1876,Company - Public,Telecommunications Services,Telecommunications,$10+ billion (USD)
Sr. Data Engineer,$120K - $153K (Employer est.),Nava4.5 ★,Remote,"About Nava
Nava is a consultancy and public benefit corporation working to make government services simple, effective, and accessible to all. Since 2013, federal, state, and local government agencies have trusted Nava to build transformative digital services to help people access public benefits. Meeting our mission is an opportunity to restore trust between people and public institutions. We focus on populations that are the least protected because the stakes are higher.

As a client services company, we work with government agencies to improve how people apply for benefits, navigate their health care, and more. We bill for our time, selling our expertise and problem-solving methodology to government clients. Our clients hire us to help improve their products and services so that their users and beneficiaries have a better customer experience.

These end-users—the humans who benefit from our work—are at the core of everything we do. We research beneficiaries’ needs to help our government clients better deliver on their missions, providing everyone at Nava opportunities to do meaningful, impactful work.

Position Summary:
Nava is at the forefront of reimagining how our government serves its people, and we are looking for a Data Engineer who will help us to drive our vision forward. Working in a highly-collaborative environment, you’ll create experiences that improve the lives of millions of Americans. We're looking for a Data Engineer who cares deeply about end users and is passionate about crafting simple and usable solutions.

In this role, you'll work with government stakeholders, alongside Nava's engineering, design and product teams, to modernize data architectures and data pipelines in critical government programs. You'll design and implement data models and databases, improve data pipelines, enhance data security, and write code to process data more efficiently and support storage, processing, and analysis of terabytes of critical data. You will provide subject matter expertise and actively contribute in data and architecture review meetings, while supporting the implementation of data integration requirements and developing the pipeline from raw data through curation layers (including data acquisition, cleaning, transformation, derivation, and aggregation) to consumable data for Data Analysts and Data Scientists.

You will deploy and operate mission-critical, highly-available, and scalable systems by adopting and defining standards and best practices in data engineering. You will support our software developers and Infrastructure Engineers on data initiatives and will help ensure data delivery is optimal and consistent throughout ongoing projects. You will support the data needs of multiple teams, systems, and products in addition to creating and optimizing our government partners' data architecture to support their data initiatives and programs.

You will have a significant strategic impact and will be uniquely positioned and empowered to make a huge impact on Nava's mission to reimagine how our government serves its people. The ideal candidate will be comfortable working directly with clients in both a consulting and delivery capacity to tackle complex, enterprise cloud, or on-premises software and technology projects. You'll have an innate desire to learn new technologies and languages as well as be a problem solver who likes and creative thinker. You're also highly organized and works well independently, as part of a team, and with stakeholders. Our ideal candidate will also be an effective oral and written communicator with a strong ability to relay technical concepts clearly and concisely to a wide audience with various levels of technical knowledge and awareness.
What You’ll Do
Document, improve, and maintain data strategies and artifacts, including logical and physical data models, data dictionary, data architecture roadmap, and data security policies, using industry best practices and adhering to federal standards, along with operational runbook procedures
Collect data access patterns and review current data models to optimize designs for customer use cases
Standardize data ingestion and processing pipelines to scale with increasing utilization
Audit and reverse-engineer business rules in legacy systems, and build data connectors for integrating them into a data and analytics platform
Implement large-scale data ecosystems within cloud-based platforms that include data management and data governance of structured and unstructured data
Leverage and enhance automation to speed development and improve reliability and performance
Work with cross-functional project teams to gather business requirements and translate to detailed technical specifications
Work with Government partners to assist and develop data engineering applications and pipelines that will enable data services and processing capabilities
Design, develop, test, automate, and deploy data engineering solutions in a cloud platforms, such as Azure
Participate in software design and code reviews.
Develop automated testing, monitoring and alerting, and CI/CD for production systems
Maintain security and privacy standards in all aspects of the data pipeline
Required Technical Skills:
7+ years data engineering experience
3+ years of experience in cloud data architecture (AWS preferred) and big data technologies
Experience with professional software engineering practices using such tools and methodologies as Agile Software Development, Test Driven Development, CI/CD, and Source Code Management
Experience with building ETL pipelines in Python
Proficient with relational databases and advanced SQL queries
Prior experience with Java, Python, Scala
Experience with data cleaning and data modeling while protecting sensitive data
Proficient with building data integrations using both API and file-based protocols
Proficient in refining high-level goals into high-impact, low-effort tasks and milestones based on human-centered design practices to prioritize options for stakeholders

Other requirements
Must have lived in the United States for at least three (3) of the past five (5) years.
An active green card or citizenship is required.
Candidates who are offered a job with Nava must possess work authorization that does not require sponsorship by the employer for a visa now or in the future.
Applicants selected for this position may be subject to a government security investigation and must meet eligibility requirements for access to classified information or applicants who are eligible for security clearances.

Perks working with Nava
Your well-being is important to us, so we offer highly competitive benefits for medical, dental, and vision
20 days of PTO accrued, 12 days paid federal holidays, 5 floating holidays, unlimited sick leave
16 weeks of fully paid parental leave and weekly meal deliveries during leave
Sabbatical Leave
401k contributions match at 4% of your salary
Flexible work arrangements
$1,000 new home office set up budget, monthly phone allowance
Monthly partial reimbursement for utilities (where applicable)
$2,000 annual tuition and professional development budget, on top of a LinkedIn Learning license
Equity stock options
Employee referral program
Commuter Benefits
Short and Long-Term Disability Insurance
Life and Accidental Death Insurance
Diverse, inclusive, highly collaborative, and vibrant culture, fostering remote work!

Location
You can work in a hybrid work arrangement from one of Nava’s offices in NYC, DC, or San Francisco. We also have fully remote options if you reside in one of the following states:

Alabama, Arizona, California, Colorado, DC, Florida, Georgia, Illinois, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, North Carolina, New Jersey, New York, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, Texas, Tennessee, Virginia, Washington, Wisconsin.

If you are not living in one of the states listed above, unfortunately, you will not be considered for a position at this time.

Stay in touch
Sign up for our newsletter to find out about career opportunities, new partnerships, and news from the broader civic tech community.

Nava PBC equal opportunity employer that is deeply committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, age, gender, religious or political beliefs, national origin or heritage, marital status, disability, sex, sexual orientation or gender identity, genetic information, pregnancy, status as a protected veteran or any characteristic protected by federal, state, or local laws. Our commitment to diversity, equity, and inclusion not only reflects our values as a public benefit corporation but also enriches our ability to do our work. Learn more about where we are today and hope to be by 2025.

Please contact the recruiting team at recruiting@navapbc.com if you would like to request reasonable accommodation during the application or interviewing process.

We participate in E-Verify. Upon hire, we will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. This role is required to work from the contiguous United States.
Show Less
Report",4.5,1001 to 5000 Employees,1996,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
Senior Data Engineer,$153K - $210K (Employer est.),Snackpass3.3 ★,"San Francisco, CA","Who we are ✨
Snackpass's mission is to unify the physical and digital world for local commerce.
We power mobile order pickup and social commerce for restaurants, modernizing the customer experience while making restaurant operators successful.
Opportunity ✨
Snackpass is one of the fastest growing marketplaces (a16z top marketplaces), and a top 100 YC company. We are backed by Andreeson Horowitz, Y Combinator, General Catalyst, First Round Capital, Craft Ventures and many others. We are hiring people who are humble and hungry to join us in any of our hubs (NYC, SF, LA) or remotely.
Our vision is to be the dominant platform for pickup, a $750B market globally.
About the Role
We are seeking a talented and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in building and maintaining data pipelines, developing and managing dashboards, and contributing to tool development. Your expertise will enable us to efficiently process and analyze large volumes of data, providing valuable insights to drive business decisions.
What we're looking for
4+ years of proven experience as a Data Engineer, with a focus on building robust data pipelines and maintaining data infrastructure.
Strong programming skills in languages like Python, Scala or Javascript, with a solid understanding of software engineering principles.
Hands-on Experience building or maintaining data pipelines with a modern orchestration tools like Airflorw/Prefect/Dagster
Strong expertise with SQL and non-SQL DBs, cloud-based data platforms (e.g., AWS, GCP) and their related services (S3, BigQuery, etc.).
Expert level writing of SQL for data manipulation, transformation and for analytics.
Experience building and maintaining interactive and intuitive dashboards using tools like Looker.
Comfortable with tool development, including building and enhancing internal data tools and frameworks.
Excellent problem-solving and analytical skills, with a strong attention to detail.
Effective communication skills, with the ability to collaborate with cross-functional teams and present complex ideas to both technical and non-technical stakeholders.
What you'll be working on
Build scalable and efficient data pipelines to collect, process, and transform large volumes of data from diverse sources.
Develop and maintain data models, ensuring data integrity and optimizing query performance.
Collaborate with stakeholders to understand their data requirements and provide the necessary infrastructure and support.
Build and maintain interactive dashboards and visualizations to enable data-driven decision-making across the organization.
Contribute to the development of internal data tools and frameworks, automating repetitive tasks and improving data accessibility and usability.
Stay up to date with the latest advancements in data engineering technologies and best practices, and proactively recommend improvements to our data infrastructure.
Mentor and provide guidance to junior data engineers, fostering a culture of learning and growth.
Cash Compensation for this role: $153,000 - $210,000
Please note: Final offer amounts are determined by multiple factors, including prior experience, expertise, and leveling. The final offer amount may vary from the amount above. Please note that this range does not represent additional compensation benefits (such as equity, 401K or medical, dental, or vision insurance).

What You Will Get From Us:
You will receive competitive compensation, a generous equity grant in a high-growth start-up, and benefits like healthcare, medical & dental coverage, unlimited PTO, a home office budget, wellness budget and more.
Importantly, you will also receive an unparalleled amount of ownership over the work you do here. We are a small team, so the opportunity to make a large impact, work on a broad spectrum of challenges and grow your personal skill-set awaits you here.
Finally, you will get a diverse and inclusive work environment where you will be surrounded by hungry and humble colleagues.
Snackpass is an equal opportunity employer and we value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. In fact, we are confident that the most inclusive and diverse teams accomplish the most extraordinary results.
Show Less
Report",3.3,1 to 50 Employees,-1,Unknown,-1,-1,$5 to $25 million (USD)
Senior Data Engineer,$150K - $190K (Employer est.),Curology2.6 ★,Remote,"As a team, Data Engineering enables teams throughout the company to make decisions with data they feel confident in. Data underpins virtually everything we do at Curology—from a truly individualized patient experience, to efficient business operations, to cutting edge Marketing workflows. Data Engineering is the foundation a data driven company is built on. For this role, we believe in the following:

Data as a product. We believe the true potential of Foundation teams lies in a product-oriented mindset and that this is even more relevant to data.

Exceptional impact. Data Engineers are force-multipliers that enable others to work better and faster. Data is deeply integrated into what we do, this role and team are key to our continued success.

Modern data stack. We use the best tools for the job and you will be part of growing and cultivating our modern data stack. AWS, Snowflake, our S3 Data Lake, we build for the future.

A talented and passionate team. Our small team of data engineers has achieved outsize results by maintaining a high bar for ownership and product quality.
In this role, you will:
Design, manage and optimize the flow of data throughout the organization.
Utilize a modern stack to build a cloud-first product.
Work closely with application and site reliability engineers to ensure data quality, integrity and availability.
Work with the team to integrate consistent and high coding standards.
Automate manual processes by working closely with teams like Marketing, BizOps, and Product to discover opportunities for programmatic efficiency.
Work on initiatives to keep our system elegant and productivity high — such as improving our metrics, analytics, and experimentation infrastructure.
Keep Privacy and Data Protection (PDP) the first-order consideration of data.
Respond quickly to data strike team requests.
You will be successful if you have:
At least 5 years experience building software with Python.
Write idiomatic python.
Write object oriented and reusable code.
Familiar with industry standard coding and documentation best practices.
At least 6 years experience modeling SQL and noSQL data.
Understand the tradeoffs between different database models.
Given an analysis problem, suggest data models and key reasons for choices.
Know major data warehousing tools and concepts.
Used Airflow or other job schedulers to develop and monitor batch data pipelines.
Strong expertise in data architecture for efficiency of storage and retrieval of data, especially in modern usage-based systems.
Experience with AWS data services.
Worked with modern database systems, especially Snowflake.
Worked with permissions and regulated or controlled data (HIPAA/GDPR/CCPA/FDA).
Experience with the following technologies: Python, MySQL, Snowflake, Airflow, Terraform, AWS
Passionate about getting the right data to the right person.
High emotional intelligence and a kind demeanor.
Willing to lead in areas of strength and learn new skills when needed.
Has earned the trust and respect of other members of the Data Engineering Team.
Engages in significant thought leadership within the Data Team.
Reaches out to other departments and areas of the company to build relationships.
Anticipates problems before they arise and devises solutions to those problems.
Takes a leading role in documenting post-mortems after problems arise.
Is able to fully architect, build and deploy a solution with no supervision.
Handles Strike Team tickets quickly and efficiently.
Is an expert in all of the technologies that Data Engineering works with on a weekly basis.
Understands advanced Snowflake capabilities, as well as when and how to use them.

Additional Optional Skills
Worked with Apache Spark or other Big Data tools.
Built systems at scale with AWS Lambda or a similar serverless technology.
Worked closely with Marketing teams and having a deep understanding of paid channel automation.
You will love working at Curology because:
Competitive salary and equity packages
Comprehensive benefits: medical, dental, and vision insurance for employees; flexible spending account; 401k; mental health & wellness programs
$75 WFH stipend (remote employees)
Home office setup stipend (remote employees)
Minimum Time Off policy (unlimited PTO, with at least 3 weeks off) for exempt employees
11 company observed holidays
Additional holidays: Curology days off (1 per quarter), 1 annual floating holiday (employee’s choice), and Gratitude Week (employees take the full week of Thanksgiving off; business critical teams observe different days)
Paid parental leave
pre-tax commuter benefits
Employee donation matching program
Company-sponsored events
Free subscription to Curology or Agency (for you and another VIP of your choice!)

The base salary for this position will be between $150,000 to $190,000 depending on your experience, skillset, and geographic location.
#LI-EH1
#LI-Remote
Start your job application: click Apply Now
Show Less
Report",2.6,201 to 500 Employees,2014,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
"Software Engineer, Data Platform",$115K - $175K (Glassdoor est.),Notion4.8 ★,"San Francisco, CA","About Us:
We're on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us can't build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft.
We've been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, we're growing fast and excited for new teammates to join us who are the best at what they do. We're passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide.
About The Role:
You'll join a team of talented engineers who will design and own foundational data products that are key to the company's business and product. Notion's data platform and infrastructure are vital to both empowering every team at Notion to make decisions using data, and are increasingly used in our product features, like search, user notifications, workspace analytics and Notion AI.
What You'll Achieve:
You'll work cross-functionally with partners from the Data Science, Data Engineering, AI, Product, Go-to-Market, Legal and Finance organizations to deliver short- and long-term impact
You'll help in executing the roadmap for data infrastructure and systems to power high volume product features using Notion's data.
You'll play a pivotal role in the development of tools and infrastructure that democratize data access and enable analytics capabilities across the organization
You'll determine the best ways to handle Notion's unique data model and usage patterns to derive insights and bring intelligence to product features like search and discovery.
Skills You'll Need to Bring:
You have worked cross-functionally to establish the right overarching data architecture for a company's needs, to build data ingestion (real-time & batch), and to provide guidance on best data practices for the business.
You have worked on data or infrastructure-focused engineering teams, particularly ones that own a wide swath of software platforms (hosted or built in-house).
You've experienced the challenges of scaling and re-architecting data platforms and infrastructure through orders of magnitude of growth and scaling data volume.
You have a deep background with big data compute, storage, and best practices that you can ask the right questions of your team, balance technology and people concerns, and make hard tradeoffs.
Nice to Haves:
You've built out data infrastructure from, or nearly from, scratch at a fast-growing startup.
You've led or managed a Data Engineering / Platform / Infrastructure Team.
You have experience building MLOps and ML serving infrastructure.
Our customers come from all walks of life and so do we. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. If you share our values and our enthusiasm for small businesses, you will find a home at Notion.
Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation made due to a disability, please let your recruiter know.
#LI-Onsite
Show Less
Report",4.8,201 to 500 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
Principal data engineer,$118K - $171K (Glassdoor est.),bp4.1 ★,"Houston, TX","Location
United States of America - Texas - Houston
Travel required
No travel is expected with this role
Job category
Digital & technology
Relocation available
This role is not eligible for relocation
Job type
Professionals
Job code
RQ063058
Experience level
Senior

Job summary
Entity:
Innovation & Engineering

Job Family Group:
IT&S Group

Job Summary:
As a Principal Data Engineer, you will be part of bp’s Compute Platforms organisation, the group responsible for the computing platforms and services that underpin all bp’s computing services. The portfolio covers technologies that include on-premise data centres, cloud infrastructure and services, high-performance computing, databases, and supporting services.

Job Description:
Key Accountabilities:
Led a team of DevOps squads made up of software engineers, platform engineers, platform architects, and platform security engineers using agile development ceremonies including Kanban, refinement, and retrospectives, to deliver the Azure platform and resource provisioning services necessary for service and platform owners to deliver their agendas.
Build the platform strategy and product roadmap for Azure by working with partners internal and external to the team.
Develop a cloud alignment strategy for both AWS and Azure to provide a consistent customer experience.
Build relationships with external teams, including Digital Security and Architecture, to provide a balance of security and functionality in the Azure platform.
Provide a principle (architecture, design, and security) led service to allow the DevOps teams to be mostly autonomous squads to help deliver solutions rapidly.
Provide governance to enable prioritization of customer demand to ensure new and enhanced services are available for customer projects.
Provide guest platforms (DBaaS, CaaS, CPIN, WVD) the ability to deliver their services while minimizing their dependency on the Azure platform team.
Identify areas of continuous improvement to reduce the effort customers must put in to consume the Azure services (reduce customer friction).
Manage suppliers providing services for the Azure platform to ensure they provide people that are able to develop products/features following out team standards and principles.
Develop training programs with Microsoft to upskill bp staff and contractors as related to Azure capabilities.
Mentor and coach team members, while defining and promoting usage of standards, principles, and lessons learned
Essential Experience:
Developing and leading digital/technology strategies
Broad and strategic knowledge of the cloud technology landscape
Demonstrate strong product management and design thinking foresight skills
Strong facilitation and leadership skills: bringing multiple partners from architecture, digital security, and product/service owners together towards agreed outcomes.
Outstanding communication and relationship skills, ability to engage with a broad range of partners, capable of leading by influence.
Knowledge and understanding of modern development methodologies (agile using Kanban, DevOps, 2-pizza teams, agile ceremonies).
Knowledge and understanding of modern approaches to source-code management and control through tools (SonarCube, Azure DevOps, coding standards)
Bachelor's degree and/or MBA in relevant subject or equivalent preferred or equivalent experience.
Why join us

At bp, we support our people to learn and grow in a diverse and exciting environment. We believe that our team is strengthened by diversity. We are committed to fostering an inclusive environment in which everyone is respected and treated fairly.

There are many aspects of our employees’ lives that are important, so we offer benefits to enable your work to fit with your life. These benefits can include flexible working options, a generous paid parental leave policy, and excellent retirement benefits, among others!

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.

Travel Requirement
No travel is expected with this role

Relocation Assistance:
This role is not eligible for relocation

Remote Type:
This position is a hybrid of office/remote working

Skills:

Legal Disclaimer:
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, neurodiversity/neurocognitive functioning, veteran status or disability status. Individuals with disabilities may request a reasonable accommodation related to bp’s recruiting process (e.g., accessing the job application, completing required assessments, participating in telephone screenings or interviews, etc.). If you would like to request an accommodation related to the recruitment process, please contact us to request accommodations.
If you are selected for a position and depending upon your role, your employment may be contingent upon adherence to local policy. This may include pre-placement drug screening, medical review of physical fitness for the role, and background checks.
Start your job application: click Apply Now
Show Less
Report",4.1,10000+ Employees,1908,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
Big Data Platform Engineer,-1,Apple4.2 ★,"Cupertino, CA","Summary
Posted: Nov 20, 2022
Weekly Hours: 40
Role Number:200444140
The SWE (Software) Data Analytics team at Apple collects, processes, and analyzes diagnostics and usage data from Apple devices across the world. We leverage streaming and batch analytics solutions to generate data that advises and drives product strategies across all of Apple software and hardware development. We discuss, analyze, and implement ground breaking solutions to problems of scale and distributed computing and are looking to expand our team with an engineer passionate about the big data workspace! Kafka, Flume, Hadoop, Spark, and other innovative technologies are core to our large scale infrastructure. You will be collaborating with data analysts, device engineers, and diverse engineering teams and drive the development of data pipelines and services with a high degree of ownership.
Key Qualifications
Experience developing large scale distributed computing systems.
In-depth knowledge and experience in one or more large scale distributed technologies including but not limited to: Hadoop ecosystem, Kafka, Samza, Flink, Storm, Flume, HBase, Cassandra, Redshift, Vertica, Spark.
Passion for and understanding of key algorithms and tools for developing high efficiency data processing systems.
Proficient in working with Linux or other POSIX operating systems, shell scripting, and networking technologies.
Problem-solving and debugging skills with experience in one or more of the following languages: Java, Python, Scala, Go, or Ruby.
There is a lot of communication involved! Excellent interpersonal skills are highly valued.
Description
As part of a team of highly skilled data engineers you will own significant responsibility in crafting, developing and maintaining our large-scale ETL pipelines, storage, and processing services. You will build self-service analytics tools to help engineering teams derive concrete metrics out of large volumes of raw data. You will partner with data science and engineering teams and develop algorithms to answer sophisticated questions on usage of Apple products. You will work closely with the DevOps team and develop monitoring and alerting scripts on various data pipelines and jobs. You will have the opportunity to learn and work on the latest Big Data technologies, lead PoCs to exercise new insights and, influence the strategic direction of our technology stack.
Education & Experience
Bachelors in Computer Science or equivalent experience
Additional Requirements
Experience using data storage technologies such as Apache Parquet or Avro Experience in machine learning algorithms is a plus.
Testing tools and methodologies to test large scale distributed computing systems.
Experience in data modeling and developing SQL database solutions is a plus.
Validated software engineering experience and field in design, test, source code management, and CI/CD practices.
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.
To apply to this job, click Apply Now
Show Less
Report",4.2,10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
Data Analytics Engineer,$87K - $115K (Glassdoor est.),Peyton Resource Group (PRG)4.2 ★,"Englewood, CO","Englewood/Remote, CO
Posted: 07/26/2023
Job Number: 20863
Job Description

About the Role

We are looking for a team member with excellent programming, data engineering, and analytical skills for the position of Senior Data Analyst, Data Science. To be successful in this role you will:
Design scalable data science programs to help capture, organize, and analyze data
Write clean, testable, and modularized code
Build automatic data science workflows and analysis of data science life cycle (including model deployment and monitoring)
Aggregate, clean, and study data while identifying trends and insights to solve significant near- and long-term business problems
Clean data and automate basic data characterization processes and anomaly detection algorithms
Identify opportunities to enhance the flow and analysis of multiple different data sets into and across the company
Transparently communicate priorities, obstacles, and progress on a regular basis to the Product Analytics Lead
Comfortably lead technical data science operations while following agile principles




Who you are:
Degree in a quantitative field such as data science, data analytics, or statistics; with applied experience in technical data science implementation (programming)
Comfortable in data validation and quality assurance methods, including validating and verifying GIS-type data
Logic-driven critical thinker hungry for precision and increasing understanding and confidence in data
Proficient in common data science programming languages such as Python or R and proficient with data management systems
Passionate about environmental sustainability with domain knowledge in agricultural practices and industrial processes
Experienced in building trusted business relationships
Strong written and verbal communicator

Education:
Bachelor's (Required) in a quantitative field such as data science, computer science, physics, engineering, applied mathematics, or statistics.
MS Analytics or similar degree (Preferred)

Experience:
Data Engineering Knowledge (SQL or similar): 5+ years (Required)
Data analytics/science: 5+ years (Required)
Programming Knowledge (R or Python): 5+ years (Required)
Data science program design: 3+ years (Preferred)
Show Less
Report",4.2,1 to 50 Employees,2001,Company - Private,HR Consulting,Human Resources & Staffing,$5 to $25 million (USD)
Data Engineer,-1,Nordic3.0 ★,United States,"Make a difference. Be happy. Grow your career.
Data Engineer
Nordic is looking for a Data Engineer to join our growing Data and Analytics team. We are looking for candidates with 3+ years of experience managing data pipelines ideally in a Healthcare environment.
The ideal candidate will have the following experience:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases
Experience with Continuous Integration/Continuous Deployment
Experience with data pipeline and workflow management tools
Experience with AWS or Azure Cloud Services
Experience with scripting languages: Python, Java, C++, Scala, etc.
Show Less
Report",3.0,1001 to 5000 Employees,2010,Company - Private,Health Care Services & Hospitals,Healthcare,$100 to $500 million (USD)
Principal Data Engineer,-1,Inclusively5.0 ★,Remote,"Inclusively is partnering with a leading insurance software company to hire a Principal Data Engineer.
ABOUT INCLUSIVELY:
Inclusively is a digital tech platform that connects candidates with disabilities, who may benefit from workplace accommodations, to inclusive employers. This includes all disabilities under the ADA, including mental health conditions (e.g. anxiety, depression, PTSD), chronic illnesses (e.g. diabetes, Long COVID), and neurodivergence (e.g. autism, ADHD). Applicants with one or more of these conditions are encouraged to apply; Inclusively does not require applicants to disclose their specific disability.
What You Will Do
Build and maintain enterprise-level cloud-native systems.
Design and implement data models that support large-scale data processing.
Lead the development of data processing pipelines using Apache Spark or similar big data frameworks.
Develop and maintain data infrastructure and tools to support the analytics team.
Manage and optimize database and data storage solutions.
Build and maintain data integrations with third-party systems.
Collaborate with cross-functional teams to define data architecture and infrastructure.
Define best practices and standards for data engineering.
Manage and mentor a team of data engineers.
Continuously research and evaluate new technologies and techniques for data engineering.
What You Will Need to Be Successful
Remote Opportunity: Ability to work 100% remotely, or from an office
Your experience should include some or all of the following:8+ years of experience in data engineering and architecture.
Strong programming skills in languages such as Python, Java, Scala, or SQL.
Extensive experience with big data technologies such as Hadoop, Spark, Kafka, or Flink.
Experience with cloud-native data platforms such as AWS, Google Cloud Platform, or Microsoft Azure.
Deep understanding of data modeling, ETL processes, and data warehousing concepts.
Knowledge of distributed computing principles and experience with parallel processing of large data sets.
Familiarity with containerization and orchestration tools such as Docker and Kubernetes.
What You’ll Gain
Benefits from Day One
Health insurance plans, dental, and vision
Wellness incentives
401(k) and/or RRSP retirement savings plans with employer match
Work-Life Balance
Competitive paid vacation time and a free day for your birthday
Personal/sick time
Paid holidays
Flex Time
Paid parental leave (U.S. candidates)
Volunteer time off
Empowering Career Growth and Success – We invest in talent, care about our people and are empowered by the results of our work. We grow our teams from within and give our employees opportunities to advance.
Job Type: Full-time
Schedule:
Monday to Friday
Work Location: Remote
Show Less
Report",5.0,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
"Senior / Principal Software Engineer, Data Platform",$140K - $190K (Employer est.),NewLimit,"San Francisco, CA",-1,5.0,-1,-1,-1,-1,-1,-1
Data Engineer,$76K - $106K (Glassdoor est.),Greystar Real Estate Partners LLC3.8 ★,"Irving, TX","At Greystar, we've launched a program aimed at bringing the real estate leasing experience for residents into the digital era.

As a data engineer, you will join the global Enterprise Data Organization and Apex Organization, which builds a resident-centric ecosystem of products that enable a 360 view of the prospect/resident to improve operational efficiency and resident satisfaction.

You will provide data capabilities and build out a common data model that supports 360 view of our prospect/resident powered by Azure SQL, Synapse data warehouse, and Microsoft Customer Insights. You will also work and support our industry-changing products and features designed to make shopping for an apartment more streamlined, e-commerce friendly, and efficient.

With your help, we will improve our customer’s apartment shopping journey and enable business intelligence to help us personalize the apartment shopping experience for our residents.

The successful candidate will have a strong sense of teamwork, personal integrity, accountability, and the ability to understand business functions and requirements, translating to innovative working applications while navigating competing priority tradeoffs.

JOB DESCRIPTION
What You Will do
100% hands-on development – develop and unit test database code, including but not limited to T-SQL, stored procedures, functions and views.
Create and maintain database structures
As part of the Scrum team, you will work with BAs, Scrum Master, Leads and engineers to provide data support to our products and build creative solutions and features to move our product roadmap forward.
Participate in the design of databases, using first, second or third normalized form as needed to support business requirements.
Create and deploy ADF pipelines, adhering to Greystar’s standards and documented best practices.
Perform analysis of complex data and document findings.
Prepare data for prescriptive and predictive modeling.
Combine raw data from different external sources.
Collaborate with data scientists and architects.
Play a direct role in the maintenance, technical support, documentation, and administration of databases.
Who You Are
Strong problem solver with excellent communication skills
Have a growth mindset with a desire to learn and embrace challenges.
Innovative and passionate about your work
""Self-starter"" attitude and the ability to make decisions independently.
What You Have
Minimum of 3 years of relevant experience in database design and development
Minimum of 2 years of relevant experience in working with Azure PaaS databases
Minimum of 1 year of relevant experience working with Azure Data Factory.
Minimum of 1 year of relevant experience working with Azure Data Lakes Gen 2.
Working knowledge of Azure Synapse.
Preferred: Experience with Customer Insights and/or Dataverse.
Preferred: Experience with Power BI.
Bachelor’s in Computer Science, related field, or equivalent work experience
Technical Pre-screening test will be required for all candidates
What the Right Candidate will Enjoy!
100% Remote flexibility!
Competitive pay, benefits, and overall compensation packages.
The chance to be part of a technology team for a thriving organization that prioritizes accountability, respect, and operational excellence!
The opportunity to join a thriving, highly visible organization during its technology transformation!
The base compensation rate will vary based on education, experience, skills, and geographic location, as applicable.
Greystar seeks to attract, recruit, advance and retain top talent. Greystar’s compensation strategy is tailored to appropriately reward the skillset and experience that a team member will bring to the organization.
Depending on the position offered, regular full-time and part-time team members may be eligible to participate in a bonus program in addition to their salary. Team members may also participate in the 401k plan, once eligible. Regular, full-time team members are offered a range of medical, financial, and other benefits from which to choose.
For Union and Prevailing Wage roles compensation and benefits may vary from the listed information above due to Collective Bargaining Agreements and/or local governing authority.
Greystar will consider for employment qualified applicants with arrest and conviction records.
Start your job application: click Apply Now
Show Less
Report",3.8,10000+ Employees,1993,Company - Private,Real Estate,Real Estate,$500 million to $1 billion (USD)
Data Engineer,$100K - $250K (Employer est.),Tradeweb Markets LLC3.8 ★,"Jersey City, NJ","The Billing Technology team is responsible for the overall ownership of our internal billing and invoicing systems including the development, quality assurance and operations of the platform.
As a member of this team, you will be able to take in a set of complex requirements and use your own engineering and architectural skills to analyze them and distill a clear plan. You will develop a strong partnership with our business counterparts, proactively identifying problems they face and apply creative technological solutions to solve them.
Trust is fundamental to our trading platforms and our business overall and an invoice is a critical touchpoint between us and our customers. Therefore, the accuracy of the calculations and system outputs are paramount. This means that quality and stability will be fundamental to all changes made within the platform and that unit testing will play a critical role in the SDLC.
Tradeweb Technology jobs are fully remote. The Tradeweb Technology hub is located in our Jersey City office which can be used for team meetings and collaboration efforts. There may be days where travel to the Jersey City office is recommended for organizational off-sites.
Job Responsibilities
Liaising with finance team members and product managers to craft requirements
Enumerating the necessary datasets and designing the data model and schemas to support the billing and revenue processes.
Core engineering including implementing the necessary ETL, mediation code and rating algorithms to calculate revenue and generate invoices within an enterprise third party SaaS billing platform.
Platform support including problem solving and debugging, especially for production issues.
Hosting and participating in code reviews and QA issue tracking/remediation
Rigorous unit testing.
Moving trade and financial data between trade reporting repositories and financial systems
Qualifications
BS or higher in a technical field: CS, Physics, Math etc.
3 + years of experience working with Python
3 + years of experience JavaScript
Proficient in writing and debugging complex SQL queries
Experience building ETL and stream processing pipelines using Kafka, Spark, Flink, Airflow/Prefect, etc.
Familiarity with data science stack: e.g. Juypter, Pandas, Scikit-learn, Dask, Pytorch, MLFlow, Kubeflow, etc.
Strong proclivity for automation and DevOps practices
Experience with managing increasing data volume, velocity and variety
Agile, self-starter and is focused on getting things done
Strong communicator
Additional Information
Tradeweb is committed to providing valuable and competitive benefits. In addition to working in our culture of innovation and collaboration, we offer:
Health Insurance: Highly competitive medical, dental, and vision programs
Hybrid Environment: Our employees have the flexibility of working in the office and from home.
Health Care and Dependent Care Flexible Spending Accounts: You may elect to set aside pre-tax earnings to pay for eligible health care and dependent day care expenses for you and your eligible family members.
Maven Family Building Benefit: Maven offers support for fertility and preconception; pregnancy and post-partum; adoption; surrogacy and pediatrics for children up to age 10. Tradeweb provide a $10,000 lifetime reimbursement towards fertility, egg freezing, adoption and surrogacy expenses.
Building Wealth - 401(k) Savings Plan: Employees are immediately eligible for the 401(k) plan. Participants may contribute up to 75% of eligible compensation into a traditional 401(k) and/or Roth 401(k). Tradeweb will match 100% of the first 4% of compensation that you contribute.
The current pay range for this role if performed in the city of New York is currently $100,000 to $250,000 per year, based on a regular, full-time schedule. The amount of pay offered will be determined by a number of factors, including but not limited to qualifications, market data, geographic location, and internal guidelines.
Other Benefit Programs
Pre-Tax Commuter Benefits Program
ARAG Legal Services
Employee Assistance Program
Tuition Reimbursement
Financial Wellness Tools
Travel Assistance Benefits
Pet Insurance
Corporate Gym Subsidies
Wellness Perks
Paid Time Off and Parental Leave
Company Description
Tradeweb Markets is a world leader in the evolution of electronic trading. A fintech company serving approximately 2,500 clients – including the world’s largest banks, asset managers, hedge funds, insurance companies, wealth managers and retail clients - in more than 65 countries across the globe. Since our first trade in 1998, we have helped transform and electronify the fixed income markets. Tradeweb is a culture built on innovation, creativity and collaboration. Through a combination of very talented and driven people, innovative products and solutions, cutting-edge technology, market data, and a vast network of clients, we continue to work together to improve the way financial markets trade.
Mission: Move first and never stop. Collaborate with clients to create and build solutions that drive efficiency, connectivity, and transparency in electronic trading.
Tradeweb Markets LLC (""Tradeweb"") is proud to be an EEO Minorities/Females/Protected Veterans/Disabled/Affirmative Action Employer.
https://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf
Start your job application: click Apply Now
Show Less
Report",3.8,501 to 1000 Employees,1996,Company - Public,Financial Transaction Processing,Financial Services,$1 to $5 billion (USD)
"Software Engineer, Data Platform",$99K - $160K (Glassdoor est.),Inclusively5.0 ★,"Seattle, WA","Inclusively is partnering with a one of the largest transportation networks to hire a Software Engineer, Data Platform.
ABOUT INCLUSIVELY:
Inclusively is a digital tech platform that connects candidates with disabilities, who may benefit from workplace accommodations, to inclusive employers. This includes all disabilities under the ADA, including mental health conditions (e.g. anxiety, depression, PTSD), chronic illnesses (e.g. diabetes, Long COVID), and neurodivergence (e.g. autism, ADHD). Applicants with one or more of these conditions are encouraged to apply; Inclusively does not require applicants to disclose their specific disability.
Responsibilities:
Design, develop, deploy, monitor, operate and maintain existing or new elements of our platform
Help establish roadmap and architecture based on technology and our needs
Write well-crafted, well-tested, readable, maintainable code
Analyze our internal systems and processes and locate areas for improvement/automation
Collaborate with product org stakeholders to address and prioritize custom edge cases
Help lead large projects from inception to positive execution
Unblock, support and communicate with internal partners to achieve results
Experience:
3+ years of software engineering industry experience and with data structures/algorithms
2+ years of experience building and developing large-scale infrastructure, distributed systems or networks, and/or experience with data infrastructure
Experience working with kubernetes and container technologies (e.g. Docker, cri-o, etc)
Familiar with a cloud-based environments such as AWS/GCP/Azure
Benefits:
Great medical, dental, and vision insurance options
Mental health benefits
Family building benefits
In addition to 12 observed holidays, salaried team members have unlimited paid time off, hourly team members have 15 days paid time off
401(k) plan to help save for your future
18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible
Pre-tax commuter benefits
Out team members get an exclusive opportunity to test new benefits of our Ridership Program
Job Type: Full-time
Benefits:
401(k)
Health insurance
Paid time off
Parental leave
Schedule:
Monday to Friday
Work Location: In person
Show Less
Report",5.0,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
Data Engineer,$90K - $110K (Employer est.),LTIMindtree3.8 ★,United States,"Technical Skillset: COSMOS / SCOPE, Maestro, Azure Data Factory, SQL Server 2016, Microsoft BI Suite – SSIS/SSAS, Azure Cosmos DB, Azure Databricks, PySpark, Python, Power BI & Excel, Azure Databricks, EDL, Azure synapse.

Job Duties:
# Experience in SQL Programming language Cosmos Scope scripting.
# Knowledge of Big Data pipelines Data Engineering.
# Working experience on Azure DevOps is a must
# Working Knowledge on MSBI stack on Azure.
# Working Knowledge on Azure Data factory, Azure Data Lake and Azure Data lake storage.
# Hands-on in Visualization like PowerBI.
# Implement end-end data pipelines using cosmos Azure Data factory.
# Should have good analytical thinking and Problem solving.
# Experience with Data quality implementations assessing data correctness, completeness, uniqueness etc
# Experience working on PII, GDPR, handling sensitive data (encryption/decryption)
# Good communication and co-ordination skills.
# Able to work as Individual contributor.
# Requirement Analysis.
# Create Maintain and Enhance Big Data Pipeline.
# Daily status reporting, interacting with Leads.
# Version control ADO GIT, CI CD.
# Marketing Campaign experiences.
# Data Platform, Product telemetry, Analytical thinking.
# Data Validation of the new streams.
# Data quality check of the new streams.
# Monitoring of data pipeline created in Azure Data factory.
# updating the Tech spec and wiki page for each implementation of pipeline.
# Updating ADO on daily basis.

About LTIMindtree
LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 750 clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com

Equal Employment Opportunity Policy
LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.

https://www.ltimindtree.com/general-privacy-policy


Salary Range - $90,000 - $110,000
Show Less
Report",3.8,10000+ Employees,1997,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
